#+TITLE: Ticket 65
#+AUTHOR: Zach Mandeville


* Ticket
  [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/65][Gitlab Link]]
  
  #+BEGIN_QUOTE
  We can model our function to load swagger files off the way we load our audit events, where we create a temporary table of data, and then use this to grab metadata and place it in the correct columns.  This will allow us to include the content-hash and url and more for each swagger in its table.
  #+END_QUOTE
* Strategy
  Our audit events are going to be coming from specific buckets and jobs, the artifacts of which hold a metadata.json file that has far more metadata than can be found in the swagger file.  With the swagger, we mostly want to know what the swagger was at the particular commit run for this audit event.  This bucket/job metadata will also help us in doing cross coverage in general.
  
  So the strategy is to create a new table for bucket/job metadata and use the details from that to determine the swagger we pull down.  We can then do a simple join on the swagger to get its commit hash from the metadata.
* Process
** Curl a sample metadata file
   We'll use the most recent job/bucket from apisnoop, following the spyglass link to get to its artifacts.
   
  #+NAME Curl Sample Metadata
  #+BEGIN_SRC shell
  curl 'https://storage.googleapis.com/kubernetes-jenkins/logs/ci-kubernetes-e2e-gci-gce/1178464478988079104/finished.json'
  #+END_SRC

  #+RESULTS:
  #+begin_EXAMPLE
  {
    "timestamp": 1569805261, 
    "version": "v1.17.0-alpha.0.1912+2b795b9825e485", 
    "result": "SUCCESS", 
    "passed": true, 
    "job-version": "v1.17.0-alpha.0.1912+2b795b9825e485", 
    "metadata": {
      "node_os_image": "cos-73-11647-163-0", 
      "infra-commit": "dba192364", 
      "master_os_image": "cos-73-11647-163-0", 
      "job-version": "v1.17.0-alpha.0.1912+2b795b9825e485", 
      "pod": "f53ddc96-e317-11e9-9585-baa219f9d09d", 
      "revision": "v1.17.0-alpha.0.1912+2b795b9825e485"
    }
  }
  #+end_EXAMPLE
  
  The commit hash is the ~2b795b9825e485~ appended to the version.  We can view the swagger for this commit at this url:
  https://raw.githubusercontent.com/kubernetes/kubernetes/2b795b9825e485/api/openapi-spec/swagger.json
  
  We can load our metadata into our db using the COPY FROM PROGRAM command.  This will just copy over our data as a json blob, so we want to create a temporary table to feed that into, then select from that to generate our actual table.
** Create job_bucket Table
#+NAME: job_bucket_full
#+BEGIN_SRC sql-mode :results silent
  DROP TABLE job_bucket;
  CREATE TABLE job_bucket (
      id int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
      ingested_at timestamp DEFAULT CURRENT_TIMESTAMP,
      bucket text,
      job text,
      commit_hash text,
      passed text,
      job_result text,
      pod text,
      infra_commit text,
      job_version text,
      job_timestamp timestamp,
      node_os_image text,
      master_os_image text ,
      swagger jsonb
  );
#+END_SRC
 
** Create python script for curl > temp table > job_bucket flow
    Am using the audit_event loading as a guide, though put in an explicit exception to help catch sql bugs.
   
#+NAME: load_job_bucket_via_curl.py
#+BEGIN_SRC python :results output
            try:
                from urllib.request import urlopen, urlretrieve
                from string import Template
                import json
                bucket = 'ci-kubernetes-e2e-gci-gce' # for debugging right now
                job = '1178464478988079104' # for debugging right now
                metadata_url = ''.join(['https://storage.googleapis.com/kubernetes-jenkins/logs/', bucket, '/', job, '/finished.json'])
                metadata = json.loads(urlopen(metadata_url).read().decode('utf-8'))
                commit_hash = metadata["version"].split("+")[1]
                swagger_url =  ''.join(['https://raw.githubusercontent.com/kubernetes/kubernetes/', commit_hash, '/api/openapi-spec/swagger.json']) 
                #swagger = json.loads(urlopen(swagger_url).read().decode('utf-8'))
                swagger = urlopen(swagger_url).read().decode('utf-8') # may change this to ascii
                # sql = Template("""
                sql = """
             INSERT INTO job_bucket(
                       bucket,
                       job,
                       commit_hash, 
                       passed,
                       job_result,
                       pod,
                       infra_commit,
                       job_version,
                       job_timestamp,
                       node_os_image,
                       master_os_image,
                       swagger
                )
               SELECT
                       $1 as bucket,
                       $2 as job,
                       $3 as commit_hash,
                       $4 as passed,
                       $5 as job_result,
                       $6 as pod,
                       $7 as infra_commit,
                       $8 as job_version,
                       (to_timestamp($9)) AT TIME ZONE 'UTC' as job_timestamp,
                       $10 as node_os_image,
                       $11 as master_os_image,
                       $12 as swagger
                """
                plan = plpy.prepare(sql, [
                    'text','text','text','text',
                    'text','text','text','text',
                    'integer','text','text','jsonb'])
                rv = plpy.execute(plan, [
                    bucket,job,commit_hash,
                    metadata['passed'],metadata['result'],
                    metadata['metadata']['pod'],
                    metadata['metadata']['infra-commit'],
                    metadata['version'],
                    int(metadata['timestamp']),
                    metadata['metadata']['node_os_image'],
                    metadata['metadata']['master_os_image'],
                    json.dumps(swagger)
                ])
                return "it worked!"
            except Exception as err:
                return Template("something went wrong, likely this: ${error}").substitute(error = err)
#+END_SRC

#+RESULTS: load_job_bucket_via_curl.py

 The only real manipulation of the data we do is:
- convert the passed value back into a boolean (it comes in as text)

- convert the unix timestamp into a postgres timestamp

- create commit hash by splitting job_version on '+' and grabbing the last part.
 
We can now create the sql function 
  #+NAME: load_job_bucket_via_curl.sql
  #+BEGIN_SRC sql-mode :noweb yes
    set role dba;
    DROP FUNCTION IF EXISTS load_job_bucket_via_curl;
    CREATE OR REPLACE FUNCTION load_job_bucket_via_curl(bucket text, job text)
    RETURNS text AS $$
    <<load_job_bucket_via_curl.py>>
    $$ LANGUAGE plpython3u ;
    reset role;
  #+END_SRC

  #+RESULTS: load_job_bucket_via_curl.sql
  #+begin_src sql-mode
  SET
  DROP FUNCTION
  apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# CREATE FUNCTION
  RESET
  #+end_src

  
** Load Job Bucket
   Now we test out the loading of the job_buckets.  We'll use the most recent api_snoop bucket and two other random ones in the jenkins logs.
 #+NAME: Test It Out pt. 2
 #+BEGIN_SRC sql-mode
  select * from load_job_bucket_via_curl('ci-kubernetes-e2e-gci-gce', '1178464478988079104');
  select * from load_job_bucket_via_curl('ci-kubernetes-e2e-gci-gce', '1148565955786313730');
  select * from load_job_bucket_via_curl('ci-kubernetes-e2e-gci-gce', '1148586594899333122');
 #+END_SRC

 #+RESULTS: Test It Out pt. 2
 #+begin_src sql-mode
  load_job_bucket_via_curl 
 --------------------------
  it worked!
 (1 row)

 #+end_src

 #+BEGIN_SRC sql-mode
 select job_version, job_timestamp, passed from job_bucket;
 #+END_SRC

 #+RESULTS:
 #+begin_src sql-mode
              job_version             |    job_timestamp    | passed 
 -------------------------------------+---------------------+--------
  v1.17.0-alpha.0.1912+2b795b9825e485 | 2019-09-30 01:01:01 | True
  v1.17.0-alpha.0.1912+2b795b9825e485 | 2019-09-30 01:01:01 | True
  v1.17.0-alpha.0.1912+2b795b9825e485 | 2019-09-30 01:01:01 | True
 (3 rows)

 #+end_src
  
 Fantastic!

* Conclusion
  We reduced the cognitive load by including the swagger into our job_bucket.  This means we'll have two tables still: one for swagger and surrounding metadata, and one for audit_events.  all views would spring from these.
  
  The step to load job_buckets is done with a mix of python and sql, and using the plpy functions of ~prepare~ and ~execute~, this lets us do simple variable subsitution and ensure json loads properly (trying to do it with python subtitution gave escaping errors).  

We still need to set keys to avoid duplication, but can verify that we can load multiple swaggers, and distinguish them by commit hash or the date of the job run.

* Next Steps
** TODO Check for primary key in the job bucket, and ensure we don't put in duplicates
   we'll use bucket + job as the primary key.
** DONE Expand api_swagger to include commit hash, job, bucket as means to connect to job_bucket.
   CLOSED: [2019-10-03 Thu 20:44]
   we did this by including the swagger in our job_bucket
** TODO Expand audit_events to include commit hash, job, bucket as means to connect to job_bucket.
** TODO Trigger an insertion into our raw_audit_events, based on insertion into job_bucket
   This is a bit more involved, and will likely require some prepping of the data before we put it into the db.
** TODO add job_bucket table to migrations, with a few swaggers loaded.
** TODO Update swagger views to pull from job_bucket instead
* Footnotes
** Connect to Database
    If you already have your db and hasura endpoint up and running:
 - [ ] Connect to your postgres db from within this file
   You'll want execute this code block by moving your cursor within and typing =,,=
  
   #+NAME: Connect org to postgres
   #+BEGIN_SRC emacs-lisp :results silent
     (sql-connect "apisnoop" (concat "*SQL: postgres:data*"))
   #+END_SRC

 - [ ] Test your connection works
   You can run this sql block, and it see a message in your minbuffer like:
   : You are connected to database "apisnoop" as user "apisnoop" on host "localhost" at port "10041".

   #+NAME: Test Connection
   #+BEGIN_SRC sql-mode :results silent
   \conninfo
   #+END_SRC


