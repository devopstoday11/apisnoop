#+TITLE: An Approach to Promotions
#+TODOs: TEST TESTING PASSED | FAILED

* Goal
 My goal is familiar: increase conformance coverage.  Since the metric for coverage is the number of endpoints hit by conformance tests, then a simple strategy would be to promote the tests hitting endpoints that are not yet conformance-tested.
 
 However, we want these endpoints to be _meaningfully_ tested-- if an endpoint is hit as a side effect of a test, this is not meaningful.  Similarly, if a test is hitting every endpoint, it likely isn't written with enough focus and wouldn't be a good promotion candidate.  I also do not want to create chaff in our pull requests, and want to be able to suggest a test with as much confidence as possible before seeking the help of experts.

 **AND SO**, I looked to create a query that would tests that hit a small number of endpoints, endpoints hit by a small number of tests, and where these two intersect.  My hypothesis is that these tests are likely well-focused, and will be easier to reason if they are good promotion candidates.  

**ALSO**, I wanted a simple way to work through the test, and check if it's a good canidate, including to make sure this is one not already up for promotion.

This is a document of my research and work.

* Process
** Our audit event set
   When APISnoop is applied to a cluster, it adds the latest set of audit events from master, and the same results tagged as 'apisnoop|live'.  The latter is meant to be updated and added to as we work through our tests, while the former servers as an immutable baseline.
   
  Here is the bucket and job  we will be working with throughout this article 
  #+NAME: bucket and job
  #+begin_src sql-mode
    SELECT
      bucket,
      job,
      job_timestamp
      FROM
          bucket_job_swagger;
  #+end_src

  #+RESULTS: bucket and job
  #+begin_src sql-mode
            bucket           |         job         |    job_timestamp    
  ---------------------------+---------------------+---------------------
   ci-kubernetes-e2e-gci-gce | 1206727790053822466 | 2019-12-17 00:44:21
   apisnoop                  | live                | 2019-12-17 00:44:21
  (2 rows)

  #+end_src

** Take a look at tests and the unique endpoints they hit
   

   Our audit events view has all the useragents.  We can reduce these down to just those that start with `e2e` to grab our tests.
   
   #+NAME: Number of tests
   #+begin_src sql-mode
     SELECT
       COUNT(DISTINCT useragent)
       FROM
           audit_event
      WHERE
        useragent LIKE 'e2e.test%'
        AND job != 'live'
        ;
   #+end_src

   #+RESULTS: Number of tests
   #+begin_src sql-mode
    count 
   -------
      823
   (1 row)

   #+end_src

   IF we look at a sampling of this, we can see how the useragent is structured.  The test is always given after '--'.
   
   #+NAME: Test Sample
   #+begin_src sql-mode
     SELECT
       DISTINCT useragent
       FROM
           audit_event
      WHERE
             useragent LIKE 'e2e.test%'
         AND job != 'live'
      LIMIT 10
            ;
   #+end_src

   #+RESULTS: Test Sample
   #+begin_src sql-mode
                                                                                                                                            useragent                                                                                                                                         
   -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    e2e.test/v1.18.0 (linux/amd64) kubernetes/a13ddd5
    e2e.test/v1.18.0 (linux/amd64) kubernetes/a13ddd5 -- [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/a13ddd5 -- [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/a13ddd5 -- [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/a13ddd5 -- [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/a13ddd5 -- [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/a13ddd5 -- [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/a13ddd5 -- [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/a13ddd5 -- [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/a13ddd5 -- [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set [NodeConformance]
   (10 rows)

   #+end_src

 This means we can user postgres' ~split_part~ function to get just the test name.
 
   #+NAME: Test Name Sample
   #+begin_src sql-mode
     SELECT
       DISTINCT split_part(useragent, '--', 2) as test
       FROM
           audit_event
      WHERE
             useragent LIKE 'e2e.test%'
         AND job != 'live'
      LIMIT 10
            ;
   #+end_src

   #+RESULTS: Test Name Sample
   #+begin_src sql-mode
                                                                                                                    test                                                                                                                  
   ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

     [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
     [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
     [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
     [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
     [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
     [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
     [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
     [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
     [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set [NodeConformance]
   (10 rows)

   #+end_src
   
  Sweet, we have a simple way to list our tests.  Next is to see how many endpoints these tests hit.

** Count the distinct endpoints each test hits
   We can do a distinct count using postgres' count filter option.

   Ordering by count, descending, will give us the tests that hit the _most_ endpoints.
   #+NAME: Test and Count, Highest 
   #+begin_src sql-mode
     SELECT DISTINCT
       COUNT(distinct operation_id) FILTER(where useragent = audit_event.useragent) as distinct_endpoints,
       split_part(useragent, '--', 2) as test
       FROM
           audit_event
      WHERE
             useragent LIKE 'e2e.test%'
         AND job != 'live'
        GROUP BY useragent
            ORDER BY distinct_endpoints DESC
      LIMIT 25
            ;
   #+end_src

   #+RESULTS: Test and Count, Highest
   #+begin_src sql-mode
    distinct_endpoints |                                                                                          test                                                                                           
   --------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
                   122 |  [sig-cli] Kubectl client kubectl get output should contain custom columns for each resource
                    64 |  [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
                    59 |  [sig-network] Services should create endpoints for unready pods
                    38 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod
                    35 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod
                    34 |  [sig-storage] CSI mock volume CSI attach test using mock driver should not require VolumeAttach for drivers without attachment
                    34 |  [sig-storage] CSI mock volume CSI attach test using mock driver should require VolumeAttach for drivers with attachment
                    34 |  [sig-storage] CSI mock volume CSI online volume expansion should expand volume without restarting pod if attach=off, nodeExpansion=on
                    34 |  [sig-storage] CSI mock volume CSI Volume expansion should expand volume by restarting pod if attach=off, nodeExpansion=on
                    34 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it
                    34 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works
                    34 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] volumes should store data
                    34 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it
                    34 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works
                    34 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source
                    34 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] volumes should store data
                    33 |  [sig-storage] CSI mock volume CSI workload information using mock driver should be passed when podInfoOnMount=true
                    33 |  [sig-storage] CSI mock volume CSI workload information using mock driver should not be passed when podInfoOnMount=false
                    33 |  [sig-storage] CSI mock volume CSI workload information using mock driver should not be passed when podInfoOnMount=nil
                    33 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
                    33 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
                    33 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory
                    33 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]
                    33 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]
                    33 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path
   (25 rows)

   #+end_src
   
   Next, the tests that hit the _least_
   
   #+NAME: Test and Count, Lowest
   #+begin_src sql-mode
     SELECT DISTINCT
       COUNT(distinct operation_id) FILTER(where useragent = audit_event.useragent) as distinct_endpoints,
       split_part(useragent, '--', 2) as test
       FROM
           audit_event
      WHERE
             useragent LIKE 'e2e.test%'
         AND job != 'live'
        GROUP BY useragent
            ORDER BY distinct_endpoints ASC
      LIMIT 25
            ;
   #+end_src

   #+RESULTS: Test and Count, Lowest
   #+begin_src sql-mode
    distinct_endpoints |                                                                    test                                                                    
   --------------------+--------------------------------------------------------------------------------------------------------------------------------------------
                     3 |  [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/json"
                     3 |  [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/json,application/vnd.kubernetes.protobuf"
                     3 |  [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/vnd.kubernetes.protobuf"
                     3 |  [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/vnd.kubernetes.protobuf,application/json"
                     6 | 
                     7 |  [k8s.io] Probing container should be restarted with a docker exec liveness probe with timeout 
                     7 |  [k8s.io] [sig-node] crictl should be able to run crictl on the node
                     7 |  [k8s.io] [sig-node] SSH should SSH to all nodes and run commands
                     7 |  [sig-api-machinery] Servers with support for Table transformation should return generic metadata details across all namespaces for nodes
                     7 |  [sig-auth] PodSecurityPolicy should forbid pod creation when no PSP is available
                     7 |  [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
                     7 |  [sig-cli] Kubectl client Kubectl apply apply set/view last-applied
                     7 |  [sig-cli] Kubectl client Kubectl apply should reuse port when apply to an existing SVC
                     7 |  [sig-cli] Kubectl client Kubectl cluster-info dump should check if cluster-info dump succeeds
                     7 |  [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]
                     7 |  [sig-cli] Kubectl client Kubectl create quota should reject quota with invalid scopes
                     7 |  [sig-cli] Kubectl client Kubectl get componentstatuses should get componentstatuses
                     7 |  [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
                     7 |  [sig-cli] Kubectl client Proxy server should support 
                     7 |  [sig-cli] Kubectl client Proxy server should support proxy with 
                     7 |  [sig-instrumentation] MetricsGrabber should grab all metrics from API server.
                     7 |  [sig-scheduling] Multi-AZ Clusters should spread the pods of a replication controller across zones
                     7 |  [sig-scheduling] Multi-AZ Clusters should spread the pods of a service across zones
                     7 |  [sig-scheduling] Multi-AZ Cluster Volumes [sig-storage] should only be allowed to provision PDs in zones where nodes exist
                     7 |  [sig-scheduling] Multi-AZ Cluster Volumes [sig-storage] should schedule pods in the same zones as statically provisioned PVs
   (25 rows)

   #+end_src

   
   So this looks like the lowest # of distinct endpoints hit by a test is 3.  

I want to do a quick sanity check, to validate this count filter.  I'll grab two tests from the above views and list their distinct operation_id's.  The number of records should match the count.
   
One with three:
   #+NAME: Test with distinct_endpoint count of 3
   #+begin_src sql-mode
     select distinct
       operation_id
       from audit_event
      where useragent like '%[sig-api-machinery] client-go should negotiate watch and report errors with accept "application/json"'
            and job != 'live'
            ;
-- records returns should be 3
   #+end_src

   #+RESULTS: Test with distinct_endpoint count of 3
   #+begin_src sql-mode
           operation_id         
   -----------------------------
    listCoreV1NamespacedService
    listCoreV1Node
    readCoreV1NamespacedService
   (3 rows)

   #+end_src
   
And one with 7:
   
   #+NAME: Test with distinct_endpoint count of 7
   #+begin_src sql-mode
          select distinct
            operation_id
            from audit_event
           where useragent like '%[sig-scheduling] Multi-AZ Clusters should spread the pods of a replication controller across zones'
                 and job != 'live'
                 ;
     -- records returns should be 7
   #+end_src

   #+RESULTS: Test with distinct_endpoint count of 7
   #+begin_src sql-mode
                     operation_id                  
   ------------------------------------------------
    createAuthorizationV1SubjectAccessReview
    createCoreV1Namespace
    createRbacAuthorizationV1NamespacedRoleBinding
    deleteCoreV1Namespace
    listCoreV1NamespacedServiceAccount
    listCoreV1Node
    readCoreV1Namespace
   (7 rows)

   #+end_src

   The numbers match, and the logic is simple enough, that I felt confident in the approach.
** Check out Distribution of distinct endpoints
   
   I was curious  on the distribution in the tests, if there's a common number of endpoints that are hit.   I'll use the query above, then select some basic calcs from it.  From this point forward, though, I'll only look at stable endpoints, since it's only these we are concerned about for conformance.

   #+NAME: stats for tests that hit stable|core endpoints
   #+begin_src sql-mode
     select
       count(distinct test) as total_tests,
       MAX(distinct_endpoints) as max_endpoints_hit_by_test,
       MIN(distinct_endpoints) as min_endpoints_hit_by_test,
       AVG(distinct_endpoints) as avg_endpoints_hit_by_test
           FROM (
     SELECT 
       COUNT(distinct ae.operation_id) FILTER(where useragent = ae.useragent) as distinct_endpoints,
       split_part(useragent, '--', 2) as test,
       useragent
       FROM
           audit_event ae
      JOIN api_operation_material ao ON (ae.operation_id = ao.operation_id)
      WHERE
         useragent LIKE 'e2e.test%'
         AND ae.job != 'live'
         AND ao.level = 'stable'
        GROUP BY useragent
            ORDER BY distinct_endpoints DESC
           ) as tests
            ;
   #+end_src

   #+RESULTS: stats for tests that hit stable|core endpoints
   #+begin_src sql-mode
    total_tests | max_endpoints_hit_by_test | min_endpoints_hit_by_test | avg_endpoints_hit_by_test 
   -------------+---------------------------+---------------------------+---------------------------
            823 |                        80 |                         3 |       14.0534629404617254
   (1 row)

   #+end_src


   There is more we can do with distribution and means and such, but I'd say looking at tests that hit 14 or less endpoints would be a useful filter.
   
   
   Then, I ran a similar query for endopints.
** Check out low-tested endpoints.  
   
   This is the same basic logic that we used for tests.  There's only a bit over a hundred stable endpoints, so I listed all to better see the pattern of distribution.
  
   #+NAME: Low Tested Endpoints
   #+begin_src sql-mode
     SELECT
          COUNT(distinct ae.useragent) FILTER(where ae.operation_id = ae.operation_id) as distinct_tests,
            ae.operation_id
            FROM
            audit_event ae
            JOIN api_operation_material ao on (ae.operation_id = ao.operation_id)
            WHERE
            useragent LIKE 'e2e.test%'
            AND ae.job != 'live'
            AND ao.level = 'stable'
            GROUP BY ae.operation_id 
            ORDER BY distinct_tests DESC
            ;
   #+end_src

   #+RESULTS: Low Tested Endpoints
   #+begin_src sql-mode
    distinct_tests |                             operation_id                              
   ----------------+-----------------------------------------------------------------------
               828 | listCoreV1Node
               823 | createCoreV1Namespace
               823 | readCoreV1Namespace
               823 | listCoreV1NamespacedServiceAccount
               823 | deleteCoreV1Namespace
               822 | createAuthorizationV1SubjectAccessReview
               822 | createRbacAuthorizationV1NamespacedRoleBinding
               582 | readCoreV1NamespacedPod
               560 | createCoreV1NamespacedPod
               452 | deleteCoreV1NamespacedPod
               393 | listCoreV1NamespacedPod
               303 | readCoreV1NamespacedPodLog
               241 | deleteCoreV1NamespacedPersistentVolumeClaim
               237 | createCoreV1NamespacedPersistentVolumeClaim
               232 | readCoreV1NamespacedPersistentVolumeClaim
               211 | readCoreV1PersistentVolume
               204 | readCoreV1Node
               186 | connectCoreV1PostNamespacedPodExec
               161 | createCoreV1PersistentVolume
               161 | deleteCoreV1PersistentVolume
               117 | deleteStorageV1StorageClass
               116 | createStorageV1StorageClass
                98 | createCoreV1NamespacedService
                67 | deleteCoreV1NamespacedService
                62 | createCoreV1NamespacedConfigMap
                60 | createRbacAuthorizationV1ClusterRoleBinding
                58 | deleteRbacAuthorizationV1NamespacedRoleBinding
                56 | deleteRbacAuthorizationV1ClusterRoleBinding
                49 | createCoreV1NamespacedSecret
                47 | deleteAppsV1NamespacedStatefulSet
                47 | createAppsV1NamespacedStatefulSet
                41 | createCoreV1NamespacedServiceAccount
                40 | createRbacAuthorizationV1ClusterRole
                39 | createRbacAuthorizationV1NamespacedRole
                39 | deleteCoreV1NamespacedServiceAccount
                38 | deleteRbacAuthorizationV1ClusterRole
                38 | readCoreV1NamespacedService
                37 | deleteRbacAuthorizationV1NamespacedRole
                34 | createAppsV1NamespacedDeployment
                33 | listAppsV1NamespacedReplicaSet
                32 | readAppsV1NamespacedDeployment
                31 | createCoreV1NamespacedReplicationController
                31 | listCoreV1NamespacedEndpoints
                30 | listRbacAuthorizationV1ClusterRole
                28 | createApiextensionsV1CustomResourceDefinition
                27 | deleteAppsV1NamespacedDeployment
                27 | deleteApiextensionsV1CustomResourceDefinition
                27 | deleteCoreV1NamespacedSecret
                23 | createCoreV1NamespacedEndpoints
                23 | deleteCoreV1NamespacedEndpoints
                19 | replaceCoreV1NamespacedPersistentVolumeClaim
                17 | readStorageV1StorageClass
                17 | replaceCoreV1Namespace
                17 | deleteCoreV1NamespacedReplicationController
                17 | readCoreV1NamespacedResourceQuota
                17 | readCoreV1NamespacedReplicationController
                17 | deleteCoreV1NamespacedConfigMap
                16 | createCoreV1NamespacedResourceQuota
                15 | createAppsV1NamespacedReplicaSet
                12 | listAppsV1NamespacedDeployment
                12 | replaceCoreV1NamespacedService
                12 | replaceCoreV1NamespacedConfigMap
                11 | readBatchV1NamespacedJob
                11 | replaceCoreV1NamespacedPod
                10 | listCoreV1NamespacedResourceQuota
                10 | createAdmissionregistrationV1ValidatingWebhookConfiguration
                10 | createBatchV1NamespacedJob
                 9 | replaceAppsV1NamespacedStatefulSet
                 9 | listCoreV1PersistentVolume
                 9 | deleteAdmissionregistrationV1ValidatingWebhookConfiguration
                 9 | createAdmissionregistrationV1MutatingWebhookConfiguration
                 9 | listCoreV1NamespacedPersistentVolumeClaim
                 9 | readAppsV1NamespacedStatefulSet
                 9 | listAppsV1NamespacedStatefulSet
                 8 | createCoreV1NamespacedPodEviction
                 8 | readCoreV1NamespacedEndpoints
                 8 | deleteAdmissionregistrationV1MutatingWebhookConfiguration
                 7 | listStorageV1StorageClass
                 7 | readCoreV1NamespacedConfigMap
                 6 | listCoreV1NamespacedConfigMap
                 6 | getAPIVersions
                 5 | listCoreV1NamespacedReplicationController
                 5 | deleteBatchV1NamespacedJob
                 5 | listBatchV1NamespacedJob
                 5 | replaceAppsV1NamespacedDeployment
                 4 | listCoreV1NamespacedService
                 4 | getAdmissionregistrationV1APIResources
                 4 | getApiextensionsV1APIResources
                 4 | readAppsV1NamespacedReplicaSet
                 4 | getCoreAPIVersions
                 3 | getAutoscalingV1APIResources
                 3 | createCoreV1NamespacedPodTemplate
                 3 | readCoreV1NamespacedServiceAccount
                 3 | getCoordinationV1APIResources
                 3 | patchApiextensionsV1CustomResourceDefinition
                 3 | getBatchV1APIResources
                 3 | getApiregistrationV1APIResources
                 3 | getSchedulingV1APIResources
                 3 | readStorageV1VolumeAttachment
                 3 | getStorageV1APIResources
                 3 | getCoreV1APIResources
                 3 | readCoordinationV1NamespacedLease
                 3 | getAppsV1APIResources
                 3 | getNetworkingV1APIResources
                 3 | getAuthenticationV1APIResources
                 3 | getRbacAuthorizationV1APIResources
                 3 | getAuthorizationV1APIResources
                 3 | readApiextensionsV1CustomResourceDefinition
                 3 | deleteCoreV1NamespacedResourceQuota
                 2 | replaceCoreV1NamespacedResourceQuota
                 2 | deleteApiregistrationV1APIService
                 2 | deleteCoordinationV1NamespacedLease
                 2 | listCoordinationV1NamespacedLease
                 2 | createSchedulingV1PriorityClass
                 2 | readCoreV1NamespacedSecret
                 2 | createCoordinationV1NamespacedLease
                 2 | deleteSchedulingV1PriorityClass
                 2 | replaceApiextensionsV1CustomResourceDefinition
                 2 | patchCoreV1NamespacedPod
                 2 | patchCoreV1Node
                 2 | listCoreV1NamespacedPodTemplate
                 2 | deleteAppsV1NamespacedReplicaSet
                 2 | createApiregistrationV1APIService
                 2 | replaceCoreV1NamespacedReplicationController
                 2 | replaceCoreV1NamespacedSecret
                 2 | connectCoreV1GetNamespacedPodPortforward
                 1 | createCoreV1NamespacedLimitRange
                 1 | createAutoscalingV1NamespacedHorizontalPodAutoscaler
                 1 | replaceAdmissionregistrationV1MutatingWebhookConfiguration
                 1 | replaceAdmissionregistrationV1ValidatingWebhookConfiguration
                 1 | replaceApiextensionsV1CustomResourceDefinitionStatus
                 1 | replaceCoreV1NodeStatus
                 1 | replaceAppsV1NamespacedReplicaSet
                 1 | createAuthorizationV1SelfSubjectAccessReview
                 1 | replaceAppsV1NamespacedStatefulSetScale
                 1 | replaceCoordinationV1NamespacedLease
                 1 | createAuthenticationV1TokenReview
                 1 | createAppsV1NamespacedDaemonSet
                 1 | replaceCoreV1NamespacedLimitRange
                 1 | createAppsV1NamespacedControllerRevision
                 1 | replaceCoreV1NamespacedReplicationControllerScale
                 1 | replaceCoreV1NamespacedServiceAccount
                 1 | deleteAdmissionregistrationV1CollectionValidatingWebhookConfiguration
                 1 | listAdmissionregistrationV1MutatingWebhookConfiguration
                 1 | listAdmissionregistrationV1ValidatingWebhookConfiguration
                 1 | listApiextensionsV1CustomResourceDefinition
                 1 | listAppsV1NamespacedDaemonSet
                 1 | getApiextensionsAPIGroup
                 1 | getAdmissionregistrationAPIGroup
                 1 | replaceCoreV1Node
                 1 | listCoreV1Namespace
                 1 | deleteStorageV1VolumeAttachment
                 1 | deleteStorageV1CSINode
                 1 | listCoreV1NamespacedLimitRange
                 1 | deleteNetworkingV1NamespacedNetworkPolicy
                 1 | connectCoreV1GetNamespacedPodExec
                 1 | listCoreV1NamespacedSecret
                 1 | deleteCoreV1NamespacedPodTemplate
                 1 | deleteCoreV1NamespacedLimitRange
                 1 | listCoreV1PodForAllNamespaces
                 1 | deleteCoordinationV1CollectionNamespacedLease
                 1 | deleteAutoscalingV1NamespacedHorizontalPodAutoscaler
                 1 | logFileListHandler
                 1 | patchAdmissionregistrationV1MutatingWebhookConfiguration
                 1 | patchAdmissionregistrationV1ValidatingWebhookConfiguration
                 1 | patchApiextensionsV1CustomResourceDefinitionStatus
                 1 | patchCoordinationV1NamespacedLease
                 1 | patchCoreV1NamespacedConfigMap
                 1 | patchCoreV1NamespacedPodStatus
                 1 | readAdmissionregistrationV1MutatingWebhookConfiguration
                 1 | readAdmissionregistrationV1ValidatingWebhookConfiguration
                 1 | readApiextensionsV1CustomResourceDefinitionStatus
                 1 | readApiregistrationV1APIService
                 1 | deleteAppsV1NamespacedDaemonSet
                 1 | readAppsV1NamespacedStatefulSetScale
                 1 | deleteAppsV1NamespacedControllerRevision
                 1 | deleteApiextensionsV1CollectionCustomResourceDefinition
                 1 | readCoreV1NamespacedLimitRange
                 1 | deleteAdmissionregistrationV1CollectionMutatingWebhookConfiguration
                 1 | createStorageV1VolumeAttachment
                 1 | createStorageV1CSINode
                 1 | readCoreV1NamespacedReplicationControllerScale
                 1 | createNetworkingV1NamespacedNetworkPolicy
   (183 rows)

   #+end_src
*** Sanity Check   
   And another sanity check.
   #+NAME: distinct tests hitting endpoint, should be 9
   #+begin_src sql-mode
              SELECT distinct
                useragent
                FROM
                    audit_event
               WHERE
     operation_id = 'listCoreV1PersistentVolume'
                 AND useragent like 'e2e.test%'
                 ;
   #+end_src
   

   #+RESULTS: distinct tests hitting endpoint, should be 9
   #+begin_src sql-mode
                                                                                                                   useragent                                                                                                                 
   ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should adopt matching orphans and release non-matching pods
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should implement legacy replacement when the update strategy is OnDelete
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should not deadlock when a pod's predecessor fails
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications with PVCs
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should provide basic identity
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
   (9 rows)

   #+end_src

   #+NAME: distinct tests hitting endpoint, should be 1
   #+begin_src sql-mode
              SELECT distinct
                useragent
                FROM
                    audit_event
               WHERE
     operation_id = 'readCoreV1NamespacedLimitRange'
                 AND useragent like 'e2e.test%'
                 ;
   #+end_src

   #+RESULTS: distinct tests hitting endpoint, should be 1
   #+begin_src sql-mode
                                                                                 useragent                                                                               
   ----------------------------------------------------------------------------------------------------------------------------------------------------------------------
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied.
   (1 row)

   #+end_src
   
   This feels good. There's a good number of endpoints that are only hit by 1 or 2 tests, so less than 5 would likely be a good filter. 

** List the focused tests
   Next, I wanted a query of focused test/endpoint pairings.  To do this, we'd need tests and the endpoints they hit and vice versa.

   First, I can list all test/endpoint pairs.  This will end up being a long query, and so will not publish the results.

   #+NAME: focused tests 
   #+begin_src sql-mode
     WITH tests as (
       SELECT DISTINCT
         COUNT(distinct operation_id) FILTER(where useragent = audit_event.useragent) as distinct_endpoints,
         split_part(useragent, '--', 2) as test,
         useragent
         FROM
             audit_event
        WHERE
              useragent LIKE 'e2e.test%'
          AND job != 'live'
        GROUP BY useragent
     )
     SELECT DISTINCT
       audit_event.operation_id,
       test, 
       tests.distinct_endpoints
       FROM tests
         JOIN
         audit_event on (audit_event.useragent = tests.useragent)
      WHERE distinct_endpoints < 14
       ORDER BY distinct_endpoints asc, test
            ;
   #+end_src

** List distinct endpoints
Similarly, we want to list the endpoints and the tests that hit them.

Both of these lists will become CTE's in our larger postgres query, they are being defined for context here.

   #+NAME: List Low Tested Endpoints
   #+begin_src sql-mode
     WITH stable_endpoints AS (
     SELECT
          COUNT(distinct ae.useragent) FILTER(where ae.operation_id = ae.operation_id) as distinct_tests,
            ae.operation_id
            FROM
            audit_event ae
            JOIN endpoint_coverage ec on (ae.operation_id = ec.operation_id)
            WHERE
            useragent LIKE 'e2e.test%'
            AND ae.job != 'live'
            AND ec.level = 'stable'
            GROUP BY ae.operation_id
     )
     SELECT DISTINCT
       stable_endpoints.operation_id,
       split_part(ae.useragent, '--', 2) as test,
       distinct_tests
       FROM
           stable_endpoints
           JOIN
           audit_event ae on (ae.operation_id = stable_endpoints.operation_id)
           WHERE distinct_tests < 5
            AND ae.useragent like 'e2e.test%'
            ORDER BY operation_id
            ;
   #+end_src

** Combine our queries together
   
   With the smaller parts in place, I could now combine the two tables. We can select focused tests, then intersect it with focused endpoints.  This would return the rows that are in both the top and bottom, or the pairings of focused tests and their focused endpoints.

   We can adjust the results returned by setting different numbers in the where clause of either of our selections.  I wanted to start very narrow, looking at tests that hit less than 5 endpoints, and the endpoints hit by less than 5 tests.

I added an additional filter for the endpoints.  Since we want to increase coverage, we aren't concerned with endpoints that are already hit by conformance tests.  So I will remove any endpoints whose useragent includes ~[Conformance]~.

   #+NAME: focused tests and endpoints
   #+begin_src sql-mode
     -- setup the tests
           WITH tests as (
             SELECT DISTINCT
               COUNT(distinct operation_id) FILTER(where useragent = audit_event.useragent) as distinct_endpoints,
               split_part(useragent, '--', 2) as test,
               useragent
               FROM
                   audit_event
              WHERE
                    useragent LIKE 'e2e.test%'
                AND job != 'live'
              GROUP BY useragent
           )
     -- setup the endpoints
               , stable_endpoints AS (
           SELECT
                COUNT(distinct ae.useragent) FILTER(where ae.operation_id = ae.operation_id) as distinct_tests,
                  ae.operation_id
                  FROM
                  audit_event ae
                  JOIN endpoint_coverage ec on (ae.operation_id = ec.operation_id)
                  WHERE
                  useragent LIKE 'e2e.test%'
                  AND ae.job != 'live'
                  AND ec.level = 'stable'
                  GROUP BY ae.operation_id
           )
     -- select focused tests.
           (SELECT DISTINCT
             audit_event.operation_id,
             test
             FROM tests
               JOIN
               audit_event on (audit_event.useragent = tests.useragent)
            WHERE distinct_endpoints < 5) --adjust to narrow or widen filter.
             INTERSECT
     --select focused endpoints.
           (SELECT DISTINCT
             stable_endpoints.operation_id,
             split_part(ae.useragent, '--', 2) as test
             FROM
                 stable_endpoints
                 JOIN
                 audit_event ae on (ae.operation_id = stable_endpoints.operation_id)
                 WHERE distinct_tests < 5 -- adjust to narrow or widen filter.
                  AND ae.useragent not like '%[Conformance]%'
      )
                  ;
   #+end_src

   #+RESULTS: focused tests and endpoints
   #+begin_src sql-mode
           operation_id         |                                                                    test                                                                    
   -----------------------------+--------------------------------------------------------------------------------------------------------------------------------------------
    listCoreV1NamespacedService |  [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/json"
    listCoreV1NamespacedService |  [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/json,application/vnd.kubernetes.protobuf"
    listCoreV1NamespacedService |  [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/vnd.kubernetes.protobuf"
    listCoreV1NamespacedService |  [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/vnd.kubernetes.protobuf,application/json"
   (4 rows)

   #+end_src


*** Verify Results
    This result is as expected on the tests side, since our distribution showed only a few tests that hit 3 endpoints...with the next set of tests being 6 or 7 endpoints.
   
    This query is a bit tangly, and the results low enough, that I could do a blunt check for each test to ensure they are hitting 5 or less endpoints including ~listCoreV1NamespacedService~.  I would expecct each of them to only hit 3 endpoints.
 
    #+NAME: Investigating test 1
    #+begin_src sql-mode
      SELECT DISTINCT
        operation_id
        FROM audit_event
       WHERE
         useragent like '%[sig-api-machinery] client-go should negotiate watch and report errors with accept "application/json"'
    ;
    #+end_src

    #+RESULTS: Investigating test 1
    #+begin_src sql-mode
            operation_id         
    -----------------------------
     listCoreV1NamespacedService
     listCoreV1Node
     readCoreV1NamespacedService
    (3 rows)

    #+end_src
   
    #+NAME: Investigating test 2
    #+begin_src sql-mode
      SELECT DISTINCT
        operation_id
        FROM audit_event
       WHERE
         useragent like '%[sig-api-machinery] client-go should negotiate watch and report errors with accept "application/json,application/vnd.kubernetes.protobuf"'
    ;
    #+end_src

    #+RESULTS: Investigating test 2
    #+begin_src sql-mode
            operation_id         
    -----------------------------
     listCoreV1NamespacedService
     listCoreV1Node
     readCoreV1NamespacedService
    (3 rows)

    #+end_src
   
    I have a feeling they are all gonna be three, and the same three. but let's be explicit.
   
    #+NAME: Investigating test 3
    #+begin_src sql-mode
        SELECT DISTINCT
          operation_id
          FROM audit_event
         WHERE
           useragent like '%[sig-api-machinery] client-go should negotiate watch and report errors with accept "application/vnd.kubernetes.protobuf,application/json"'
      ;
    #+end_src

    #+RESULTS: Investigating test 3
    #+begin_src sql-mode
            operation_id         
    -----------------------------
     listCoreV1NamespacedService
     listCoreV1Node
     readCoreV1NamespacedService
    (3 rows)

    #+end_src
   
    #+NAME: Investigating test 4
    #+begin_src sql-mode
        SELECT DISTINCT
          operation_id
          FROM audit_event
         WHERE
           useragent like '%[sig-api-machinery] client-go should negotiate watch and report errors with accept "application/vnd.kubernetes.protobuf"'
      ;
    #+end_src

    #+RESULTS: Investigating test 4
    #+begin_src sql-mode
            operation_id         
    -----------------------------
     listCoreV1NamespacedService
     listCoreV1Node
     readCoreV1NamespacedService
    (3 rows)

    #+end_src
   
    As we expected!  Similarly, we want to make sure ~listCoreV1NamespacedService~ is hit by less than 5 tests.
   
    #+NAME: Investigating endpoint
    #+begin_src sql-mode
        SELECT DISTINCT
          split_part(ae.useragent, '--',2) as test
          FROM audit_event ae
         WHERE
           ae.operation_id = 'listCoreV1NamespacedService'
           AND useragent like 'e2e.test%'
      ;
    #+end_src

    #+RESULTS: Investigating endpoint
    #+begin_src sql-mode
                                                                        test                                                                    
    --------------------------------------------------------------------------------------------------------------------------------------------
      [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/json"
      [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/json,application/vnd.kubernetes.protobuf"
      [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/vnd.kubernetes.protobuf"
      [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/vnd.kubernetes.protobuf,application/json"
    (4 rows)

    #+end_src
   
    Beautiful.  It's the same results as before, of course, but it let us know our combined query was working.  

** Listing A larger set of Focused Tests and Endpoints
   What does it look like when I set wider filters?  For example, tests that hit less than 14 endpoints (the average).  
   
   #+NAME: Test that hit less than 14 endpoints
   #+begin_src sql-mode
     -- setup the tests
           WITH tests as (
             SELECT DISTINCT
               COUNT(distinct operation_id) FILTER(where useragent = audit_event.useragent) as distinct_endpoints,
               split_part(useragent, '--', 2) as test,
               useragent
               FROM
                   audit_event
              WHERE
                    useragent LIKE 'e2e.test%'
                AND job != 'live'
              GROUP BY useragent
           )
     -- setup the endpoints
               , stable_endpoints AS (
           SELECT
                COUNT(distinct ae.useragent) FILTER(where ae.operation_id = ae.operation_id) as distinct_tests,
                  ae.operation_id
                  FROM
                  audit_event ae
                  JOIN endpoint_coverage ec on (ae.operation_id = ec.operation_id)
                  WHERE
                  useragent LIKE 'e2e.test%'
                  AND ae.job != 'live'
                  AND ec.level = 'stable'
                  GROUP BY ae.operation_id
           )
     -- select focused tests.
           (SELECT DISTINCT
             audit_event.operation_id,
             test
             FROM tests
               JOIN
               audit_event on (audit_event.useragent = tests.useragent)
            WHERE distinct_endpoints < 14) --adjust to narrow or widen filter.
             INTERSECT
     --select focused endpoints.
           (SELECT DISTINCT
             stable_endpoints.operation_id,
             split_part(ae.useragent, '--', 2) as test
             FROM
                 stable_endpoints
                 JOIN
                 audit_event ae on (ae.operation_id = stable_endpoints.operation_id)
                 WHERE distinct_tests < 5 -- adjust to narrow or widen filter.
                  AND ae.useragent not like '%[Conformance]%'
      )
               ORDER BY test
                  ;
   #+end_src

   #+RESULTS: Test that hit less than 14 endpoints
   #+begin_src sql-mode
                  operation_id               |                                                                    test                                                                    
   ------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------
    listAppsV1NamespacedDaemonSet            | 
    listCoordinationV1NamespacedLease        |  [k8s.io] NodeLease when the NodeLease feature is enabled should have OwnerReferences set
    readCoordinationV1NamespacedLease        |  [k8s.io] NodeLease when the NodeLease feature is enabled the kubelet should create and update a lease in the kube-node-lease namespace
    readCoordinationV1NamespacedLease        |  [k8s.io] NodeLease when the NodeLease feature is enabled the kubelet should report node status infrequently
    patchCoreV1NamespacedPodStatus           |  [k8s.io] Pods should support pod readiness gates [NodeFeature:PodReadinessGate]
    listCoreV1PodForAllNamespaces            |  [k8s.io] [sig-node] NodeProblemDetector [DisabledForLargeClusters] should run without error
    listCoreV1NamespacedService              |  [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/json"
    listCoreV1NamespacedService              |  [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/json,application/vnd.kubernetes.protobuf"
    listCoreV1NamespacedService              |  [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/vnd.kubernetes.protobuf"
    listCoreV1NamespacedService              |  [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/vnd.kubernetes.protobuf,application/json"
    listBatchV1NamespacedJob                 |  [sig-api-machinery] Garbage collector should delete jobs and pods created by cronjob
    createCoreV1NamespacedPodTemplate        |  [sig-api-machinery] Servers with support for API chunking should return chunks of results for list calls
    listCoreV1NamespacedPodTemplate          |  [sig-api-machinery] Servers with support for API chunking should return chunks of results for list calls
    createCoreV1NamespacedPodTemplate        |  [sig-api-machinery] Servers with support for Table transformation should return chunks of table results for list calls
    listCoreV1NamespacedPodTemplate          |  [sig-api-machinery] Servers with support for Table transformation should return chunks of table results for list calls
    listBatchV1NamespacedJob                 |  [sig-apps] CronJob should not emit unexpected warnings
    listBatchV1NamespacedJob                 |  [sig-apps] CronJob should replace jobs when ReplaceConcurrent
    listBatchV1NamespacedJob                 |  [sig-apps] CronJob should schedule multiple jobs concurrently
    replaceAppsV1NamespacedReplicaSet        |  [sig-apps] ReplicaSet should surface a failure condition on a common issue like exceeded quota
    readAppsV1NamespacedReplicaSet           |  [sig-apps] ReplicaSet should surface a failure condition on a common issue like exceeded quota
    replaceCoreV1NamespacedServiceAccount    |  [sig-auth] ServiceAccounts should ensure a single API token exists
    readCoreV1NamespacedServiceAccount       |  [sig-auth] ServiceAccounts should ensure a single API token exists
    connectCoreV1GetNamespacedPodPortforward |  [sig-cli] Kubectl Port forwarding With a server listening on 0.0.0.0 should support forwarding over websockets
    connectCoreV1GetNamespacedPodPortforward |  [sig-cli] Kubectl Port forwarding With a server listening on localhost should support forwarding over websockets
    getCoreAPIVersions                       |  [sig-network] Networking should provide unchanging, static URL paths for kubernetes api services
    logFileListHandler                       |  [sig-network] Networking should provide unchanging, static URL paths for kubernetes api services
   (26 rows)

   #+end_src
   
   A decent number of results.  I am interested in what people who knows these tests well think of a selection like this, and whether they're likely well-focused tests or if it's random.

** Checking Whether our chosen tests are valid for promotion   
  
   
   So let's say we pick a test from the selection above.  Is there a rigorous way we could check if it's a good candidate for promotion?

   
   The k8s community provides[[https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/conformance-tests.md#conformance-test-requirements][ A guide for Conformance Tests]], which is (as of 20 December):
   
   #+begin_quote
   Conformance tests currently test only GA, non-optional features or APIs. More specifically, a test is eligible for promotion to conformance if:

    it tests only GA, non-optional features or APIs (e.g., no alpha or beta endpoints, no feature flags required, no deprecated features)
    it does not require direct access to kubelet's API to pass (nor does it require indirect access via the API server node proxy endpoint); it MAY use the kubelet API for debugging purposes upon failure
    it works for all providers (e.g., no SkipIfProviderIs/SkipUnlessProviderIs calls)
    it is non-privileged (e.g., does not require root on nodes, access to raw network interfaces, or cluster admin permissions)
    it works without access to the public internet (short of whatever is required to pre-pull images for conformance tests)
    it works without non-standard filesystem permissions granted to pods
    it does not rely on any binaries that would not be required for the linux kernel or kubelet to run (e.g., can't rely on git)
    where possible, it does not depend on outputs that change based on OS (nslookup, ping, chmod, ls)
    any container images used within the test support all architectures for which kubernetes releases are built
    it passes against the appropriate versions of kubernetes as spelled out in the conformance test version skew policy
    it is stable and runs consistently (e.g., no flakes), and has been running for at least two weeks
    new conformance tests or updates to conformance tests for additional scenarios are only allowed before code freeze dates set by the release team to allow enough soak time of the changes and gives folks a chance to kick the tires either in the community CI or their own infrastructure to make sure the tests are robust
    it has a name that is a literal string

Examples of features which are not currently eligible for conformance tests:

    node/platform-reliant features, eg: multiple disk mounts, GPUs, high density, etc.
    optional features, eg: policy enforcement
    cloud-provider-specific features, eg: GCE monitoring, S3 Bucketing, etc.
    anything that requires a non-default admission plugin

Conformance tests are intended to be stable and backwards compatible according to the standard API deprecation policies. Therefore any test that relies on specific output that is not subject to the deprecation policy cannot be promoted to conformance. Examples of tests which are not eligible to conformance:

    anything that checks specific Events are generated, as we make no guarantees about the contents of events, nor their delivery
        If a test depends on events it is recommended to change the test to use an informer pattern and watch specific resource changes instead.
    anything that checks optional Condition fields, such as Reason or Message, as these may change over time (however it is reasonable to verify these fields exist or are non-empty)
        If the test is checking for specific conditions or reasons, it is considered overly specific and it is recommended to simply look for pass/failure criteria where possible, and output the condition/reason for debugging purposes only.

Examples of areas we may want to relax these requirements once we have a sufficient corpus of tests that define out of the box functionality in all reasonable production worthy environments:

    tests may need to create or set objects or fields that are alpha or beta that bypass policies that are not yet GA, but which may reasonably be enabled on a conformant cluster (e.g., pod security policy, non-GA scheduler annotations)

   #+end_quote
   
   We could break these requirements into a TODO checklist, to ensure we are going through the full guide with each test.
   
   For an example, let's look at the test from our highly focused query: 
   : [sig-api-machinery] client-go should negotiate watch and report errors with accept *
   The test is written in ~test/e2e/apimachinery/protocol.go~ 
   This file is as follows:
   #+NAME: protocol.go
   #+begin_quote go
  /*
Copyright 2019 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package apimachinery

import (
	"fmt"
	"strconv"

	g "github.com/onsi/ginkgo"
	o "github.com/onsi/gomega"

	"k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/watch"
	"k8s.io/client-go/kubernetes"

	"k8s.io/kubernetes/test/e2e/framework"
)

var _ = SIGDescribe("client-go should negotiate", func() {
	f := framework.NewDefaultFramework("protocol")
	f.SkipNamespaceCreation = true

	for _, s := range []string{
		"application/json",
		"application/vnd.kubernetes.protobuf",
		"application/vnd.kubernetes.protobuf,application/json",
		"application/json,application/vnd.kubernetes.protobuf",
	} {
		accept := s
		g.It(fmt.Sprintf("watch and report errors with accept %q", accept), func() {
			cfg, err := framework.LoadConfig()
			framework.ExpectNoError(err)

			cfg.AcceptContentTypes = accept

			c := kubernetes.NewForConfigOrDie(cfg)
			svcs, err := c.CoreV1().Services("default").Get("kubernetes", metav1.GetOptions{})
			framework.ExpectNoError(err)
			rv, err := strconv.Atoi(svcs.ResourceVersion)
			framework.ExpectNoError(err)
			w, err := c.CoreV1().Services("default").Watch(metav1.ListOptions{ResourceVersion: strconv.Itoa(rv - 1)})
			framework.ExpectNoError(err)
			defer w.Stop()

			evt, ok := <-w.ResultChan()
			o.Expect(ok).To(o.BeTrue())
			switch evt.Type {
			case watch.Added, watch.Modified:
				// this is allowed
			case watch.Error:
				err := errors.FromObject(evt.Object)
				if errors.IsGone(err) {
					// this is allowed, since the kubernetes object could be very old
					break
				}
				if errors.IsUnexpectedObjectError(err) {
					g.Fail(fmt.Sprintf("unexpected object, wanted v1.Status: %#v", evt.Object))
				}
				g.Fail(fmt.Sprintf("unexpected error: %#v", evt.Object))
			default:
				g.Fail(fmt.Sprintf("unexpected type %s: %#v", evt.Type, evt.Object))
			}
		})
	}
})
   #+end_quote
   
*** PASSED it tests only GA, non-optional features or APIs (e.g., no alpha or beta endpoints, no feature flags required, no deprecated features)
    We can check this using our api queries.  We'll run through all op_id's that our test touches, and check their properties and parameters. 
    If any op_id is alpha or beta, or deprecated, then the test is not a candidate for promotion.
    If any op_id has a required parameter that is turned on with a feature gate, it is also not a candidate for promotion.
    
    #+NAME: Tests only GA, non opt features or APIs
    #+begin_src sql-mode
      SELECT DISTINCT
        ae.operation_id,
        ao.level,
        ao.deprecated,
        ap.param_name,
        ap.required,
        CASE
            WHEN (ap.param_description ~ 'feature gate') then true
        ELSE false
            END as feature_gate
        FROM audit_event ae
               JOIN api_operation_material ao on (ae.operation_id = ao.operation_id)
               JOIN api_operation_parameter_material ap on (ae.operation_id = ap.param_op)
       WHERE
         ae.useragent like '%[sig-api-machinery] client-go should negotiate watch and report errors with accept "application/json"'
         ;
    #+end_src

    #+RESULTS: Tests only GA, non opt features or APIs
    #+begin_src sql-mode
            operation_id         | level  | deprecated |     param_name      | required | feature_gate 
    -----------------------------+--------+------------+---------------------+----------+--------------
     listCoreV1NamespacedService | stable | f          | allowWatchBookmarks | f        | t
     listCoreV1NamespacedService | stable | f          | continue            | f        | f
     listCoreV1NamespacedService | stable | f          | fieldSelector       | f        | f
     listCoreV1NamespacedService | stable | f          | labelSelector       | f        | f
     listCoreV1NamespacedService | stable | f          | limit               | f        | f
     listCoreV1NamespacedService | stable | f          | resourceVersion     | f        | f
     listCoreV1NamespacedService | stable | f          | timeoutSeconds      | f        | f
     listCoreV1NamespacedService | stable | f          | watch               | f        | f
     listCoreV1Node              | stable | f          | allowWatchBookmarks | f        | t
     listCoreV1Node              | stable | f          | continue            | f        | f
     listCoreV1Node              | stable | f          | fieldSelector       | f        | f
     listCoreV1Node              | stable | f          | labelSelector       | f        | f
     listCoreV1Node              | stable | f          | limit               | f        | f
     listCoreV1Node              | stable | f          | resourceVersion     | f        | f
     listCoreV1Node              | stable | f          | timeoutSeconds      | f        | f
     listCoreV1Node              | stable | f          | watch               | f        | f
     readCoreV1NamespacedService | stable | f          | exact               | f        | f
     readCoreV1NamespacedService | stable | f          | export              | f        | f
    (18 rows)

    #+end_src
     
    It all looks good with the only question being allowWatchBookmarks, which is a non-required feature_gate.  In the [[https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/#feature-gates][k8s guide to feature gates]], we can see that watchBookmarks has a default set to true as of v. 1.17.  So it looks like allowWatchBookmarks would be a parameter by default, and the test does not require the enabling of a feature gate.  
    
    From this perspective, it looks like the test would pass.
*** PASSED it has a name that is a literal string
    We can see this from the fact that it shows correctly in our query.
*** PASSED it is stable and runs consistently (e.g., no flakes), and has been running for at least two weeks
    For this, we will want to use a combination of github and testgrid.  Github will help us see when the test was added to e2e, based on when its file was merged into the codebase.
     
    Looking at the file in github, it was merged over a month ago, and so has been a part of e2e for over two weeks.
     
    If we take a look at the test in testgrid, it is solid green and does not appear to be flaky.
     
    It looks like it'd pass for this too.
*** PASSED it works for all providers (e.g., no SkipIfProviderIs/SkipUnlessProviderIs calls)
    We can verify this reading through the code quoted above.
*** TEST it does not require direct access to kubelet's API to pass (nor does it require indirect access via the API server node proxy endpoint); it MAY use the kubelet API for debugging purposes upon failure
*** TEST it is non-privileged (e.g., does not require root on nodes, access to raw network interfaces, or cluster admin permissions)
*** TEST it works without access to the public internet (short of whatever is required to pre-pull images for conformance tests)
*** TEST it works without non-standard filesystem permissions granted to pods
*** TEST it does not rely on any binaries that would not be required for the linux kernel or kubelet to run (e.g., can't rely on git)
*** TEST where possible, it does not depend on outputs that change based on OS (nslookup, ping, chmod, ls)
*** TEST any container images used within the test support all architectures for which kubernetes releases are built
*** TEST it passes against the appropriate versions of kubernetes as spelled out in the conformance test version skew policy
    
* Conclusions | Next Steps
* Footnotes
** Bringing Open PR's into the database
   One of the first important questions to ask on promotion is "is someone else already doing this?"  With backlog, there may already be an open PR for promoting the tests you're looking at, and you don't want to spend time doing already done work.  However, I was finding it difficult navigating quickly through the issues to see whether or not a PR was made.  The ieal, I think, would be to look for open PR's tagged area/conformance whose files changed include the test you are looking at.  This would give a high indication that it deals with promotion.  This is not something you can do easily with the advanced search or the api of github, but it _is_ somethign we could do with a postgres database.
   
   And so, let's bring github into our db! (or at least a lil portion).
*** Create Python function for fetching from github.   
    #+NAME: ping github api with python
    #+begin_src python :results output
      import requests
      import json

      query = """
      query{
        search(query: "repo:kubernetes/kubernetes label:area/conformance state:open type:pr in:title Promote" type: ISSUE, first: 100) {
          nodes {
            ...on PullRequest {
              state
              title
              id
              files(first: 100) {
                edges {
                  node {
                    path
                  }
                }
              }
            }
          }
        }
      }
      """

      GH_TOKEN = "d31d95980fbc9e8cef5b848a60e79452975da04c"
      headers = {'Authorization': 'token ' + GH_TOKEN}
      url = 'https://api.github.com/graphql'

      r = requests.post(url, json={'query': query}, headers=headers)
      raw_data = json.loads(r.text)['data']['search']['nodes']
      data = [entry for entry in raw_data if 'title' in entry]
      for entry in data:
        print(map(lambda x: x['node']['path'], entry['files']['edges']))
    #+end_src

    #+RESULTS: ping github api with python
    #+begin_src python
    #+end_src

 Great, this will get us all open area/conformacne pull requests.  We mcan work to expand this to be all pull requests and such, but we are going to hit rate limiting issues. the results we want. 

 Now we want a table we can insert this into, following the same style as our `load_bucket_job_swagger`

*** Create PR Table
    #+NAME: open_pull_requests
    #+begin_src sql-mode
      CREATE TABLE open_pull_requests(
        title text,
        id text unique,
        changed_files text[],
        PRIMARY KEY(id)
       ); 
    #+end_src

    #+RESULTS: open_pull_requests
    #+begin_src sql-mode
    ERROR:  relation "open_pull_requests" already exists
    #+end_src

*** Function to insert PR's into table
    #+NAME: load_open_pull_requests.py
    #+begin_src python
      try:
          import requests
          import json
          from string import Template

          query = """
               query{
             search(query: "repo:kubernetes/kubernetes label:area/conformance state:open type:pr in:title Promote" type: ISSUE, first: 100) {
               nodes {
                 ...on PullRequest {
                   state
                   title
                   number
                   permalink
                   files(first: 100) {
                     edges {
                       node {
                         path
                       }
                     }
                   }
                 }
               }
             }
           }
          """

          GH_TOKEN = "d31d95980fbc9e8cef5b848a60e79452975da04c"
          headers = {'Authorization': 'token ' + GH_TOKEN}
          url = 'https://api.github.com/graphql'

          r = requests.post(url, json={'query': query}, headers=headers)
          raw_data = json.loads(r.text)['data']['search']['nodes']
          data = [entry for entry in raw_data if 'title' in entry]
          for entry in data:
             files_changed = list(map(lambda x: x['node']['path'], entry['files']['edges']))
             sql = """
             INSERT INTO open_pull_requests(
             title,
             id,
             changed_files
             )
             SELECT
             $1 as title,
             $2 as id,
             $3 as changed_files
             """
             plan = plpy.prepare(sql, ['text','text', 'text[]'])
             rv = plpy.execute(plan, [
                 entry['title'],
                 entry['id'],
                 files_changed
             ])
          return 'successfully added some pull requests!'
      except Exception as err:
          return Template("something went wrong, likely this: ${error}").substitute(error = err)
    #+end_src

   
    #+NAME: load_pull_requests.sql
    #+begin_src sql-mode
      set role dba;
      CREATE OR REPLACE FUNCTION load_open_pull_requests()
      RETURNS text AS $$
      <<load_open_pull_requests.py>>
      $$ LANGUAGE plpython3u ;
      reset role;
    #+end_src

    #+RESULTS: load_pull_requests.sql
    #+begin_src sql-mode
    SET
    apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# CREATE FUNCTION
    RESET
    #+end_src

    #+begin_src sql-mode
      select * from load_open_pull_requests();
    #+end_src

    #+RESULTS:
    #+begin_src sql-mode
      load_open_pull_requests                                           
      -------------------------------------------------------------------------------------------------------------
       something went wrong, likely this: duplicate key value violates unique constraint "open_pull_requests_pkey"
      (1 row)

    #+end_src
   
    #+begin_src sql-mode
    select id, changed_files from open_pull_requests;
    #+end_src

    #+RESULTS:
    #+begin_src sql-mode
                    id                |                                  changed_files                                  
    ----------------------------------+---------------------------------------------------------------------------------
     MDExOlB1bGxSZXF1ZXN0MzIzNDkyNTY4 | {test/conformance/testdata/conformance.txt,test/e2e/scheduling/preemption.go}
     MDExOlB1bGxSZXF1ZXN0MzM2NDE0Mzc2 | {test/conformance/testdata/conformance.txt,test/e2e/apps/disruption.go}
     MDExOlB1bGxSZXF1ZXN0MzM5ODI3Njky | {test/conformance/testdata/conformance.txt,test/e2e/common/expansion.go}
     MDExOlB1bGxSZXF1ZXN0MzQzNzAyMzA2 | {test/conformance/testdata/conformance.txt,test/e2e/common/container_probe.go}
     MDExOlB1bGxSZXF1ZXN0MzAwOTMzMTYz | {test/conformance/testdata/conformance.txt,test/e2e/scheduling/preemption.go}
     MDExOlB1bGxSZXF1ZXN0MzI2MDQ2MDg5 | {test/conformance/testdata/conformance.txt,test/e2e/scheduling/predicates.go}
     MDExOlB1bGxSZXF1ZXN0MjY5NjEyMjky | {test/conformance/testdata/conformance.txt,test/e2e/network/service.go}
     MDExOlB1bGxSZXF1ZXN0MzQwMTg1NjI5 | {test/conformance/testdata/conformance.txt,test/e2e/storage/csi_mock_volume.go}
    (8 rows)

    #+end_src
   
    With this, we can run a query like the one below, to check whether there's any open conformance pr's that involve the test file:
    
    #+NAME: Confirmed File IS IN open PR's
    #+begin_src sql-mode
      SELECT
        title,
        id,
        changed_files
        FROM
            open_pull_requests
       WHERE
         'test/e2e/scheduling/preemption.go' = ANY(changed_files)
         ;
    #+end_src
    
    Based on our list of id's and files, the above query should return 2 results.
    
    #+RESULTS: Confirmed File IS IN open PR's
    #+begin_src sql-mode
                        title                    |                id                |                                 changed_files                                 
    ---------------------------------------------+----------------------------------+-------------------------------------------------------------------------------
     Promote pod preemption verification         | MDExOlB1bGxSZXF1ZXN0MzIzNDkyNTY4 | {test/conformance/testdata/conformance.txt,test/e2e/scheduling/preemption.go}
     Promote preemption e2e tests to Conformance | MDExOlB1bGxSZXF1ZXN0MzAwOTMzMTYz | {test/conformance/testdata/conformance.txt,test/e2e/scheduling/preemption.go}
    (2 rows)

    #+end_src
   
** Bringing All PR's into the database
   So we know we can bring in PR's using the v4 api.  Now, we want to extend this to include _all area/test pull requests_.  this would include open and merged.  This would let us see when a test was added to e2e, easily checking off one of the conformance requirements.  It'd also let us see if there's a current PR for it to be promoted, which would potentiallys ave a lot of wassted work.
   
   So what we want then is a loop.  We grab the results, and if 'HasNextPage' is true, we recurse with the same function but now with the end cursor adjusted.
*** Create PR Table
    #+NAME: pull_requests
    #+begin_src sql-mode
      CREATE TABLE pull_requests(
        title text,
        id text unique,
        number text,
        permalink text,
        changed_files text[],
        PRIMARY KEY(id)
       ); 
    #+end_src

    #+RESULTS: pull_requests
    #+begin_src sql-mode
    DROP TABLE
    CREATE TABLE
    #+end_src

*** Function to insert PR's into table
   #+NAME: insert_requests 
   #+begin_src python
     def insert_requests (pull_requests):
         for entry in pull_requests:
             files_changed = list(map(lambda x: x['node']['path'], entry['files']['edges']))
             sql = """
            INSERT INTO open_pull_requests(
            title,
            id,
            changed_files
            )
            SELECT
            $1 as title,
            $2 as id,
            $3 as changed_files
            """
             plan = plpy.prepare(sql, ['text','text', 'text[]'])
             rv = plpy.execute(plan, [
                 entry['title'],
                 entry['id'],
                 files_changed
             ])
            return 'pr batch inserted'
   #+end_src
    
    #+NAME: load_pull_requests.py
    #+begin_src python :results output
      import requests
      import json
      from string import Template

      GH_TOKEN = "9becb9617f00d34e4cc49f582dc0b47a5c5d1e02"
      headers = {'Authorization': 'token ' + GH_TOKEN}
      url = 'https://api.github.com/graphql'

      def insert_requests (pull_requests):
          for entry in pull_requests:
              files_changed = list(map(lambda x: x['node']['path'], entry['files']['edges']))
              sql = """
          INSERT INTO open_pull_requests(
          title,
          id,
          changed_files
          )
          SELECT
          $1 as title,
          $2 as id,
          $3 as changed_files
          """
              plan = plpy.prepare(sql, ['text','text', 'text[]'])
              rv = plpy.execute(plan, [
                  entry['title'],
                  entry['id'],
                  files_changed
              ])
          return 'pr batch inserted'


      def fetch_next_requests (cursor):
          if cursor is None:
              query = """
              query{
                  search(query: "repo:kubernetes/kubernetes label:area/test type:pr", type: ISSUE, first: 100) {
                      pageInfo {
                      hasNextPage
                      endCursor
                      }
                      edges {
                      node {
                          ... on PullRequest {
                          title
                          id
                          state
                          merged
                          mergedAt
                          labels(first: 50) {
                              edges {
                              node {
                                  name
                              }
                              }
                          }
                          files(first: 100) {
                              edges {
                              node {
                                  path
                              }
                              }
                          }
                          }
                      }
                      }
                  }
              }
              """
          else:
              query = Template("""
              query{
                  search(query: "repo:kubernetes/kubernetes label:area/test type:pr", type: ISSUE, first: 100, after: "$cursor") {
                      pageInfo {
                      hasNextPage
                      endCursor
                      }
                      edges {
                      node {
                          ... on PullRequest {
                          title
                          id
                          state
                          merged
                          mergedAt
                          labels(first: 50) {
                              edges {
                              node {
                                  name
                              }
                              }
                          }
                          files(first: 100) {
                              edges {
                              node {
                                  path
                              }
                              }
                          }
                          }
                      }
                      }
                  }
              }
              """).substitute(cursor=cursor)
              print('query', query)

          r = requests.post(url, json={'query': query}, headers=headers)
          raw_data = json.loads(r.text)['data']['search']
          has_next_page = raw_data['pageInfo']['hasNextPage']
          end_cursor = raw_data['pageInfo']['endCursor']
          edges = raw_data['edges']
          pull_requests = [node for node in edges if 'title' in node]
          insert_requests(pull_requests)

          if has_next_page is False:
              return 'all prs inserted to db'
          else:
              fetch_next_requests(end_cursor)
      try:
          fetch_next_requests(None)
      except Exception as err:
          raise err
    #+end_src

    #+RESULTS: load_pull_requests.py
    #+begin_src python
    #+end_src


    #+NAME: new load_pull_requests.sql
    #+begin_src sql-mode :results silent
      set role dba;
      CREATE OR REPLACE FUNCTION load_pull_requests()
      RETURNS text AS $$
      <<load_pull_requests.py>>
      $$ LANGUAGE plpython3u ;
      reset role;
    #+end_src


    #+begin_src sql-mode
      select * from load_pull_requests();
    #+end_src

    #+RESULTS:
    #+begin_src sql-mode
    ERROR:  TypeError: 'NoneType' object is not subscriptable
    CONTEXT:  Traceback (most recent call last):
      PL/Python function "load_pull_requests", line 123, in <module>
        raise err
      PL/Python function "load_pull_requests", line 121, in __plpython_procedure_load_pull_requests_31993
        fetch_next_requests(None)
      PL/Python function "load_pull_requests", line 119, in fetch_next_requests
        fetch_next_requests(end_cursor)
      PL/Python function "load_pull_requests", line 119, in fetch_next_requests
        fetch_next_requests(end_cursor)
      PL/Python function "load_pull_requests", line 119, in fetch_next_requests
        fetch_next_requests(end_cursor)
      PL/Python function "load_pull_requests", line 119, in fetch_next_requests
        fetch_next_requests(end_cursor)
      PL/Python function "load_pull_requests", line 119, in fetch_next_requests
        fetch_next_requests(end_cursor)
      PL/Python function "load_pull_requests", line 119, in fetch_next_requests
        fetch_next_requests(end_cursor)
      PL/Python function "load_pull_requests", line 119, in fetch_next_requests
        fetch_next_requests(end_cursor)
      PL/Python function "load_pull_requests", line 119, in fetch_next_requests
        fetch_next_requests(end_cursor)
      PL/Python function "load_pull_requests", line 119, in fetch_next_requests
        fetch_next_requests(end_cursor)
      PL/Python function "load_pull_requests", line 109, in fetch_next_requests
        raw_data = json.loads(r.text)['data']['search']
    PL/Python function "load_pull_requests"
    #+end_src

   
    #+begin_src sql-mode
    select id, changed_files from pull_requests limit 10;
    #+end_src

    #+RESULTS:
    #+begin_src sql-mode
     id | changed_files 
    ----+---------------
    (0 rows)

    #+end_src
   
    With this, we can run a query like the one below, to check whether there's any open conformance pr's that involve the test file:
    
    #+NAME: Confirmed File IS IN open PR's
    #+begin_src sql-mode
      SELECT
        title,
        id
        FROM
            open_pull_requests
       WHERE
         'test/e2e/scheduling/preemption.go' = ANY(changed_files)
         ;
    #+end_src
    
    Based on our list of id's and files, the above query should return 2 results.
    
    #+RESULTS: Confirmed File IS IN open PR's
    #+begin_src sql-mode
                        title                    |                id                
    ---------------------------------------------+----------------------------------
     Promote pod preemption verification         | MDExOlB1bGxSZXF1ZXN0MzIzNDkyNTY4
     Promote preemption e2e tests to Conformance | MDExOlB1bGxSZXF1ZXN0MzAwOTMzMTYz
    (2 rows)

    #+end_src
   


** 250: api_schema view
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/250_view_api_schema.up.sql
  :END:
*** Create

 #+NAME: api_schema view
 #+BEGIN_SRC sql-mode 
   CREATE OR REPLACE VIEW "public"."api_schema" AS 
    SELECT 
       bjs.bucket,
       bjs.job,
       d.key AS schema_name,
       (((d.value -> 'x-kubernetes-group-version-kind'::text) -> 0) ->> 'kind'::text) AS k8s_kind,
       (d.value ->> 'type'::text) AS resource_type,
       (((d.value -> 'x-kubernetes-group-version-kind'::text) -> 0) ->> 'version'::text) AS k8s_version,
       (((d.value -> 'x-kubernetes-group-version-kind'::text) -> 0) ->> 'group'::text) AS k8s_group,
       ARRAY(SELECT jsonb_array_elements_text(d.value -> 'required')) as required_fields,
       (d.value -> 'properties'::text) AS properties,
       d.value
      FROM bucket_job_swagger bjs
        , jsonb_each((bjs.swagger -> 'definitions'::text)) d(key, value)
      GROUP BY bjs.bucket, bjs.job, d.key, d.value;

 #+END_SRC

 #+RESULTS: api_schema view
 #+begin_src sql-mode
   CREATE VIEW
 #+end_src

** 260: api_schema_field view
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/260_view_api_schema_field.up.sql
  :END:
*** Create

    
