#+TITLE: 88: Enable Front ends
#+TODO: TODO IN-PROGRESS BLOCKED | TADA


* The Ticket
  [[https://gitlab.ii.coop/cncf/apisnoop/issues/89][gitlab link]]
  #+begin_quote
  When someone is on a cluster that has apisnoop applied, they can load up hasura's frontend that shows data from this cluster.  They can also load a custom website we create that shows their current endpoint hit by their test, and the projecte change in coverage if this test was merged into e2e.
Steps:


 Make it so Hasura frontend in cluster can be accessed through a port forward

 build a minimal Svelte app and add to cluster.

 ensure Svelte app can be accessed through port forward

 ensure Svelte app can fetch and display data from cluster's internal hasura endpoint.

 Integrate 'endpoints_hit_by_test' and 'projected_change_in_coverage' to hasura docker image.

 update Svelte app so it displays our endpoints hit by tests and our projected change in coverage.
  #+end_quote
* Process
** IN-PROGRESS Make Hasura api in cluster accessible through a port forward
   We can do it similar to how we forward our postgres port.
  #+BEGIN_SRC tmate :session foo:apisnoop
    HASURA_POD=$(kubectl get pod --selector=io.apisnoop.graphql=hasura -o name | sed s:pod/::)
    HASURA_PORT=$(kubectl get pod $HASURA_POD --template='{{(index (index .spec.containers 0).ports 0).containerPort}}{{"\n"}}')
    kubectl port-forward $HASURA_POD $(id -u)2:$HASURA_PORT
  #+END_SRC
  This will be an update to our workflow to include this step.
** IN-PROGRESS build a minimal Svelte app and add to cluster.
   This is living in apps in the root of this folder.
** TODO ensure Svelte app can be accessed through port forward
** TODO ensure Svelte app can fetch and display data from cluster's internal hasura endpoint.
** TODO Integrate 'endpoints_hit_by_test' and 'projected_change_in_coverage' to hasura docker image.
** TODO update Svelte app so it displays our endpoints hit by tests and our projected change in coverage.
* Conclusions || Next Steps
* Footnotes
- [X] Setup Port-Forwarding from us to sharing to the cluster

  We'll need to port forward.  We do this as a tmate, as the prompt will remain open for as long as the forwarding is up.
  #+BEGIN_SRC tmate :session foo:apisnoop
    export GOOGLE_APPLICATION_CREDENTIALS=$HOME/.gcreds.json
    export K8S_NAMESPACE="kube-system"
    kubectl config set-context $(kubectl config current-context) --namespace=$K8S_NAMESPACE 2>&1 > /dev/null
    POSTGRES_POD=$(kubectl get pod --selector=io.apisnoop.db=postgres -o name | sed s:pod/::)
    POSTGRES_PORT=$(kubectl get pod $POSTGRES_POD --template='{{(index (index .spec.containers 0).ports 0).containerPort}}{{"\n"}}')
    kubectl port-forward $POSTGRES_POD $(id -u)1:$POSTGRES_PORT
  #+END_SRC
- [X] Connect Org to our apisnoop db
  #+NAME: ReConnect org to postgres
  #+BEGIN_SRC emacs-lisp :results silent
    (if (get-buffer "*SQL: postgres:data*")
        (with-current-buffer "*SQL: postgres:data*"
          (kill-buffer)))
    (sql-connect "apisnoop" (concat "*SQL: postgres:data*"))
  #+END_SRC

  #+begin_src sql-mode
select * from projected_change_in_coverage;
  #+end_src

  #+RESULTS:
  #+begin_src sql-mode
     category    | total_endpoints | old_coverage | new_coverage | change_in_number 
  ---------------+-----------------+--------------+--------------+------------------
   test_coverage |             430 |          167 |          167 |                0
  (1 row)

  #+end_src
