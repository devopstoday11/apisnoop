# -*- ii: enabled; -*-
#+TITLE: APIsnoop Tables and Views
#+AUTHOR: ii team
#+DATE: 07 October 2019
#+INCLUDE: "config.org"
#+TODO: TODO(t) NEXT(n) IN-PROGRESS(i) BLOCKED(b) | TADA(d)
#+ARCHIVE: archive/tables_and_views.archive.org::
#+PROPERTY: header-args:sql-mode+ :results silent
#+PROPERTY: header-args:sql-mode+ :comments both

* Purpose
  This org houses all the tables and views that are migrated upon creation of apisnoop: BOT VERSION.  So these are the crucial parts for our prow pr bot.

  If you are working on a new view, it is best to do it in our ~explorations~ folder, where it can be iterated and reviewed.  Once that view is determined to be crucial for the database, it would be ported to here, and tangled to a migration file.

* Working in this file
  If you are reading this in kubemacs, and have deployed apisnoop to your kubemacs cluster, then you can connect directly to the APISnoop database from this file (and any org file) through ~sql-mode~ code blocks.  

  To get started, place your cursor within a sql-mode code block and execute it by pressing ~,,~.  This will prompt you to start up a connection, asking for your user, database, and server.  These are:
  
- User :: apisnoop
- Database :: apisnoop
- Server :: postgres

You can try this now with the below code block.  It should display in your minibuffer the testing coverage for the most recent test run.

#+begin_src sql-mode
SELECT * FROM stable_endpoint_stats;
#+end_src

You can create new sql-mode blocks with this key command: ~,ibQ~ (or: Insert Block sQl)
* snoopUtils: Our Python Functions
  :PROPERTIES:
  :header-args:python: :tangle "../apps/postgres/snoopUtils.py" :noweb yes :comments none :session "snoopUtils"
  :END:
** Our Imports
   #+NAME: snoopUtils Imports
   #+begin_src python :results silent
     import os
     import json
     from urllib.request import urlopen, urlretrieve
     from string import Template
     import requests
     import re
     from copy import deepcopy
     from functools import reduce
     from collections import defaultdict
     from urllib.parse import urlparse
     from bs4 import BeautifulSoup
     import subprocess
     import warnings
     from tempfile import mkdtemp
     import time
     import glob
   #+end_src
** Our Constants
   #+NAME: snoopUtil Constants
   #+begin_src python :results silent
      GCS_LOGS="https://storage.googleapis.com/kubernetes-jenkins/logs/"
      DEFAULT_BUCKET="ci-kubernetes-gci-gce"
      K8S_GITHUB_RAW= "https://raw.githubusercontent.com/kubernetes/kubernetes/"
      # Why do we have to had this?
      # The k8s VERB (aka action) mapping to http METHOD
      # Our audit logs do NOT contain any mention of METHOD, only the VERB
      VERB_TO_METHOD={
          'get': 'get',
          'list': 'get',
          'proxy': 'proxy',
          'create': 'post',
          'post':'post',
          'put':'post',
          'update':'put',
          'patch':'patch',
          'connect':'connect',
          'delete':'delete',
          'deletecollection':'delete',
          'watch':'get'
      }
   #+end_src

** get_json  Definition
   #+NAME: get_json
   #+begin_src python :results silent
     def get_json(url):
         """Given a json url path, return json as dict"""
         body = urlopen(url).read()
         data = json.loads(body)
         return data
   #+end_src

** determine_bucket_job Definition
   
   #+begin_src python :results silent
     def determine_bucket_job(custom_bucket=None, custom_job=None):
         """return tuple of bucket, job, using latest succesfful job of default bucket if no custom bucket or job is given"""
         #establish bucket we'll draw test results from.
         baseline_bucket = os.environ['APISNOOP_BASELINE_BUCKET'] if 'APISNOOP_BASELINE_BUCKET' in os.environ.keys() else 'ci-kubernetes-e2e-gci-gce'
         bucket =  baseline_bucket if custom_bucket is None else custom_bucket
         #grab the latest successful test run for our chosen bucket.
         testgrid_history = get_json(GCS_LOGS + bucket + "/jobResultsCache.json")
         latest_success = [x for x in testgrid_history if x['result'] == 'SUCCESS'][-1]['buildnumber']
         #establish job
         baseline_job = os.environ['APISNOOP_BASELINE_JOB'] if 'APISNOOP_BASELINE_JOB' in os.environ.keys() else latest_success
         job = baseline_job if custom_job is None else custom_job
         return (bucket, job)

   #+end_src

** fetch_swagger Definition
   :PROPERTIES:
   :header-args:python: :tangle no :comments none
   :END:
   This function is designed so, given just a bucket and a job, we can fetch a swagger.json file from github at the job's exact commit, and prepare it as a dict perfect for passing along to our load_swagger function.
   
   The structure of it is as so:

   #+NAME: define fetch_swagger
   #+begin_src python :tangle "../apps/postgres/snoopUtils.py" :session snoopUtils :results silent
     def fetch_swagger(bucket, job):
         """fetches swagger for given bucket and job and returns it, and its appropariate metadata, in a dict"""
         <<grab metadata for bucket job>>
         <<determine commit hash from metadata>>
         <<grab swagger at commit hash>>
         <<return swagger and metadata>>
    
   #+end_src

   All the metadata we need can be found in the finished.json file for that bucket and job
   
   #+NAME: grab metadata for bucket job
   #+begin_src python
     metadata_url = ''.join([GCS_LOGS, bucket, '/', job, '/finished.json'])
     metadata = json.loads(urlopen(metadata_url).read().decode('utf-8'))
   #+end_src
   
   Within the version of the metadata is the commit hash that triggered this particular job.  We want to pull the swagger from the right period as the API can change quickly, for example an endpoint moving from alpha to beta to stable.
   #+NAME: determine commit hash from metadata 
   #+begin_src python
    commit_hash = metadata["version"].split("+")[1]
   #+end_src
   
   Then we grab the raw swagger.json from the k8s github, which lets us fetch the code from a particular hash.
   #+NAME:grab swagger at commit hash
   #+begin_src python
    swagger_url =  ''.join([K8S_GITHUB_RAW, commit_hash, '/api/openapi-spec/swagger.json'])
    swagger = json.loads(urlopen(swagger_url).read().decode('utf-8')) # may change this to ascii
   #+end_src
   
   Our bucket_job_swagger table will want to know the bucket, job, metadata, and swagger so we put them all into a dict, and we done!
   
   #+NAME: return swagger and metadata
   #+begin_src python
     return (swagger, metadata, commit_hash);
   #+end_src
   
** deep_merge Definition
   
   Current understanding: 
  - this is a recursive function for merging two dicts.
  - We are doing a reduce starting with an empty dict.
  - We have merge into that will be comparing the two dicts in a list
  - for each key in the second dict (d2) we see if that key exists in first dict d1
  - if it does, we check if that key is also a dict
  - if both true, we merge the two keys together, by stepping into both and recursivel.y checking again
  - If not, we copy all of d2[key] into d1[key]
  
  I'd like to show an example, but it seems to be returning nothing....a bit odd
  Where does this get used?  Is it a  part of the cache?
    #+NAME: deep_merge example
    #+begin_src python :tangle no :results output
      d1 = {"a": "", "b": {"b1": "cool"}}
      d2 = {"a": {"a1": "fun"}, "b": {"b1": "awesome"}, "c": "sweet"}
      dicts = [d1, d2]
    #+end_src

    #+NAME: Define merge_into
    #+begin_src python
      def merge_into(d1, d2):
          for key in d2:
              if key not in d1 or not isinstance(d1[key], dict):
                  d1[key] = deepcopy(d2[key])
              else:
                  d1[key] = merge_into(d1[key], d2[key])
          return d1

    #+end_src


   #+NAME: Define deep_merge
   #+BEGIN_SRC python
     def deep_merge(*dicts, update=False):
         if update:
             return reduce(merge_into, dicts[1:], dicts[0])
         else:
             return reduce(merge_into, dicts, {})
    
   #+END_SRC

   #+RESULTS: Define deep_merge
   #+begin_src python
   #+end_src

** get_html definition
  #+NAME: get_html 
  #+begin_src python
    def get_html(url):
        """return html content of given url"""
        html = urlopen(url).read()
        soup = BeautifulSoup(html, 'html.parser')
        return soup

  #+end_src
** download_url_to_path definition
   #+NAME: download_url_to_path
   #+begin_src python
     def download_url_to_path(url, local_path, dl_dict):
         """
         downloads contents to local path, creating path if needed,
         then updates given downloads dict.
         """
         local_dir = os.path.dirname(local_path)
         if not os.path.isdir(local_dir):
             os.makedirs(local_dir)
         if not os.path.isfile(local_path):
             process = subprocess.Popen(['wget', '-q', url, '-O', local_path])
             dl_dict[local_path] = process
   #+end_src
** get_all_auditlog_links
   #+begin_src python
     def get_all_auditlog_links(au):
         """
         given an artifacts url, au, return a list of all
         audit.log.* within it.
         (some audit.logs end in .gz)
         """
         soup = get_html(au)
         master_link = soup.find(href=re.compile("master"))
         master_soup = get_html("https://gcsweb.k8s.io" + master_link['href'])
         return master_soup.find_all(href=re.compile("audit.log"))

   #+end_src
** load_openapi_spec Definition
   This loads just the paths portion of the open_api spec into a dictionary made up of paths and methods...with each part of the path its own dict ... this also seems to be recursive but am not fully tracking what the python is doing.  its iterating over each path, but does it create a flat dict at the end, or does each path have its smaller parts within it?  so 
   
   #+NAME: load_openapi_spec
   #+BEGIN_SRC python 
     def load_openapi_spec(url):
         # Usually, a Python dictionary throws a KeyError if you try to get an item with a key that is not currently in the dictionary.
         # The defaultdict in contrast will simply return an empty dict.
         cache=defaultdict(dict)
         openapi_spec = {}
         openapi_spec['hit_cache'] = {}
         swagger = requests.get(url).json()
         # swagger contains other data, but paths is our primary target
         for path in swagger['paths']:
             # parts of the url of the 'endpoint'
             path_parts = path.strip("/").split("/")
             # how many parts?
             path_len = len(path_parts)
             # current_level = path_dict  = {}
             last_part = None
             last_level = None
             path_dict = {}
             current_level = path_dict
             # look at each part of the url/path
             for part in path_parts:
                 # if the current level doesn't have a key (folder) for this part, create an empty one
                 if part not in current_level:
                     current_level[part] = {}
                     # last_part will be this part, unless there are more parts 
                     last_part=part
                     # last_level will be this level, unless there are more levels
                     last_level = current_level
                     # current_level will be this this 'folder/dict', this might be empty
                     # /api will be the top level v. often, and we only set it once
                     current_level = current_level[part]
             # current level is now pointing to the inmost url part hash
             # now we iterate through the http methods for this path/endpoint
             for method, swagger_method in swagger['paths'][path].items():
                 # If the method is parameters, we don't look at it
                 # think this method is only called to explore with the dynamic client
                 if method == 'parameters':
                     next
                 else:
                     # for the nested current_level (end of the path/url) use the method as a lookup to the operationId
                     current_level[method]=swagger_method.get('operationId', '')
                     # cache = {}
                     # cache = {3 : {'/api','v1','endpoints'} 
                     # cache = {3 : {'/api','v1','endpoints'} {2 : {'/api','v1'} 
                     # cache uses the length of the path to only search against other paths that are the same length
                     cache = deep_merge(cache, {path_len:path_dict})
                     openapi_spec['cache'] = cache
         return openapi_spec
   #+END_SRC

** find_operation_id Definition

#+begin_src python
#+end_src
#+NAME: find_operation_id
#+BEGIN_SRC python
  def find_operation_id(openapi_spec, event):
    method=VERB_TO_METHOD[event['verb']]
    url = urlparse(event['requestURI'])
    # 1) Cached seen before results
    # IS the URL in the hit_cach?
    if url.path in openapi_spec['hit_cache']:
      # Is the method for this url cached?
      if method in openapi_spec['hit_cache'][url.path].keys():
        # Useful when url + method is already hit multiple times in an audit log
        return openapi_spec['hit_cache'][url.path][method]
    # part of the url of the http/api request
    uri_parts = url.path.strip('/').split('/')
    # IF we git a proxy component, the rest of this is just parameters and we don't "count" them
    if 'proxy' in uri_parts:
        # uri /orig/path/to/proxy/args/to/proxie
        # z is our temporary var to update uri_parts to contain:
        z=uri_parts[0:uri_parts.index('proxy')]
        # z = [ orig path to ]
        z.append('proxy')
        # z = [ orig path to proxy ]
        # However all parameters after that are another single argument
        # to the proxy
        z.append('/'.join(
            uri_parts[uri_parts.index('proxy')+1:]))
        # z = [ orig path to proxy "args/to/proxy"]
        # allows dict_len and uri hit cache to work
        uri_parts = z
    # We index our cache primarily on part_count
    part_count = len(uri_parts)
    # INSTEAD of try: except: maybe look into if cache has part count and complain explicitely with a good error
    try: # may have more parts... so no match
        # If we hit a length / part_count that isn't in the APISpec... this an invalid api request
        # our load_openapispec should populate all possible url length in our cache
        cache = openapi_spec['cache'][part_count]
    except Exception as e:
      # If you hit here, you are debugging... hence the warnings
      warnings.warn("part_count was:" + part_count)
      warnings.warn("spec['cache'] keys was:" + openapi_spec['cache'])
      raise e
    last_part = None
    last_level = None
    current_level = cache
    for idx in range(part_count):
      part = uri_parts[idx]
      last_level = current_level
      if part in current_level:
        current_level = current_level[part] # part in current_level
      elif idx == part_count-1:
        # This elsif is ONLY run when we are at the end of the url /path/to/{namespace}, /path/to/metrics
        # TODO turn into an if in constant array
        if part == 'metrics':
          return None
        if part == 'readyz':
          return None
        if part == 'livez':
          return None
        if part == 'healthz':
          return None
        if 'discovery.k8s.io' in uri_parts:
          return None
        #   elif part == '': # The last V
        #     current_level = last_level
        #       else:
        # variable levels are like {namespace}, etc
        variable_levels=[x for x in current_level.keys() if '{' in x] # vars at current(final) level?
        # If at some point in the future we have more than one... this will let un know
        if len(variable_levels) > 1:
          raise "If we have more than one variable levels... this should never happen."
        # TODO inspect that variable_levels is not zero in length
        variable_level=variable_levels[0] # the var is the next level
        # TODO inspect that variable level is a key for current_level
        current_level = current_level[variable_level] # variable part is final part
      else:
        next_part = uri_parts[idx+1]
        # TODO reduce this down to , find the single next level with a "{" in it 
        variable_levels={next_level:next_part in current_level[next_level].keys() for next_level in [x for x in current_level.keys() if '{' in x]}
        # variable_level=[x for x in current_level.keys() if '{' in x] .first
        if not variable_levels: # there is no match
          # TODO turn into an if in constant array OR just return None and we look in the logs
          if 'example.com' in part:
            return None
          elif 'kope.io' in part:
            return None
          elif 'snapshot.storage.k8s.io' in part:
            return None
          elif 'discovery.k8s.io' in part:
            return None
          elif 'metrics.k8s.io' in part:
            return None
          elif 'wardle.k8s.io' in part:
            return None
          elif ['openapi','v2'] == uri_parts: # not part our our spec
            return None
          else:
            # TODO this is NOT valid, AND we didn't plan for it
            print(url.path)
            return None
        # try: # may have more parts... so no match # variable_levels -> {'{namespace}': False}
        next_level=variable_levels[0]
        # next_level={v: k for k, v in variable_levels.items()}[True]
        # except Exception as e: # TODO better to not use try/except (WE DON"T HAVE ANY CURRENT DATA")
        #   next_level=[*variable_levels][0]
        # import ipdb; ipdb.set_trace()
        current_level = current_level[next_level] #coo
    try:
      op_id=current_level[method]
    except Exception as err:
      warnings.warn("method was:" + method)
      warnings.warn("current_level keys:" + current_level.keys())
      raise err
    if url.path not in openapi_spec['hit_cache']:
      openapi_spec['hit_cache'][url.path]={method:op_id}
    else:
      openapi_spec['hit_cache'][url.path][method]=op_id
    return op_id
#+END_SRC
** download_and_process_auditlogs definitions
   #+NAME: download_and_process_auditlogs
   #+begin_src  python
     def download_and_process_auditlogs(bucket,job):
         """
         Grabs all audits logs available for a given bucket/job, combines them into a
         single audit log, then returns the path for where the raw combined audit logs are stored.
         The processed logs are in json, and include the operationId when found.
         """
         # BUCKETS_PATH = 'https://storage.googleapis.com/kubernetes-jenkins/logs/'
         ARTIFACTS_PATH ='https://gcsweb.k8s.io/gcs/kubernetes-jenkins/logs/'
         K8S_GITHUB_REPO = 'https://raw.githubusercontent.com/kubernetes/kubernetes/'
         downloads = {}
         # bucket_url = BUCKETS_PATH + bucket + '/' + job + '/'
         artifacts_url = ARTIFACTS_PATH + bucket + '/' +  job + '/' + 'artifacts'
         download_path = mkdtemp( dir='/tmp', prefix='apisnoop-' + bucket + '-' + job ) + '/'
         combined_log_file = download_path + 'audit.log'
         swagger, metadata, commit_hash = fetch_swagger(bucket, job)

         # download all metadata
         # job_metadata_files = [
         #     'finished.json',
         #     'artifacts/metadata.json',
         #     'artifacts/junit_01.xml',
         #     'build-log.txt'
         # ]
         # for jobfile in job_metadata_files:
         #     download_url_to_path( bucket_url + jobfile,
         #                           download_path + jobfile, downloads )

         # download all logs
         log_links = get_all_auditlog_links(artifacts_url)
         for link in log_links:
             log_url = link['href']
             log_file = download_path + os.path.basename(log_url)
             download_url_to_path( log_url, log_file, downloads)

         # Our Downloader uses subprocess of curl for speed
         for download in downloads.keys():
             # Sleep for 5 seconds and check for next download
             while downloads[download].poll() is None:
                 time.sleep(5)

         # Loop through the files, (z)cat them into a combined audit.log
         with open(combined_log_file, 'ab') as log:
             for logfile in sorted(
                     glob.glob(download_path + '*kube-apiserver-audit*'), reverse=True):
                 if logfile.endswith('z'):
                     subprocess.run(['zcat', logfile], stdout=log, check=True)
                 else:
                     subprocess.run(['cat', logfile], stdout=log, check=True)

         # Process the resulting combined raw audit.log by adding operationId
         swagger_url = K8S_GITHUB_REPO + commit_hash + '/api/openapi-spec/swagger.json'
         openapi_spec = load_openapi_spec(swagger_url)
         infilepath=combined_log_file
         outfilepath=combined_log_file+'+opid'
         with open(infilepath) as infile:
             with open(outfilepath,'w') as output:
                 for line in infile.readlines():
                     event = json.loads(line)
                     event['operationId']=find_operation_id(openapi_spec,event)
                     output.write(json.dumps(event)+'\n')
         return outfilepath
   #+end_src
** json_to_sql definitions
   #+NAME: json_to_sql
   #+begin_src  python
     def json_to_sql(bucket,job,auditlog_path):
         """
           Turns json+audits into load.sql
         """
         try:
             sql = Template("""
     CREATE TEMPORARY TABLE raw_audit_event_import (data jsonb not null) ;
     COPY raw_audit_event_import (data)
     FROM '${audit_logfile}' (DELIMITER e'\x02', FORMAT 'csv', QUOTE e'\x01');

     INSERT INTO raw_audit_event(bucket, job,
                                  audit_id, stage,
                                  event_verb, request_uri,
                                  operation_id,
                                  data)
     SELECT '${bucket}', '${job}',
            (raw.data ->> 'auditID'), (raw.data ->> 'stage'),
            (raw.data ->> 'verb'), (raw.data ->> 'requestURI'),
            (raw.data ->> 'operationId'),
            raw.data
       FROM raw_audit_event_import raw;
             """).substitute(
                 audit_logfile = auditlog_path,
                 bucket = bucket,
                 job = job
             )
             return sql
         except:
             return "something unknown went wrong"
   #+end_src
** insert_audits_into_db definition                                   :notes:
This is good for notes, but not used 
   #+NAME: insert_audits_into_db
   #+begin_src python :tangle no
      def insert_audits_into_db (download_path, auditlog_path):
          # try:
          #     plpy.here?
          # except:
          #     from SQL import sqllib as plpy
          bucket, job = determine_bucket_job("ci-kubernetes-e2e-gci-gce","1232358105564581890")
          download_path, auditlog_path = load_audit_events(bucket, job)
          sql_string = json_to_sql(bucket, job, download_path)
          rv = plpy.execute(sql)
   #+end_src
* 100: Raw Data Tables and Helper Functions
** 100: bucket_job_swagger table
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/100_table_bucket_job_swagger.up.sql
  :header-args:sql-mode+: :var heading=(org-entry-get nil "ITEM")
  :END:
*** Create Table
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/100_table_bucket_job_swagger.up.sql
    :END:
 #+NAME: bucket_job_swagger
 #+BEGIN_SRC sql-mode :results silent
   CREATE TABLE bucket_job_swagger (
       ingested_at timestamp DEFAULT CURRENT_TIMESTAMP,
       bucket text,
       job text,
       commit_hash text,
       passed text,
       job_result text,
       pod text,
       infra_commit text,
       job_version text,
       job_timestamp timestamp,
       node_os_image text,
       master_os_image text ,
       swagger jsonb,
       PRIMARY KEY (bucket, job)
   );
 #+END_SRC
*** Index Table
 #+NAME: general index the raw_swagger
 #+BEGIN_SRC sql-mode
   CREATE INDEX idx_swagger_jsonb_ops ON bucket_job_swagger
     USING GIN (swagger jsonb_ops);
   CREATE INDEX idx_swagger_jsonb_path_ops ON bucket_job_swagger
     USING GIN (swagger jsonb_path_ops);
 #+END_SRC
** 101: Function to Load Swagger
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/101_function_load_swagger.up.sql
  :END:
    #+NAME: load_swagger.sql
    #+BEGIN_SRC sql-mode :noweb yes :results silent
      set role dba;
      DROP FUNCTION IF EXISTS load_swagger;
      CREATE OR REPLACE FUNCTION load_swagger(
        custom_bucket text default null,
        custom_job text default null,
        live boolean default false)
      RETURNS text AS $$
      <<load_swagger.py>>
      $$ LANGUAGE plpython3u ;
      reset role;
    #+END_SRC
*** The Python function
    #+NAME: load_swagger.py
    #+BEGIN_SRC python :eval never :exports code
      #Import our snoop utilities and values
      import json
      from snoopUtils import determine_bucket_job, fetch_swagger

      bucket, job = determine_bucket_job(custom_bucket, custom_job)
      swagger, metadata, commit_hash = fetch_swagger(bucket, job)

      ## define our sql statement
      sql = """
      INSERT INTO bucket_job_swagger(
              bucket,
              job,
              commit_hash,
              passed,
              job_result,
              infra_commit,
              job_version,
              job_timestamp,
              node_os_image,
              master_os_image,
              swagger
      )
      SELECT
              $1 as bucket,
              $2 as job,
              $3 as commit_hash,
              $4 as passed,
              $5 as job_result,
              $6 as infra_commit,
              $7 as job_version,
              (to_timestamp($8)) AT TIME ZONE 'UTC' as job_timestamp,
              $9 as node_os_image,
              $10 as master_os_image,
              $11 as swagger
      """

      ## Submit sql statement with values substituted in
      plan = plpy.prepare(sql, [
          'text','text','text','text',
          'text','text','text',
          'integer','text','text','jsonb'])
      try:
        rv = plpy.execute(plan, [
            bucket if not live else 'apisnoop',
            job if not live else 'live',
            commit_hash,
            metadata['passed'],
            metadata['result'],
            metadata['metadata']['infra-commit'],
            metadata['version'],
            int(metadata['timestamp']),
            metadata['metadata']['node_os_image'],
            metadata['metadata']['master_os_image'],
            json.dumps(swagger)
        ])
        ## Celebrate
        return ''.join(["Success!  Added the swagger for job ", job, " from bucket ", bucket])
      except:
        e = sys.exc_info()[0]
        print("<p>Error: %s</p>" % e )
    #+END_SRC
** 110: raw_audit_event Table
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/110_table_raw_audit_event.up.sql
  :END:
*** Create
#+NAME: raw_audit_event
#+BEGIN_SRC sql-mode
  CREATE UNLOGGED TABLE raw_audit_event (
    -- id int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    -- ingested_at timestamp DEFAULT CURRENT_TIMESTAMP,
    bucket text,
    job text,
    audit_id text NOT NULL,
    stage text NOT NULL,
    event_verb text NOT NULL,
    request_uri text NOT NULL,
    operation_id text,
    data jsonb NOT NULL
  );
#+END_SRC
*** TODO Index
I am not sure why our create index and alter table lines are commented out.
the TODO is to enquire on why these lines are commented
#+NAME: index the raw_audit_event
#+BEGIN_SRC sql-mode
-- CREATE INDEX idx_audit_event_primary          ON raw_audit_event (bucket, job, audit_id, stage);
-- ALTER TABLE raw_audit_event add primary key using index idx_audit_event_primary;
CREATE INDEX idx_audit_event_jsonb_ops        ON raw_audit_event USING GIN (data jsonb_ops);
CREATE INDEX idx_audit_event_jsonb_path_jobs  ON raw_audit_event USING GIN (data jsonb_path_ops);
#+END_SRC

** 111: load_audit_event Function
  :PROPERTIES:
  :header-args:sql-mode+:  :tangle ../apps/hasura/migrations/111_function_load_audit_event.up.sql
  :END:
*** Create
#+NAME: load_audit_events.sql
#+BEGIN_SRC sql-mode :noweb yes :results silent
  set role dba;
  CREATE OR REPLACE FUNCTION load_audit_events(
    custom_bucket text default null,
    custom_job text default null)
    RETURNS text AS $$
    from snoopUtils import determine_bucket_job, download_and_process_auditlogs, json_to_sql
    bucket, job = determine_bucket_job(custom_bucket, custom_job)
    auditlog_path = download_and_process_auditlogs(bucket, job)
    sql_string = json_to_sql(bucket,job,auditlog_path) 
    try:
        plpy.execute(sql_string)
        return "it worked"
    except plpy.SPIError:
        return "something went wrong with plpy"
    except:
        return "something unknown went wrong"
  $$ LANGUAGE plpython3u ;
  reset role;
#+END_SRC
** 112: add_opp_id function
  :PROPERTIES:
  :header-args:sql-mode+:  :tangle ../apps/hasura/migrations/112_function_add_opp_id.up.sql
  :END:
*** Python Code
   #+NAME: add_opp_id.py
   #+begin_src python :results silent :noweb yes :tangle no
     import json
     from urllib.request import urlopen, urlretrieve
     import os
     import re
     from bs4 import BeautifulSoup
     import subprocess
     import time
     import glob
     from tempfile import mkdtemp
     from string import Template
     from urllib.parse import urlparse
     import requests
     import hashlib
     from collections import defaultdict
     import json
     import csv
     import sys
     from snoopUtils import deep_merge, load_openapi_spec, find_operation_id
     import warnings

     if "spec" not in GD:
         GD["spec"] = load_openapi_spec('https://raw.githubusercontent.com/kubernetes/kubernetes/7d13dfe3c34f44/api/openapi-spec/swagger.json')
     spec = GD["spec"]
     event = json.loads(TD["new"]["data"])
     if TD["new"]["operation_id"] is None:
         TD["new"]["operation_id"] = find_operation_id(spec, event);
     return "MODIFY";
   #+end_src

*** Create
#+NAME: add_opp_id.sql
#+begin_src sql-mode :noweb yes :results silent
  set role dba;
  CREATE FUNCTION add_op_id() RETURNS TRIGGER as $$
  <<add_opp_id.py>>
  $$ LANGUAGE plpython3u;
  reset role;
#+end_src
** 113: add_opp_id trigger
  :PROPERTIES:
  :header-args:sql-mode+:  :tangle ../apps/hasura/migrations/113_trigger_add_opp_id.up.sql
  :END:
   #+NAME: Create Trigger
   #+begin_src sql-mode :results silent :tangle no
     CREATE TRIGGER add_op_id BEFORE INSERT ON raw_audit_event
       FOR EACH ROW EXECUTE PROCEDURE add_op_id();
   #+end_src
* 200: API Views
** 200: api_operation_material view
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/200_view_api_operation_material.up.sql
  :END:
  We can track this, but it won't show up in Hasura as it does not support materialized views yet.  We can still use it to create _other_ views hasura can see though.
*** Define regex_from_path function
#+NAME: regex_from_path.py
#+BEGIN_SRC python :eval never :export none
  import re
  if path is None:
    return None
  K8S_PATH_VARIABLE_PATTERN = re.compile("{(path)}$")
  VARIABLE_PATTERN = re.compile("{([^}]+)}")
  path_regex = K8S_PATH_VARIABLE_PATTERN.sub("(.*)", path).rstrip('/')
  path_regex = VARIABLE_PATTERN.sub("([^/]*)", path_regex).rstrip('/')
  if not path_regex.endswith(")") and not path_regex.endswith("?"):
    path_regex += "([^/]*)"
  if path_regex.endswith("proxy"):
      path_regex += "/?$"
  else:
      path_regex += "$"
  return path_regex
#+END_SRC

#+NAME: regex_from_path.sql
#+BEGIN_SRC sql-mode :noweb yes
  set role dba;
  CREATE OR REPLACE FUNCTION regex_from_path(path text)
  RETURNS text AS $$
  <<regex_from_path.py>>
  $$ LANGUAGE plpython3u ;
  reset role;
#+END_SRC

*** Create

#+NAME: api_operation_material
#+BEGIN_SRC sql-mode
  CREATE MATERIALIZED VIEW "public"."api_operation_material" AS
    SELECT
      (d.value ->> 'operationId'::text) AS operation_id,
      CASE
      WHEN paths.key ~~ '%alpha%' THEN 'alpha'
      WHEN paths.key ~~ '%beta%' THEN 'beta'
      ELSE 'stable'
           END AS level,
      split_part((cat_tag.value ->> 0), '_'::text, 1) AS category,
      ((d.value -> 'x-kubernetes-group-version-kind'::text) ->> 'group'::text) AS k8s_group,
      ((d.value -> 'x-kubernetes-group-version-kind'::text) ->> 'kind'::text) AS k8s_kind,
      ((d.value -> 'x-kubernetes-group-version-kind'::text) ->> 'version'::text) AS k8s_version,
      CASE
      WHEN (lower((d.value ->> 'description'::text)) ~~ '%deprecated%'::text) THEN true
      ELSE false
           END AS deprecated,
      (d.value ->> 'description'::text) AS description,
      d.key AS http_method,
      (d.value ->> 'x-kubernetes-action'::text) AS k8s_action,
      CASE
      WHEN (d.value ->> 'x-kubernetes-action'::text) = 'get' THEN ARRAY ['get']
      WHEN (d.value ->> 'x-kubernetes-action'::text) =  'list' THEN ARRAY [ 'list' ]
      WHEN (d.value ->> 'x-kubernetes-action'::text) = 'proxy' THEN ARRAY [ 'proxy' ]
      WHEN (d.value ->> 'x-kubernetes-action'::text) = 'deletecollection' THEN ARRAY [ 'deletecollection' ]
      WHEN (d.value ->> 'x-kubernetes-action'::text) = 'watch' THEN ARRAY [ 'watch' ]
      WHEN (d.value ->> 'x-kubernetes-action'::text) = 'post' THEN ARRAY [ 'post', 'create' ]
      WHEN (d.value ->> 'x-kubernetes-action'::text) =  'put' THEN ARRAY [ 'put', 'update' ]
      WHEN (d.value ->> 'x-kubernetes-action'::text) = 'patch' THEN ARRAY [ 'patch' ]
      WHEN (d.value ->> 'x-kubernetes-action'::text) = 'connect' THEN ARRAY [ 'connect' ]
      ELSE NULL
             END as event_verb,
      paths.key AS path,
      (d.value -> 'consumes'::text)::jsonb AS consumes,
      (d.value -> 'responses'::text)::jsonb AS responses,
      (d.value -> 'parameters'::text)::jsonb AS parameters,
      string_agg(btrim((jsonstring.value)::text, '"'::text), ', '::text) AS tags,
      string_agg(btrim((schemestring.value)::text, '"'::text), ', '::text) AS schemes,
      regex_from_path(paths.key) as regex,
      bjs.bucket AS bucket,
      bjs.job AS job
      FROM bucket_job_swagger bjs
           , jsonb_each((bjs.swagger -> 'paths'::text)) paths(key, value)
           , jsonb_each(paths.value) d(key, value)
           , jsonb_array_elements((d.value -> 'tags'::text)) cat_tag(value)
           , jsonb_array_elements((d.value -> 'tags'::text)) jsonstring(value)
           , jsonb_array_elements((d.value -> 'schemes'::text)) schemestring(value)
     GROUP BY bjs.bucket, bjs.job, paths.key, d.key, d.value, cat_tag.value
     ORDER BY paths.key;
#+END_SRC

*** Index
#+NAME: index the api_operation_material
#+BEGIN_SRC sql-mode :tangle ../apps/hasura/migrations/201_view_api_operation_material.up.sql :results silent
  CREATE INDEX api_operation_materialized_bucket      ON api_operation_material            (bucket);
  CREATE INDEX api_operation_materialized_event_verb  ON api_operation_material            (event_verb);
  CREATE INDEX api_operation_materialized_k8s_action  ON api_operation_material            (k8s_action);
  CREATE INDEX api_operation_materialized_k8s_group   ON api_operation_material            (k8s_group);
  CREATE INDEX api_operation_materialized_k8s_version ON api_operation_material            (k8s_version);
  CREATE INDEX api_operation_materialized_k8s_kind    ON api_operation_material            (k8s_kind);
  CREATE INDEX api_operation_materialized_tags        ON api_operation_material            (tags);
  CREATE INDEX api_operation_materialized_schemes     ON api_operation_material            (schemes);
  CREATE INDEX api_operation_materialized_regex_gist  ON api_operation_material USING GIST (regex gist_trgm_ops);
  CREATE INDEX api_operation_materialized_regex_gin   ON api_operation_material USING GIN  (regex gin_trgm_ops);
  CREATE INDEX api_operation_materialized_consumes_ops   ON api_operation_material USING GIN  (consumes jsonb_ops);
  CREATE INDEX api_operation_materialized_consumes_path  ON api_operation_material USING GIN  (consumes jsonb_path_ops);
  CREATE INDEX api_operation_materialized_parameters_ops   ON api_operation_material USING GIN  (parameters jsonb_ops);
  CREATE INDEX api_operation_materialized_parameters_path  ON api_operation_material USING GIN  (parameters jsonb_path_ops);
  CREATE INDEX api_operation_materialized_responses_ops   ON api_operation_material USING GIN  (responses jsonb_ops);
  CREATE INDEX api_operation_materialized_responses_path  ON api_operation_material USING GIN  (responses jsonb_path_ops);
#+END_SRC

** 210: api_operation
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/210_view_api_operation.up.sql
  :END:
  
  #+begin_src sql-mode
    CREATE OR REPLACE VIEW api_operation AS
      SELECT
        ,*
        FROM
            api_operation_material;
  #+end_src
** 220: api_operation_parameter_material
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/220_view_api_operation_parameter_material.up.sql
  :END:
*** Create
Using our api_operation_material view, look into the parameters field in each one.
#+NAME: api_operation_parameter_material view
#+BEGIN_SRC sql-mode
  CREATE MATERIALIZED VIEW "public"."api_operation_parameter_material" AS
    SELECT ao.operation_id AS param_op,
    (param.entry ->> 'name'::text) AS param_name,
           -- for resource:
           -- if param is body in body, take its $ref from its schema
           -- otherwise, take its type
           replace(
             CASE
             WHEN ((param.entry ->> 'in'::text) = 'body'::text)
              AND ((param.entry -> 'schema'::text) is not null)
               THEN ((param.entry -> 'schema'::text) ->> '$ref'::text)
             ELSE (param.entry ->> 'type'::text)
             END, '#/definitions/','') AS param_schema,
           CASE
           WHEN ((param.entry ->> 'required'::text) = 'true') THEN true
           ELSE false
            END AS required,
           (param.entry ->> 'description'::text) AS param_description,
           CASE
           WHEN ((param.entry ->> 'uniqueItems'::text) = 'true') THEN true
           ELSE false
           END AS unique_items,
           (param.entry ->> 'in'::text) AS "in",
           ao.bucket,
           ao.job,
           param.entry as entry
      FROM api_operation_material ao
           , jsonb_array_elements(ao.parameters) WITH ORDINALITY param(entry, index)
            WHERE ao.parameters IS NOT NULL;
#+END_SRC
*** Index
#+NAME: index the api_operation_material
#+BEGIN_SRC sql-mode
    CREATE INDEX api_parameters_materialized_schema      ON api_operation_parameter_material            (param_schema);
#+END_SRC

* 300: Audit Event Views
** 300: Audit Event
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/310_view_audit_event.up.sql
  :END:
  
  #+NAME: view audit_event
  #+BEGIN_SRC sql-mode
    CREATE VIEW "public"."audit_event" AS
      SELECT (raw.data ->> 'auditID') as audit_id,
              raw.bucket,
              raw.job,
              raw.data ->> 'level' as event_level,
              raw.data ->> 'stage' as event_stage,
              raw.operation_id,
              aop.param_schema,
              raw.data ->> 'verb' as event_verb,
              raw.data ->> 'apiVersion' as api_version,
              raw.data ->> 'requestURI' as request_uri,
              -- Always "Event"
              -- raw.data ->> 'kind' as kind,
              raw.data ->> 'userAgent' as useragent,
              raw.data -> 'user' as event_user,
              raw.data #>> '{objectRef,namespace}' as object_namespace,
              raw.data #>> '{objectRef,resource}' as object_type,
              raw.data #>> '{objectRef,apiGroup}' as object_group,
              raw.data #>> '{objectRef,apiVersion}' as object_ver,
              raw.data -> 'sourceIPs' as source_ips,
              raw.data -> 'annotations' as annotations,
              raw.data -> 'requestObject' as request_object,
              raw.data -> 'responseObject' as response_object,
              raw.data -> 'responseStatus' as response_status,
              raw.data ->> 'stageTimestamp' as stage_timestamp,
              raw.data ->> 'requestReceivedTimestamp' as request_received_timestamp,
              raw.data as data
        FROM raw_audit_event raw
                LEFT JOIN (
                  select param_op, param_schema
                    from api_operation_parameter_material
                  WHERE param_name = 'body'
                ) aop
                    ON (raw.operation_id = aop.param_op);
  #+END_SRC
* 500: Endpoint Coverage Views
   :PROPERTIES:
   :header-args:sql-mode+: :results silent
   :END:
** 500: Endpoint Coverage Material View
   :PROPERTIES:
   :header-args:sql-mode+: :tangle ../apps/hasura/migrations/500_view_endpoint_coverage_material.up.sql
   :END:

   developed in [[file:explorations/ticket_50_endpoint_coverage.org][ticket 50: endpoint coverage]]
   #+NAME: Endpoint Coverage View
   #+BEGIN_SRC sql-mode
     CREATE MATERIALIZED VIEW "public"."endpoint_coverage_material" AS
      SELECT DISTINCT
        bjs.job_timestamp::date as date,
        ao.bucket as bucket,
        ao.job as job,
        ao.operation_id as operation_id,
        ao.level,
        ao.category,
        ao.k8s_group as group,
        ao.k8s_kind as kind,
        ao.k8s_version as version,
        EXISTS (
          select *
            from audit_event
           where audit_event.operation_id = ao.operation_id
             AND audit_event.bucket = ao.bucket
             AND audit_event.job = ao.job
             AND audit_event.useragent like 'e2e.test%') as tested,
        EXISTS (
          select *
            from audit_event
           where audit_event.operation_id = ao.operation_id
             AND audit_event.bucket = ao.bucket
             AND audit_event.job = ao.job
             AND audit_event.useragent like '%[Conformance]%') as conf_tested,
        EXISTS (
          select *
            from audit_event
           where audit_event.operation_id = ao.operation_id
             AND audit_event.bucket = ao.bucket
             AND audit_event.job = ao.job) as hit
        FROM api_operation_material ao
               LEFT JOIN bucket_job_swagger bjs ON (ao.bucket = bjs.bucket AND ao.job = bjs.job)
          WHERE ao.deprecated IS False and ao.job != 'live'
        GROUP BY ao.operation_id, ao.bucket, ao.job, date, ao.level, ao.category, ao.k8s_group, ao.k8s_kind, ao.k8s_version;
    #+END_SRC

*** Index
   #+NAME: Add indexes 
   #+begin_src sql-mode :results silent
  CREATE INDEX idx_endpoint_coverage_material_bucket             ON endpoint_coverage_material                       (bucket);
  CREATE INDEX idx_endpoint_coverage_material_job                ON endpoint_coverage_material                          (job);
  CREATE INDEX idx_endpoint_coverage_material_operation_id       ON endpoint_coverage_material                 (operation_id);
   #+end_src
   
** 510: Endpoint Coverage View
   :PROPERTIES:
   :header-args:sql-mode+: :tangle ../apps/hasura/migrations/510_view_endpoint_coverage.up.sql
   :END:
    #+NAME: endpoint_coverage_material
    #+BEGIN_SRC sql-mode
      CREATE OR REPLACE VIEW "public"."endpoint_coverage" AS
      SELECT
        *
        FROM
            endpoint_coverage_material;
    #+END_SRC
   
** 520: stable endpoint_stats_view
   :PROPERTIES:
   :header-args:sql-mode+: :tangle ../apps/hasura/migrations/520_view_stable_endpoint_stats.up.sql
   :END:
   Based on the update we give to dan, developed in [[file:explorations/ticket_50_endpoint_coverage.org][ticket 50: endpoint coverage]]
   #+NAME: Endpoint Stats View
   #+BEGIN_SRC sql-mode
     CREATE OR REPLACE VIEW "public"."stable_endpoint_stats" AS
     SELECT
       ec.job,
       ec.date,
       COUNT(1) as total_endpoints,
       COUNT(1) filter(WHERE tested is true) as test_hits,
       COUNT(1) filter(WHERE conf_tested is true) as conf_hits,
       ROUND(((count(*) filter(WHERE tested is true)) * 100 )::numeric / count(*), 2) as percent_tested,
       ROUND(((count(*) filter(WHERE conf_tested is true)) * 100 )::numeric / count(*), 2) as percent_conf_tested
       FROM endpoint_coverage ec
         WHERE ec.level = 'stable'
      GROUP BY ec.date, ec.job;
   #+END_SRC
** 530: Change in Coverage
   :PROPERTIES:
   :header-args:sql-mode+: :notangle ../apps/hasura/migrations/530_view_change_in_coverage.up.sql
   :END:

   Meant to look at the last two test runs in database and calculate their change in coverage.  This was assuming we were loading multiple audit events.  Currently the flow is to load one baseline eent, and then compare the testing we do against it.  As such, removing this view until it is needed, to not confuse the tester working with apisnoop.
   #+NAME: Change in Coverage
   #+BEGIN_SRC sql-mode :results replace
   CREATE OR REPLACE VIEW "public"."change_in_coverage" AS
     with last_two_runs as (
       select
         *
         FROM
             stable_endpoint_stats
        ORDER BY
          date DESC
        LIMIT 2
     ), new_coverage as (
       SELECT *
         FROM last_two_runs
        order by date desc
        limit 1
     ), old_coverage as (
       SELECT *
         FROM last_two_runs
        order by date asc
        limit 1
     )
         (
           select
             'test hits' as category,
             old_coverage.test_hits as old_coverage,
             new_coverage.test_hits as new_coverage,
             (new_coverage.test_hits - old_coverage.test_hits) as change_in_number,
             (new_coverage.percent_tested - old_coverage.percent_tested) as change_in_percent
             from old_coverage
                  , new_coverage
         )
         UNION
         (
           select
             'conf hits' as category,
             old_coverage.conf_hits as old_coverage,
             new_coverage.conf_hits as new_coverage,
             (new_coverage.conf_hits - old_coverage.conf_hits) as change_in_number,
             (new_coverage.percent_conf_tested - old_coverage.percent_conf_tested) as change_in_percent
             from
                 old_coverage
               , new_coverage
         )
         ;
   #+END_SRC

** 540: Change in Tests
   :PROPERTIES:
   :header-args:sql-mode+: :notangle ../apps/hasura/migrations/540_view_change_in_tests.up.sql
   :END:
   Meant to look at the last two test runs in database and calculate their change in coverage.  This was assuming we were loading multiple audit events.  Currently the flow is to load one baseline eent, and then compare the testing we do against it.  As such, removing this view until it is needed, to not confuse the tester working with apisnoop.
   #+NAME: Change in Tests
   #+begin_src sql-mode
   CREATE OR REPLACE VIEW "public"."change_in_tests" AS
     with last_two_runs as (
       select
         job, job_timestamp
         FROM
             bucket_job_swagger
        ORDER BY
          job_timestamp DESC
        LIMIT 2
     ),
       new_run as (
         SELECT
           job
           FROM last_two_runs
          order by job_timestamp DESC
          limit 1
       ),
       old_run as (
         SELECT
           job
           FROM
               last_two_runs
          order by job_timestamp asc
          limit 1
       )
         (
           SELECT
             test,
             'added' as status
             FROM
                 (
                   (
                     SELECT DISTINCT
                       split_part(useragent, '--', 2) as test
                       FROM
                           audit_event
                           INNER JOIN new_run on (audit_event.job = new_run.job)
                   )
                   EXCEPT
                   (
                     SELECT DISTINCT
                       split_part(useragent, '--', 2) as test
                       FROM
                           audit_event
                           INNER JOIN old_run on (audit_event.job = old_run.job)
                   )
                 ) added_tests
         )
         UNION
         (
           SELECT
             test,
             'removed' as status
             FROM
                 (
                   (
                     SELECT DISTINCT
                       split_part(useragent, '--', 2) as test
                       FROM
                           audit_event
                           INNER JOIN old_run on (audit_event.job = old_run.job)
                   )
                   EXCEPT
                   (
                     SELECT DISTINCT
                       split_part(useragent, '--', 2) as test
                       FROM
                           audit_event
                           INNER JOIN new_run on (audit_event.job = new_run.job)
                   )
                 ) removed_tests
         )
         ;

   #+end_src
* 600: Test Writing Views
** 600: Untested Stable Core Endpoints
   :PROPERTIES:
   :header-args:sql-mode+: :tangle ../apps/hasura/migrations/600_view_untested_stable_core_endpoints.up.sql
   :END:

#+NAME: untested endpoints
#+begin_src sql-mode
  CREATE OR REPLACE VIEW "public"."untested_stable_core_endpoints" AS
    SELECT
      ec.*,
      ao.description,
      ao.http_method,
      ao.k8s_action,
      ao.path
      FROM endpoint_coverage ec
             JOIN
             api_operation_material ao ON (ec.bucket = ao.bucket AND ec.job = ao.job AND ec.operation_id = ao.operation_id)
     WHERE ec.level = 'stable'
       AND ec.category = 'core'
       AND tested is false
       AND ao.deprecated IS false
       AND ec.job != 'live'
     ORDER BY hit desc
              ;
#+end_src

** 610: Endpoints Hit by New Test
   :PROPERTIES:
   :header-args:sql-mode+: :tangle ../apps/hasura/migrations/610_view_endpoints_hit_by_new_test.up.sql
   :END:
  #+NAME: endpoints hit by new test
  #+begin_src sql-mode
    CREATE VIEW "public"."endpoints_hit_by_new_test" AS
      WITH live_testing_endpoints AS (
        SELECT DISTINCT
          operation_id,
          useragent,
          count(*) as hits
          FROM
              audit_event
         GROUP BY operation_id, useragent
      ), baseline AS  (
        SELECT DISTINCT
          operation_id,
          tested,
          conf_tested
          FROM endpoint_coverage
         WHERE bucket != 'apisnoop'
      )
      SELECT DISTINCT
        lte.useragent,
        lte.operation_id,
        b.tested as hit_by_ete,
        lte.hits as hit_by_new_test
        FROM live_testing_endpoints lte
               JOIN baseline b ON (b.operation_id = lte.operation_id);
  #+end_src

** 620:Projected Change in Coverage
   :PROPERTIES:
   :header-args:sql-mode+: :tangle ../apps/hasura/migrations/620_view_projected_change_in_coverage.up.sql
   :END:
   #+NAME: PROJECTED Change in Coverage
   #+BEGIN_SRC sql-mode :results replace
     CREATE OR REPLACE VIEW "public"."projected_change_in_coverage" AS
      WITH baseline AS (
        SELECT *
          FROM
              stable_endpoint_stats
         WHERE job != 'live'
      ), test AS (
        SELECT
          COUNT(1) AS endpoints_hit
          FROM
              (
                SELECT
                  operation_id
          FROM audit_event
           WHERE useragent like 'live-test%'
          EXCEPT
          SELECT
            operation_id
          FROM
              endpoint_coverage
              WHERE tested is true
                    ) tested_endpoints
      ), coverage AS (
        SELECT
        baseline.test_hits AS old_coverage,
        (baseline.test_hits::int + test.endpoints_hit::int) AS new_coverage
        FROM baseline, test
      )
      SELECT
        'test_coverage' AS category,
        baseline.total_endpoints,
        coverage.old_coverage,
        coverage.new_coverage,
        (coverage.new_coverage - coverage.old_coverage) AS change_in_number
        FROM baseline, coverage
               ;
   #+END_SRC

* 700: Tests and UserAgents
** 710: tests
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/710_tests.up.sql
  :END:
*** Create
#+NAME: tests view
#+BEGIN_SRC sql-mode
  CREATE OR REPLACE VIEW "public"."tests" AS
    WITH raw_tests AS (
      SELECT audit_event.operation_id,
             audit_event.bucket,
             audit_event.job,
             array_to_string(regexp_matches(audit_event.useragent, '\[[a-zA-Z0-9\.\-:]*\]'::text, 'g'::text), ','::text) AS test_tag,
             split_part(audit_event.useragent, '--'::text, 2) AS test
        FROM audit_event
       WHERE ((audit_event.useragent ~~ 'e2e.test%'::text) AND (audit_event.job <> 'live'::text))
    )
    SELECT DISTINCT raw_tests.bucket,
                    raw_tests.job,
                    raw_tests.test,
                    array_agg(DISTINCT raw_tests.operation_id) AS operation_ids,
                    array_agg(DISTINCT raw_tests.test_tag) AS test_tags
      FROM raw_tests
     GROUP BY raw_tests.test, raw_tests.bucket, raw_tests.job;
#+END_SRC
** 720: useragents
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/720_useragents.up.sql
  :END:
*** Create
#+NAME: tests view
#+BEGIN_SRC sql-mode
  CREATE OR REPLACE VIEW "public"."useragents" AS
    WITH raw_useragents AS (
      SELECT audit_event.operation_id,
             audit_event.bucket,
             audit_event.job,
             audit_event.useragent
        FROM audit_event
       WHERE (audit_event.job <> 'live'::text)
    )
    SELECT DISTINCT raw_useragents.bucket,
                    raw_useragents.job,
                    raw_useragents.useragent,
                    array_agg(DISTINCT raw_useragents.operation_id) AS operation_ids
      FROM raw_useragents
     GROUP BY raw_useragents.useragent, raw_useragents.bucket, raw_useragents.job;
#+END_SRC
* 900: Tracking and Population
** 910: Populate Swaggers Up
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/910_load_and_populate_swaggers.up.sql
  :header-args:sql-mode+: :results silent
  :END:
  #+begin_src sql-mode :tangle no
    select * from load_swagger();
    --populate the apisnoop/live bucket/job to help when writing test functions
    select * from load_swagger(null, null, true);
  #+end_src
** 920: Populate Audits Up
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/920_populate_audit_events.up.sql
  :END:
  #+begin_src sql-mode :tangle no
    select * from load_audit_events();
    REFRESH MATERIALIZED VIEW api_operation_material;
    REFRESH MATERIALIZED VIEW api_operation_parameter_material;
    REFRESH MATERIALIZED VIEW endpoint_coverage_material;
  #+end_src
** 980: Comment on DB
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/980_comment_on_db.up.sql
  :END:
*** 100: Bucket Job Swagger
#+NAME: Comments on bucket_job_swagger
#+begin_src sql-mode
  COMMENT ON TABLE bucket_job_swagger IS 'metadata for audit events  and their respective swagger.json';
  COMMENT ON column bucket_job_swagger.ingested_at IS 'timestamp for when data added to table';
  COMMENT ON column bucket_job_swagger.bucket IS 'storage bucket for audit event test run and swagger';
  COMMENT ON column bucket_job_swagger.job IS 'specific job # of audit event test run';
  COMMENT ON column bucket_job_swagger.commit_hash IS 'git commit hash for this particular test run';
  COMMENT ON column bucket_job_swagger.passed IS 'whether test run passed';
  COMMENT ON column bucket_job_swagger.job_result IS 'whether test run was successful.';
  COMMENT ON column bucket_job_swagger.pod IS 'The pod this test was run on';
  COMMENT ON column bucket_job_swagger.job_version IS 'version of k8s on which this job was run';
  COMMENT ON column bucket_job_swagger.job_timestamp IS 'timestamp when job was run.  Will be different from ingested_at.';
  COMMENT ON column bucket_job_swagger.node_os_image IS 'id for which node image was used for test run';
  COMMENT ON column bucket_job_swagger.node_os_image IS 'id for which master os image was used for test run';
  COMMENT ON column bucket_job_swagger.swagger IS 'raw json of the open api spec for k8s as of the commit hash for this test run.';
#+end_src
*** 110: raw_audit_event
    #+begin_src sql-mode
      COMMENT ON TABLE  raw_audit_event IS 'a record for each audit event in an audit log';
      COMMENT ON COLUMN raw_audit_event.bucket IS 'The testrun bucket for the event';
      COMMENT ON COLUMN raw_audit_event.job IS 'The testrun job for the event';
      COMMENT ON COLUMN raw_audit_event.audit_id IS 'The id for the event';
      COMMENT ON COLUMN raw_audit_event.stage IS 'stage of event';
      COMMENT ON COLUMN raw_audit_event.event_verb IS 'verb of event';
      COMMENT ON COLUMN raw_audit_event.request_uri IS 'cluster uri that event requested';
      COMMENT ON COLUMN raw_audit_event.operation_id IS 'operation_id hit by event';
      COMMENT ON COLUMN raw_audit_event.data IS 'full raw data of event';
    #+end_src
*** 200: api_operation_material
    #+begin_src sql-mode
      COMMENT ON MATERIALIZED VIEW api_operation_material IS 'details on each operation_id as taken from the openAPI spec';
      COMMENT ON COLUMN api_operation_material.operation_id IS 'Also referred to as endpoint.  Name for the action at a given path';
      COMMENT ON COLUMN api_operation_material.level IS 'Alpha, Beta, or Stable. The level of stability of an endpoint';
      COMMENT ON COLUMN api_operation_material.category IS 'will either be analogous with the k8s group or "core".';
      COMMENT ON COLUMN api_operation_material.k8s_group IS 'kubernetes group this operation_id belongs to';
      COMMENT ON COLUMN api_operation_material.k8s_version IS 'kubernetes version (e.g alpha or beta or stable)';
      COMMENT ON COLUMN api_operation_material.k8s_kind IS 'kubernetes kind';
      COMMENT ON COLUMN api_operation_material.deprecated IS 'whether operation_id has deprecated in its description';
      COMMENT ON COLUMN api_operation_material.description IS 'description of operation_id';
      COMMENT ON COLUMN api_operation_material.http_method IS 'http equivalent for operation, e.g. GET, POST, DELETE';
      COMMENT ON COLUMN api_operation_material.k8s_action IS 'the k8s analog for the http_method';
      COMMENT ON COLUMN api_operation_material.event_verb IS 'a more succinct description of k8s_action';
      COMMENT ON COLUMN api_operation_material.path IS 'location in cluster of endpoint for this operation_id';
      COMMENT ON COLUMN api_operation_material.consumes IS 'what the operation_id consumes';
      COMMENT ON COLUMN api_operation_material.responses IS 'how the operation_id responds';
      COMMENT ON COLUMN api_operation_material.parameters IS 'parameters of operation_id';
      COMMENT ON COLUMN api_operation_material.tags IS 'tags of operation_id';
      COMMENT ON COLUMN api_operation_material.schemes IS 'schemes of operation_id';
      COMMENT ON COLUMN api_operation_material.regex IS 'regex pattern for how to match to this operation_id. Likely  not needed anymore.';
      COMMENT ON COLUMN api_operation_material.bucket IS 'the testrun bucket this operation_id belongs to';
      COMMENT ON COLUMN api_operation_material.job IS 'the testrun job this operation_id belongs to';

    #+end_src
*** 220: api_operation_material
    #+begin_src sql-mode
      COMMENT ON MATERIALIZED VIEW api_operation_parameter_material IS 'the parameters for each operation_id in open API spec';
      COMMENT ON column api_operation_parameter_material.param_op IS 'the operation_id this parameter belongs to';
      COMMENT ON column api_operation_parameter_material.param_name IS 'the name of the parameter';
      COMMENT ON column api_operation_parameter_material.param_schema IS 'schema for param, if available, otherwise its type';
      COMMENT ON column api_operation_parameter_material.required IS 'whether operation_id requires this parameter';
      COMMENT ON column api_operation_parameter_material.param_description IS 'description given for parameter';
      COMMENT ON column api_operation_parameter_material.unique_items IS 'whether parameter has unique items';
      COMMENT ON column api_operation_parameter_material.in IS 'value of "in" key in parameter entry';
      COMMENT ON column api_operation_parameter_material.bucket IS 'testrun bucket of operation_id this parameter belongs to';
      COMMENT ON column api_operation_parameter_material.job IS 'testrun job of operation_id this parameter belongs to';
      COMMENT ON column api_operation_parameter_material.entry IS 'full json blog of parameter entry';
    #+end_src
*** 300: audit_event
    #+begin_src sql-mode
      COMMENT ON VIEW audit_event IS 'a record for each audit event in an audit log';
      COMMENT ON COLUMN audit_event.audit_id IS 'The id for the event';
      COMMENT ON COLUMN audit_event.bucket IS 'The testrun bucket for the event';
      COMMENT ON COLUMN audit_event.job IS 'The testrun job for the event';
      COMMENT ON COLUMN audit_event.event_level IS 'level of event';
      COMMENT ON COLUMN audit_event.event_stage IS 'stage of event';
      COMMENT ON COLUMN audit_event.operation_id IS 'operation_id hit by event';
      COMMENT ON COLUMN audit_event.param_schema IS 'parameter schema for operation_id';
      COMMENT ON COLUMN audit_event.api_version IS 'k8s api version used in testrun';
      COMMENT ON COLUMN audit_event.request_uri IS 'cluster uri that event requested';
      COMMENT ON COLUMN audit_event.useragent IS 'useragent making request';
      COMMENT ON COLUMN audit_event.object_namespace IS 'namespace from objectRef of event';
      COMMENT ON COLUMN audit_event.object_type IS 'resource from objectRef of event';
      COMMENT ON COLUMN audit_event.object_group IS 'apiGroup from objectRef of event';
      COMMENT ON COLUMN audit_event.object_ver IS 'apiVersion from objectRef of event';
      COMMENT ON COLUMN audit_event.source_ips IS 'sourceIPs of event';
      COMMENT ON COLUMN audit_event.annotations IS 'annotations of event';
      COMMENT ON COLUMN audit_event.request_object IS 'full requestObject from event';
      COMMENT ON COLUMN audit_event.response_object IS 'full responseObject from event';
      COMMENT ON COLUMN audit_event.stage_timestamp IS 'timestamp of event';
      COMMENT ON COLUMN audit_event.request_received_timestamp IS 'timestamp when request received';
      COMMENT ON COLUMN audit_event.data IS 'full raw data of event';
    #+end_src
*** 500: endpoint_coverage
    #+begin_src sql-mode
      COMMENT ON VIEW endpoint_coverage IS 'the test hits and conformance test hits per operation_id & other useful details';
      COMMENT ON COLUMN endpoint_coverage.date IS 'Date of test run according to its metadata';
      COMMENT ON COLUMN endpoint_coverage.bucket IS 'The testrun bucket for the event';
      COMMENT ON COLUMN endpoint_coverage.job IS 'The testrun job for the event';
      COMMENT ON COLUMN endpoint_coverage.operation_id IS 'operation_id of endpoint.  Two terms used interchangably';
      COMMENT ON COLUMN endpoint_coverage.level IS 'Alpha, Beta, or Stable. The level of stability of an endpoint';
      COMMENT ON COLUMN endpoint_coverage.category IS 'will either be analogous with the k8s group or "core".';
      COMMENT ON COLUMN endpoint_coverage.group IS 'kubernetes group this operation_id belongs to';
      COMMENT ON COLUMN endpoint_coverage.version IS 'kubernetes version (e.g alpha or beta or stable)';
      COMMENT ON COLUMN endpoint_coverage.kind IS 'kubernetes kind';
      COMMENT ON COLUMN endpoint_coverage.tested IS 'boolean on whether any e2e. useragent hits this endpoint';
      COMMENT ON COLUMN endpoint_coverage.conf_tested IS 'boolean on whether any useragent with [Conformance] in name hits endpoint';
      COMMENT ON COLUMN endpoint_coverage.hit IS 'boolean whether endpoint hit by any useragent';
    #+end_src

*** 520: stable_endpoint_stats
    #+begin_src sql-mode
      COMMENT ON VIEW stable_endpoint_stats IS 'coverage stats for entire test run, looking only at its stable endpoints';
      COMMENT ON COLUMN stable_endpoint_stats.job IS 'The testrun job';
      COMMENT ON COLUMN stable_endpoint_stats.date IS 'Date of test run according to its metadata';
      COMMENT ON COLUMN stable_endpoint_stats.total_endpoints IS 'number of stable endpoints in this test run';
      COMMENT ON COLUMN stable_endpoint_stats.test_hits IS 'number of stable, tested endpoints in this test run';
      COMMENT ON COLUMN stable_endpoint_stats.conf_hits IS 'number of stable, conformance tested endpoints in this test run';
      COMMENT ON COLUMN stable_endpoint_stats.percent_tested IS 'percent of total, stable endpoints in the run that are tested';
      COMMENT ON COLUMN stable_endpoint_stats.percent_conf_tested IS 'percent of stable endpoints in the run that are conformance tested';
    #+end_src

*** 600: untested_stable_core_endpoints
    #+begin_src sql-mode
      COMMENT ON VIEW untested_stable_core_endpoints IS 'list stable core endpoints not hit by any tests, according to their test run';
      COMMENT ON COLUMN untested_stable_core_endpoints.date IS 'Date of test run according to its metadata';
      COMMENT ON COLUMN untested_stable_core_endpoints.bucket IS 'The testrun bucket for the event';
      COMMENT ON COLUMN untested_stable_core_endpoints.job IS 'The testrun job for the event';
      COMMENT ON COLUMN untested_stable_core_endpoints.operation_id IS 'operation_id of endpoint.  Two terms used interchangably';
      COMMENT ON COLUMN untested_stable_core_endpoints.level IS 'Alpha, Beta, or Stable. The level of stability of an endpoint';
      COMMENT ON COLUMN untested_stable_core_endpoints.category IS 'will either be analogous with the k8s group or "core".';
      COMMENT ON COLUMN untested_stable_core_endpoints.group IS 'kubernetes group this operation_id belongs to';
      COMMENT ON COLUMN untested_stable_core_endpoints.version IS 'kubernetes version (e.g alpha or beta or stable)';
      COMMENT ON COLUMN untested_stable_core_endpoints.kind IS 'kubernetes kind';
      COMMENT ON COLUMN untested_stable_core_endpoints.description IS 'description of operation_id';
      COMMENT ON COLUMN untested_stable_core_endpoints.http_method IS 'http equivalent for operation, e.g. GET, POST, DELETE';
      COMMENT ON COLUMN untested_stable_core_endpoints.k8s_action IS 'the k8s analog for the http_method';
      COMMENT ON COLUMN untested_stable_core_endpoints.path IS 'location in cluster of endpoint for this operation_id';
    #+end_src

*** 610: endpoints_hit_by_new_test
    #+begin_src sql-mode
      COMMENT ON VIEW endpoints_hit_by_new_test IS 'list endpoints hit during our live auditing alongside their current test coverage';
      COMMENT ON COLUMN endpoints_hit_by_new_test.useragent IS 'the useragent that hit the endpoint as captured by apisnoop';
      COMMENT ON COLUMN endpoints_hit_by_new_test.operation_id IS 'the operation_id hit';
      COMMENT ON COLUMN endpoints_hit_by_new_test.hit_by_ete IS 'number of times this endpoint is hit, according to latest test run';
      COMMENT ON COLUMN endpoints_hit_by_new_test.hit_by_new_test IS 'number of times the useragent hit this endpoint, according to apisnoop';
    #+end_src

*** 620: projected_change_in_coverage
    #+begin_src sql-mode
      COMMENT ON VIEW projected_change_in_coverage IS 'overview of coverage stats if the e2e suite included your tests';
      COMMENT ON COLUMN projected_change_in_coverage.total_endpoints IS 'number of stable, core endpoints as of the latest test run';
      COMMENT ON COLUMN projected_change_in_coverage.old_coverage IS 'number of stable, core endpoints hit by tests, as of the latest test run';
      COMMENT ON COLUMN projected_change_in_coverage.new_coverage IS 'number of stable, core endpoints hit by tests, when including those hit by your tests';
      COMMENT ON COLUMN projected_change_in_coverage.change_in_number IS 'new_coverage less old_coverage';
    #+end_src

** 998: Tracking Tables
   :PROPERTIES:
   :header-args:yaml+: :tangle ../apps/hasura/migrations/998_tracking.up.yaml
   :header-args:yaml+: :comments org
   :END:
*** bucket_job_swagger
#+NAME: track api_swagger
#+BEGIN_SRC yaml
- type: track_table
  args:
    schema: public
    name: bucket_job_swagger
#+END_SRC
*** raw_audit_event
#+NAME: track raw_audit_event
#+BEGIN_SRC yaml
- type: track_table
  args:
    schema: public
    name: raw_audit_event
#+END_SRC
*** audit_event
 #+NAME: track audit_event
 #+BEGIN_SRC yaml
 - type: track_table
   args:
     schema: public
     name: audit_event
 #+END_SRC

*** api_operation
 #+NAME: track api_operation
 #+BEGIN_SRC yaml
 - type: track_table
   args:
     schema: public
     name: api_operation
 #+END_SRC
*** endpoint_coverage
 #+NAME: track endpoint_coverage
 #+BEGIN_SRC yaml
 - type: track_table
   args:
     schema: public
     name: endpoint_coverage
 #+END_SRC
*** stable_endpoint_stats
 #+NAME: track endpoint_stats
 #+BEGIN_SRC yaml
 - type: track_table
   args:
     schema: public
     name: stable_endpoint_stats
 #+END_SRC
*** untested_stable_core_endpoints
 #+NAME: track untested_stable_core_endpoints
 #+BEGIN_SRC yaml
 - type: track_table
   args:
     schema: public
     name: untested_stable_core_endpoints
 #+END_SRC
*** endpoints_hit_by_new_test
 #+NAME: track endpoints_hit_by_new_test
 #+BEGIN_SRC yaml
 - type: track_table
   args:
     schema: public
     name: endpoints_hit_by_new_test
 #+END_SRC
*** projected_change_in_coverage
 #+NAME: track projected_change_in_coverage
 #+BEGIN_SRC yaml
 - type: track_table
   args:
     schema: public
     name: projected_change_in_coverage
 #+END_SRC
*** tests
 #+NAME: track tests
 #+BEGIN_SRC yaml
 - type: track_table
   args:
     schema: public
     name: tests
 #+END_SRC
*** useragents
 #+NAME: track useragents
 #+BEGIN_SRC yaml
 - type: track_table
   args:
     schema: public
     name: useragents
 #+END_SRC
** 999: replace metadata
   :PROPERTIES:
   :header-args:yaml+: :notangle ../apps/hasura/migrations/999_replace_metadata.up.yaml
   :header-args:yaml+: :comments org
   :END:
#+NAME: replace metadata
#+begin_src yaml
- type: replace_metadata
  args: {"functions":[],"remote_schemas":[],"query_collections":[],"allowlist":[],"version":2,"tables":[{"table":"api_operation","is_enum":false,"configuration":{"custom_root_fields":{"select":null,"select_by_pk":null,"select_aggregate":null,"insert":null,"update":null,"delete":null},"custom_column_names":{}},"object_relationships":[],"array_relationships":[],"insert_permissions":[],"select_permissions":[],"update_permissions":[],"delete_permissions":[],"event_triggers":[],"computed_fields":[]},{"table":"audit_event","is_enum":false,"configuration":{"custom_root_fields":{"select":null,"select_by_pk":null,"select_aggregate":null,"insert":null,"update":null,"delete":null},"custom_column_names":{}},"object_relationships":[],"array_relationships":[],"insert_permissions":[],"select_permissions":[],"update_permissions":[],"delete_permissions":[],"event_triggers":[],"computed_fields":[]},{"table":"bucket_job_swagger","is_enum":false,"configuration":{"custom_root_fields":{"select":null,"select_by_pk":null,"select_aggregate":null,"insert":null,"update":null,"delete":null},"custom_column_names":{}},"object_relationships":[],"array_relationships":[],"insert_permissions":[],"select_permissions":[],"update_permissions":[],"delete_permissions":[],"event_triggers":[],"computed_fields":[]},{"table":"endpoint_coverage","is_enum":false,"configuration":{"custom_root_fields":{"select":null,"select_by_pk":null,"select_aggregate":null,"insert":null,"update":null,"delete":null},"custom_column_names":{}},"object_relationships":[{"using":{"manual_configuration":{"remote_table":"api_operation","column_mapping":{"bucket":"bucket","operation_id":"operation_id","job":"job"}}},"name":"details","comment":null}],"array_relationships":[],"insert_permissions":[],"select_permissions":[],"update_permissions":[],"delete_permissions":[],"event_triggers":[],"computed_fields":[]},{"table":"endpoints_hit_by_new_test","is_enum":false,"configuration":{"custom_root_fields":{"select":null,"select_by_pk":null,"select_aggregate":null,"insert":null,"update":null,"delete":null},"custom_column_names":{}},"object_relationships":[],"array_relationships":[],"insert_permissions":[],"select_permissions":[],"update_permissions":[],"delete_permissions":[],"event_triggers":[],"computed_fields":[]},{"table":"projected_change_in_coverage","is_enum":false,"configuration":{"custom_root_fields":{"select":null,"select_by_pk":null,"select_aggregate":null,"insert":null,"update":null,"delete":null},"custom_column_names":{}},"object_relationships":[],"array_relationships":[],"insert_permissions":[],"select_permissions":[],"update_permissions":[],"delete_permissions":[],"event_triggers":[],"computed_fields":[]},{"table":"raw_audit_event","is_enum":false,"configuration":{"custom_root_fields":{"select":null,"select_by_pk":null,"select_aggregate":null,"insert":null,"update":null,"delete":null},"custom_column_names":{}},"object_relationships":[],"array_relationships":[],"insert_permissions":[],"select_permissions":[],"update_permissions":[],"delete_permissions":[],"event_triggers":[],"computed_fields":[]},{"table":"stable_endpoint_stats","is_enum":false,"configuration":{"custom_root_fields":{"select":null,"select_by_pk":null,"select_aggregate":null,"insert":null,"update":null,"delete":null},"custom_column_names":{}},"object_relationships":[],"array_relationships":[],"insert_permissions":[],"select_permissions":[],"update_permissions":[],"delete_permissions":[],"event_triggers":[],"computed_fields":[]},{"table":"tests","is_enum":false,"configuration":{"custom_root_fields":{"select":null,"select_by_pk":null,"select_aggregate":null,"insert":null,"update":null,"delete":null},"custom_column_names":{}},"object_relationships":[],"array_relationships":[],"insert_permissions":[],"select_permissions":[],"update_permissions":[],"delete_permissions":[],"event_triggers":[],"computed_fields":[]},{"table":"untested_stable_core_endpoints","is_enum":false,"configuration":{"custom_root_fields":{"select":null,"select_by_pk":null,"select_aggregate":null,"insert":null,"update":null,"delete":null},"custom_column_names":{}},"object_relationships":[],"array_relationships":[],"insert_permissions":[],"select_permissions":[],"update_permissions":[],"delete_permissions":[],"event_triggers":[],"computed_fields":[]},{"table":"useragents","is_enum":false,"configuration":{"custom_root_fields":{"select":null,"select_by_pk":null,"select_aggregate":null,"insert":null,"update":null,"delete":null},"custom_column_names":{}},"object_relationships":[],"array_relationships":[],"insert_permissions":[],"select_permissions":[],"update_permissions":[],"delete_permissions":[],"event_triggers":[],"computed_fields":[]}]}
#+end_src

* Footnotes
* scratch
  
  https://prow.k8s.io/view/gcs/kubernetes-jenkins/logs/
  #+begin_src sql-mode :results output replace
     -- select * from load_audit_events('ci-kubernetes-e2e-gci-gce', '1232358105564581890');
    -- select * from raw_audit_event limit 10;
     -- select bucket, job, job_timestamp from bucket_job_swagger;
  #+end_src

  #+RESULTS:
  #+begin_SRC example
  ERROR:  function load_swagger(unknown, unknown) does not exist
  LINE 1: select * from load_swagger('ci-kubernetes-e2e-gci-gce', '123...
                        ^
  HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
  #+end_SRC
* aoeu
  #+begin_example
"ERROR:  ImportError: cannot import name ’load_audit_events’ from ’snoopUtils’ (/usr/local/lib/python3.7/dist-packages/snoopUtils.py)
CONTEXT:  Traceback (most recent call last):
  PL/Python function \"load_audit_events\", line 2, in <module>
    from snoopUtils import determine_bucket_job, load_audit_events, insert_audit_events_into_db
PL/Python function \"load_audit_events\"
  #+end_example
  #+begin_src sql-mode
select * from load_audit_events('ci-kubernetes-e2e-gci-gce','1232358105564581890');
  #+end_src
