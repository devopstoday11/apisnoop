#+AUTHOR: ii team
#+TITLE: APIsnoop BOT
#+DATE: 15 August 2019
#+INCLUDE: "config.org"
#+TODO: TODO(t) NEXT(n) IN-PROGRESS(i) BLOCKED(b) | TADA(d)
#+ARCHIVE: archive/meta_bot.archive.org::
#+PROPERTY: header-args:sql-mode+ :results silent


* Purpose
* Welcome, ii dev!
If you're reading this org file, from within spacemacs, and expecting to do your work from within an org-file...then welcome, fellow ii dev (or honorary ii dev)!

To ensure you're set up properly, you want to run down this checklist:
** TADA [0%] Ready to Work!
    :PROPERTIES:
    :RESET_CHECK_BOXES: t
    :LOGGING: nil
    :END:
- [ ] Ensure you are reading this, and working, from within a tmate session on sharing.io.
  how do to do this is outside scope of this org, but ensure you've ssh'ed into sharing.io, and started a tmate session.
  
  #+begin_src tmate
  echo "Hello, $USER!"
  #+end_src
- [ ] Start up dev environment with the ii env variables
  : open a new tmate pane (ctrl+b c) or run `, ,` within the block to run the following:
  #+NAME: start docker containers
  #+BEGIN_SRC tmate :noweb eval :session apisnoop:docker
    # These  sequences are to allow you to use this as an iteration loop
    
    
    
    cd ~/ii/apisnoop/apps # docker-compose.yaml is here
    # Retangle / export code from org/documentation usually into ./apps/*
    # the eval: local vars set this emacs instance server-name to apisnoop
    rm hasura/migrations/* # just in case we move files around
    echo 'tangling files'
    emacsclient -s apisnoop -e "(org-babel-tangle-file \"../org/meta_bot.org\")"
    emacsclient -s apisnoop -e "(org-babel-tangle-file \"../org/tables_and_views_bot.org\")"
    # We have a sharing.io check to ensure everyone get's there own PORTS
    . .loadenv
    # Bring down anything you may already have up
    docker-compose down -t 0 --remove-orphans
    docker-compose rm -f
    # Build your containers
    docker-compose build
    # Bring up your containers
    docker-compose up --remove-orphans
  #+END_SRC
- [ ] perform migrations (if not using .cli-migrations hasura image)
  #+NAME: Perform Migrations 
  #+BEGIN_SRC tmate :noweb eval :session apisnoop:migrations
    cd ~/ii/apisnoop/apps/hasura
    hasura migrate apply
    hasura migrate status
  #+END_SRC
- [ ] Connect to your postgres db and confirm connection 

  connect with this elisp block
  #+NAME: Connect org to postgres
  #+BEGIN_SRC emacs-lisp :results silent
    (sql-connect "apisnoop" (concat "*SQL: postgres:data*"))
  #+END_SRC
  
  check your connection with \conninfo.  If successful you should see this message in your minibuffer
  : You are connected to database "apisnoop" as user "apisnoop" on host "localhost" at port "10041".

  #+NAME: Test Connection
  #+BEGIN_SRC sql-mode :results silent
  \conninfo
  #+END_SRC
- [ ] Test that audit events and swagger loaded.
  If  both loaded successfully, then the below query should return two results, that show hits and conf_hits for both.  It is possible that one of the audit events did not load (and so you'd see 0 hits across the board).  If that's the case, run the load_audit_event section again for just that event, refresh your views, and try again.  It'll get there!
  
  #+NAME: Endpoint Stats
  #+BEGIN_SRC sql-mode :results silent
   select * from stable_endpoint_stats
    ;
  #+END_SRC
- [ ] IF NEEDED: Load swaggers

  #+NAME: Load Swaggers
  #+begin_src sql-mode
    select * from load_bucket_job_swagger_via_curl('ci-kubernetes-e2e-gci-gce', '1188637253832806405');
    REFRESH MATERIALIZED VIEW api_operation_material;
    REFRESH MATERIALIZED VIEW api_operation_parameter_material;
  #+end_src

  #+RESULTS: Load Swaggers
  #+begin_src sql-mode
                                        load_bucket_job_swagger_via_curl                                       
  -------------------------------------------------------------------------------------------------------------
   something went wrong, likely this: duplicate key value violates unique constraint "bucket_job_swagger_pkey"
  (1 row)

  #+end_src
  
  #+begin_src sql-mode
  select distinct bucket, job from raw_audit_event;
  #+end_src

  #+RESULTS:
  #+begin_src sql-mode
   bucket | job 
  --------+-----
  (0 rows)

  #+end_src

- [ ] IF NEEDED: load audit_events
  #+NAME: Load Audit Events
  #+begin_src sql-mode
    select * from load_audit_events('ci-kubernetes-e2e-gci-gce', '1188637253832806405');
    -- select * from load_audit_events('ci-kubernetes-e2e-gci-gce', '1183553991464718336');
  #+end_src

  #+RESULTS: Load Audit Events
  #+begin_src sql-mode
   load_audit_events 
  -------------------

  (1 row)

  #+end_src
- [ ] IF NEEDED: Refresh Materialized Views
  we have a few materialized views that will need to run after we loaded thangs up
  #+NAME: refresh our materialized views
  #+begin_src sql-mode
    REFRESH MATERIALIZED VIEW api_operation_material;
    REFRESH MATERIALIZED VIEW api_operation_parameter_material;
  #+end_src

  #+RESULTS: refresh our materialized views
  #+begin_src sql-mode
  REFRESH MATERIALIZED VIEW
  #+end_src
- [ ] Start up a psql interpreter with the ii env variables
  It can be useful to have psql up to run queries directly in the interpreter.
  : open a new tmate pane (ctrl+b c), or navigate to one used to load cached audit event
  #+NAME: start docker containers
  #+BEGIN_SRC tmate :noweb eval :session apisnoop:psql
    cd ~/ii/apisnoop/apps # docker-compose.yaml is here
    . .loadenv
    psql
  #+END_SRC
- [ ] Test your  hasura endpoint is up
  If all is working right, you should be able to visit =$YOURUSERNAME-hasura.sharing.io=.  This will give a public graphql explorer for our views.  This means we can also post queries to our graphql endpoint and get results back.
  
  Let's test that by seeing the change in coverage between our two audit events.  If it works, you should see json returned in your minibuffer, showing change in conformance and test coverage.
  
  #+NAME: Check Hasura is Up
  #+begin_src shell :results silent
    curl \
        -X POST \
        -H "Content-Type: application/json" \
        --data '{ "query": "{change_in_coverage {category change_in_number change_in_percent new_coverage old_coverage }}
    " }' \
        https://$USER-hasura.sharing.io/v1/graphql\
    | jq .
  #+end_src
- [ ] Get a drink of water and mark this todo as DONE
  You're all set up and ready to go, but hydration is important!  Get a drink of water, stretch, and recharge before you crush it today!  
  Also, =gh gh gh= to go back to the top then =,TTd= to mark this task as DONE!

* Tables and Views
  All the critical tables and views for apisnoop are documented and tangled from [[file:tables_and_views.org][tables_and_views.org]].
  Explorations into new views, or ad-hoc queries to help answer a problem, are done in our ~explorations~ folder.
* Apps
** .env
#+BEGIN_SRC shell :tangle ../apps/.env_sample
  PGADMIN_PORT=9001
  HASURA_PORT=9000
  # UID based PGPORT used to expose per user postgresql ports on same box
  PGPORT=54321
  PGHOST=localhost
  PGUSER=apisnoop
  PGPASS=s3cr3tsauc3
  PGDATABASE=apisnoop
  CONN="host=127.0.0.1 port=54321 user=apisnoop dbname=apisnoop password=s3cr3tsauc3 sslmode=disable client_encoding=UTF8"
#+END_SRC
** .loadenv
#+NAME: .loadenv
#+BEGIN_SRC shell :tangle ../apps/.loadenv
  # If we are on sharing.io, use UID based *_PORTs
  if [ $(hostname) = "sharing.io" ]
  then
      # Overwriting .env based on \*-${USER}.sharing.io
      echo Using sharing.io setup 1>&2
  (
      # UID based *_PORT used to expose per user postgresql,hasura, and pgadmin ports on same box
      if [ "$KOMPOSE" = "true" ]
      then
              echo Using kompose https://$USER-hasura.apisnoop.io 1>&2
              echo PGPORT=5432
              echo HASURA_PORT=8080
              echo endpoint: https://$USER-hasura.apisnoop.io > $PWD/hasura/config.yaml
      else
              echo Using docker-compose https://$USER-hasura.sharing.io 1>&2
              echo PGPORT=$(id -u)1
              echo HASURA_PORT=$(id -u)0
              echo endpoint: https://$USER-hasura.sharing.io > $PWD/hasura/config.yaml
      fi

      # for running in docker-compose, different localhost:port per user id same
      # for running in docker-compose, different localhost:port per user id same
      # echo PG_CONTAINER_PORT=$PGPORT
      # for running in kompose
      echo PGADMIN_PORT=$(id -u)2
      echo PGHOST=localhost
      echo PGDATABASE=apisnoop
      echo PGUSER=apisnoop
      echo PGPASS=s3cr3tsauc3
      echo PGPASSFILE=$PWD/pgpass
      echo COMPOSE_PROJECT_NAME=apisnoop_$USER
      TAG=$(TZ='Pacific/Auckland'; export TZ ; date +%F-%H-%M)
      echo TAG=$TAG
      # echo HASURA_IMAGE=raiinbow/hasura:$TAG
      # echo POSTGRES_IMAGE=raiinbow/postgres:$TAG
      # echo $PGHOST:$PGPORT:$PGDATABASE:$PGUSER:$PGPASS > $PWD/pgpass
      echo GOOGLE_APPLICATION_CREDENTIALS=$HOME/.gcreds.json
      echo GKS_ZONE="australia-southeast1-a"
      echo GCS_CLUSTER="single-node-cluster"
      echo APISNOOP_NAMESPACE="apisnoop-$USER"
      echo TRAEFIK_NAMESPACE="kube-system"
      echo TRAEFIK_DEPLOYMENT="ii-traefik"
      echo SQLITE_DB=$PWD/sqlite/raiinbow.db
  ) > .env
      export $(grep -v '^#' .env | xargs -d '\n')
      gcloud container clusters get-credentials ${GCS_CLUSTER} --zone ${GKS_ZONE} 2> /dev/null || echo cluster gcreds error
      kubectl config set-context $(kubectl config current-context) --namespace=${APISNOOP_NAMESPACE} 2>&1 > /dev/null
  else
      cp .env_sample .env
      echo SQLITE_DB=$PWD/raiinbow.db >> .env
      cp hasura/config_sample.yaml hasura/config.yaml
      export $(grep -v '^#' .env | xargs -d '\n')
  fi

  PGPASSFILE=$(pwd)/pgpass
  echo $PGHOST:$PGPORT:$PGDATABASE:$PGUSER:$PGPASS > $PGPASSFILE
  chmod 600 $PGPASSFILE
  export CONN="host=127.0.0.1 port=$PGPORT user=$PGUSER dbname=$PGDATABASE password=$PGPASS sslmode=disable client_encoding=UTF8"
#+END_SRC

** Hasura
*** config.yml

#+NAME: hasura config sample
#+BEGIN_SRC text :tangle ../apps/hasura/config_sample.yaml :noweb yes
endpoint: http://localhost:9000
#+END_SRC
*** Dockerfile
#+NAME: apisnoop hasura dockerfile
#+BEGIN_SRC dockerfile :tangle ../apps/hasura/Dockerfile
  FROM hasura/graphql-engine:v1.0.0-beta.8.cli-migrations
  # FROM hasura/graphql-engine:v1.0.0-beta.8
  MAINTAINER Hippie Hacker <hh@ii.coop>
  COPY ./migrations /hasura-migrations
#+END_SRC

#+RESULTS:
#+begin_EXAMPLE
Sending build context to Docker daemon  58.88kB
Step 1/3 : FROM hasura/graphql-engine:v1.0.0-beta.8.cli-migrations
 ---> b24aef502cb9
Step 2/3 : MAINTAINER Hippie Hacker <hh@ii.coop>
 ---> Using cache
 ---> 82ecaff3a6e8
Step 3/3 : COPY ./migrations /hasura-migrations
 ---> 436ba05fbc37
Successfully built 436ba05fbc37
Successfully tagged raiinbow/hasura:2019-12-03-14-17
The push refers to repository [docker.io/raiinbow/hasura]
ec8d150bcf5f: Preparing
aa0837a119d4: Preparing
aa0837a119d4: Preparing
52b3e4e57c36: Preparing
3de0b48304b2: Preparing
aa0837a119d4: Layer already exists
3de0b48304b2: Layer already exists
52b3e4e57c36: Layer already exists
ec8d150bcf5f: Pushed
2019-12-03-14-17: digest: sha256:3b91b57fbcd4b8755b57d4580a608b2ad73e31e7d819352a7203278f30568d95 size: 1367
#+end_EXAMPLE

*** Build App
 #+NAME: build hasura container image
 #+begin_src shell :dir ../apps/hasura
   export TAG=$(TZ='Pacific/Auckland'; export TZ ; date +%F-%H-%M)
   docker build -t raiinbow/hasura:$TAG .
   docker push raiinbow/hasura:$TAG
 #+end_src

#+NAME: build hasura container image
#+begin_src shell :dir ../apps/hasura
  export TAG=$(TZ='Pacific/Auckland'; export TZ ; date +%F-%H-%M)
  docker build -t raiinbow/hasura:$TAG .
  docker push raiinbow/hasura:$TAG
#+end_src

#+RESULTS: build hasura container image
#+begin_EXAMPLE
Sending build context to Docker daemon   68.1kB
Step 1/3 : FROM hasura/graphql-engine:v1.0.0-beta.8.cli-migrations
 ---> b24aef502cb9
Step 2/3 : MAINTAINER Hippie Hacker <hh@ii.coop>
 ---> Using cache
 ---> 82ecaff3a6e8
Step 3/3 : COPY ./migrations /hasura-migrations
 ---> Using cache
 ---> bb2e8f81a468
Successfully built bb2e8f81a468
Successfully tagged raiinbow/hasura:2019-12-08-21-00
The push refers to repository [docker.io/raiinbow/hasura]
4de5b6dfe50f: Preparing
aa0837a119d4: Preparing
aa0837a119d4: Preparing
52b3e4e57c36: Preparing
3de0b48304b2: Preparing
3de0b48304b2: Layer already exists
4de5b6dfe50f: Layer already exists
52b3e4e57c36: Layer already exists
aa0837a119d4: Layer already exists
2019-12-08-21-00: digest: sha256:66833e0081ce1549fbdcedd5c38cf1c769086fde6088336df8db83a778bec37b size: 1368
#+end_EXAMPLE

** Postgres
*** init-apisnoop-db.sh

 #+NAME: init apisnoop db
 #+BEGIN_SRC sql-mode :tangle ../apps/postgres/initdb/apisnoop_db.sql
   -- ERROR:  database "apisnoop" already exists
   -- create database apisnoop;
   -- create user myuser with encrypted password 'mypass';
   grant all privileges on database apisnoop to apisnoop;
   create role dba with superuser noinherit;
   grant dba to apisnoop;
   \connect apisnoop
   -- we generate uuids
   CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
   -- we write python functions
   CREATE EXTENSION IF NOT EXISTS plpython3u;
   -- we write shell functions
   CREATE EXTENSION IF NOT EXISTS plsh;
   -- regex indexes required trgm
   CREATE EXTENSION IF NOT EXISTS pg_trgm;
   -- paths need an index too
   CREATE EXTENSION IF NOT EXISTS ltree;
   -- hasura needs hash functions
   CREATE EXTENSION IF NOT EXISTS pgcrypto;
   -- hasura db catalog and views
   CREATE SCHEMA IF NOT EXISTS hdb_catalog;
   CREATE SCHEMA IF NOT EXISTS hdb_views;
   -- make the user an owner of system schemas
   ALTER SCHEMA hdb_catalog OWNER TO apisnoop;
   ALTER SCHEMA hdb_views OWNER TO apisnoop;
   GRANT SELECT ON ALL TABLES IN SCHEMA information_schema TO apisnoop;
   GRANT SELECT ON ALL TABLES IN SCHEMA pg_catalog TO apisnoop;
   GRANT USAGE ON SCHEMA public TO apisnoop;
   GRANT ALL ON ALL TABLES IN SCHEMA public TO apisnoop;
   GRANT ALL ON ALL SEQUENCES IN SCHEMA public TO apisnoop;
   GRANT pg_execute_server_program TO apisnoop;
 #+END_SRC
*** Dockerfile
#+BEGIN_SRC dockerfile :tangle ../apps/postgres/Dockerfile
  FROM postgres:12.0
  MAINTAINER Hippie Hacker <hh@ii.coop>
  RUN apt-get update \
    && apt-get install -y --no-install-recommends \
    postgresql-plpython3-12 \
    postgresql-12-plsh \
    python3-bs4\
    python3-requests \
    wget \
    make \
    gcc \
    libc6-dev \
    curl \
    jq \
    git \
    software-properties-common \
    apt-transport-https
  #  && rm -rf /var/lib/apt/lists/*

  RUN curl -L https://dl.google.com/go/go1.12.4.linux-amd64.tar.gz \
    | tar -C /usr/local -xzf - \
    && echo 'export PATH=$PATH:/usr/local/go/bin' \
    > /etc/profile.d/usr-local-go-path.sh \
    && echo 'export PATH=$PATH:$HOME/go/bin' \
    > /etc/profile.d/homedir-go-path.sh
  RUN . /etc/profile.d/usr-local-go-path.sh \
    && . /etc/profile.d/homedir-go-path.sh \
    && go get github.com/golangci/gofmt/gofmt \
    && go get -u golang.org/x/lint/golint \
    && go get golang.org/x/tools/cmd/goimports \
    && go get github.com/jgautheron/goconst/cmd/goconst \
    && go get github.com/jgautheron/usedexports \
    && go get -u github.com/kisielk/errcheck \
    && go get github.com/ii/apisnoopregexp \
    && cd ~/go/src/github.com/ii/apisnoopregexp \
    && make install \
    && cp ~/go/bin/rmatch /usr/local/bin
  COPY ./initdb /docker-entrypoint-initdb.d
  HEALTHCHECK --interval=10s --timeout=5s --start-period=5s --retries=5 \
    CMD ["pg_isready", "-U ", "apisnoop"] || exit 1
 # RUN sed -i -e"s/^#logging_collector = off.*$/logging_collector = on/" /var/lib/postgresql/data/postgresql.conf

#+END_SRC

#+begin_src shell :dir ../apps/postgres
  export TAG=$(TZ='Pacific/Auckland'; export TZ ; date +%F-%H-%M)
  docker build -t raiinbow/postgres:$TAG .
  docker push raiinbow/postgres:$TAG
#+end_src

#+RESULTS:
#+begin_EXAMPLE
Sending build context to Docker daemon  5.632kB
Step 1/7 : FROM postgres:12.0
 ---> f88dfa384cc4
Step 2/7 : MAINTAINER Hippie Hacker <hh@ii.coop>
 ---> Using cache
 ---> 94003ccb5b6a
Step 3/7 : RUN apt-get update    && apt-get install -y --no-install-recommends    postgresql-plpython3-12    postgresql-12-plsh    python3-bs4   python3-requests    wget    make    gcc    libc6-dev    curl    jq    git    software-properties-common    apt-transport-https
 ---> Using cache
 ---> 68e3d36cdb61
Step 4/7 : RUN curl -L https://dl.google.com/go/go1.12.4.linux-amd64.tar.gz    | tar -C /usr/local -xzf -    && echo 'export PATH=$PATH:/usr/local/go/bin'    > /etc/profile.d/usr-local-go-path.sh    && echo 'export PATH=$PATH:$HOME/go/bin'    > /etc/profile.d/homedir-go-path.sh
 ---> Using cache
 ---> 3690d673eabd
Step 5/7 : RUN . /etc/profile.d/usr-local-go-path.sh    && . /etc/profile.d/homedir-go-path.sh    && go get github.com/golangci/gofmt/gofmt    && go get -u golang.org/x/lint/golint    && go get golang.org/x/tools/cmd/goimports    && go get github.com/jgautheron/goconst/cmd/goconst    && go get github.com/jgautheron/usedexports    && go get -u github.com/kisielk/errcheck    && go get github.com/ii/apisnoopregexp    && cd ~/go/src/github.com/ii/apisnoopregexp    && make install    && cp ~/go/bin/rmatch /usr/local/bin
 ---> Using cache
 ---> db8c609add28
Step 6/7 : COPY ./initdb /docker-entrypoint-initdb.d
 ---> Using cache
 ---> e8ad5bc8a65c
Step 7/7 : HEALTHCHECK --interval=10s --timeout=5s --start-period=5s --retries=5    CMD ["pg_isready", "-U ", "apisnoop"] || exit 1
 ---> Using cache
 ---> e712ce7cc2a6
Successfully built e712ce7cc2a6
Successfully tagged raiinbow/postgres:2019-12-03-14-19
The push refers to repository [docker.io/raiinbow/postgres]
57cc3977e29d: Preparing
a5f4141a90fb: Preparing
71410a046adb: Preparing
4deeca980f47: Preparing
eb866fc784ea: Preparing
db69dddb7910: Preparing
9b3cf71cdd1a: Preparing
038d25594180: Preparing
8f1685521006: Preparing
a6a5c9284280: Preparing
662023ffbbc5: Preparing
9cf9bdafe72e: Preparing
22f5303efdd7: Preparing
64f23a63821f: Preparing
ddb50a567803: Preparing
809946863d5e: Preparing
3ba344018aaf: Preparing
b67d19e65ef6: Preparing
9cf9bdafe72e: Waiting
64f23a63821f: Waiting
22f5303efdd7: Waiting
ddb50a567803: Waiting
a6a5c9284280: Waiting
b67d19e65ef6: Waiting
809946863d5e: Waiting
3ba344018aaf: Waiting
db69dddb7910: Waiting
038d25594180: Waiting
662023ffbbc5: Waiting
8f1685521006: Waiting
eb866fc784ea: Layer already exists
db69dddb7910: Layer already exists
9b3cf71cdd1a: Layer already exists
038d25594180: Layer already exists
8f1685521006: Layer already exists
a6a5c9284280: Layer already exists
662023ffbbc5: Layer already exists
9cf9bdafe72e: Layer already exists
22f5303efdd7: Layer already exists
57cc3977e29d: Pushed
64f23a63821f: Layer already exists
ddb50a567803: Layer already exists
809946863d5e: Layer already exists
3ba344018aaf: Layer already exists
b67d19e65ef6: Layer already exists
a5f4141a90fb: Pushed
71410a046adb: Pushed
4deeca980f47: Pushed
2019-12-03-14-19: digest: sha256:45d56529b17e9814eb467d9f73d7c1f05dbcc857e5014e47383136960fe3d1ba size: 4090
#+end_EXAMPLE

** auditlogger
*** javascript code
 #+NAME: auditlogger bot (nodejs)
 #+begin_src js :tangle ../apps/auditlogger/auditlogger.js
   // apisnoop auditlogger
   const connectionString = typeof process.env.PG_CONNECTION_STRING !== 'undefined' ? process.env.PG_CONNECTION_STRING : 'postgres://apisnoop:s3cretsauc3@postgres/apisnoop?sslmode=disable'
   const rawAuditTableName = typeof process.env.APP_DB_READINESS_TABLE !== 'undefined' ? process.env.APP_DB_READINESS_TABLE : 'audit_event'
   const appPort = typeof process.env.APP_PORT !== 'undefined' ? process.env.APP_PORT : '9900'
   const appDisableLogs = typeof process.env.APP_DISABLE_LOGS !== 'undefined' ? process.env.APP_DISABLE_LOGS : 'false'
   const express = require('express')
   const app = express()
   const bodyParser = require('body-parser')
   const morgan = require('morgan')
   const knex = require('knex')({
       client: 'pg',
       connection: connectionString
   })

   var postgresIsReady = false

   console.log(`[status] using connection string: ${connectionString}`)

   function logs(...messages) {
       if (appDisableLogs == 'true') {
           return
       }
       console.log(...messages)
   }

   function hello (req, res, next) {
       const helloMsg = 'Hey! I\'m your friendly neighbourhood auditlogger. Note: the endpoint /events is where logging takes place.'
       res.json({ message: helloMsg })
       return res.end()
   }

   function requestFailure (req, res, next, message) {
       res.status(400)
       res.json({ message })
       return res.end()
   }

   function checkForBodyContent (req, res, next) {
       if (Object.keys(req.body).length === 0 || typeof req.body !== 'object') {
           console.log('[error] request contains no body')
           return requestFailure(req, res, next, 'request must contain a body')
       }
       return next()
   }

   function checkUserAgent (req, res, next) {
       const requestContent = req.body
       if (req.headers['user-agent'] !== 'kube-apiserver-admission') {
           console.log('[error] request didn\'t come from kube-apiserver')
           return requestFailure(req, res, next, 'Error: request must come from Kubernetes apiserver')
       }
       return next()
   }

   function postgresReadyCheck (req, res, next) {
       if (postgresIsReady === true) {
           return next()
       }
       knex.raw(`SELECT to_regclass('${rawAuditTableName}');`).then(resp => {
           postgresIsReady = resp.rows[0].to_regclass !== null
       })
   }

   function logEventsToDB (req, res, next) {
       const requestContent = req.body
       const items = requestContent.items[0]

       logs(JSON.stringify(requestContent, null, 2))

       logs('[status] inserting into database')
       var dataToInsert = {
            bucket: 'apisnoop',
            job: 'live',
            audit_id: items.auditID,
            stage: items.stage,
            event_verb: items.verb,
            request_uri: items.requestURI,
           data: JSON.stringify(items)
       }
       logs(dataToInsert)

       knex.transaction((trx) => {
            knex(`${rawAuditTableName}`).transacting(trx).insert(dataToInsert)
                .then(trx.commit)
                .catch(trx.rollback)
       }).then(resp => {
           logs('[status] successfully submitted entry')
           res.json({ message: 'operation complete; data inserted' })
           return res.end()
       }).catch(err => {
           console.log(`[error] database: ${err}`)
           requestFailure(req, res, next, `[error] database: ${err}`)
       })
   }

   console.log('[status] starting apisnoop-auditlog-event-handler')

   app.use(bodyParser.urlencoded({
       extended: true
   }))
   app.use(express.json())
   app.use(morgan('combined'))

   app.get('/', hello)
   app.post('/events', [checkForBodyContent, checkUserAgent, postgresReadyCheck], logEventsToDB)

   knex.raw('select 0;').then(() => {
       console.log('[status] connected to database')
       app.listen(appPort, () => {
           console.log(`[status] started; listening on port ${appPort}`)
       })
   }).catch(err => {
       console.log('[error] No database connection found.')
       console.log(err)
       process.exit(1)
   })
 #+end_src

 #+RESULTS: auditlogger bot (nodejs)
 #+begin_src js
 #+end_src

 #+NAME: auditlogger bot (nodejs) package
 #+begin_src json :tangle ../apps/auditlogger/package.json :comments no
 {
   "name": "apisnoop-auditlogger",
   "version": "0.0.2",
   "description": "AuditLogger for apisnoop",
   "main": "index.js",
   "scripts": {
     "test": "echo \"Error: no test specified\" && exit 1"
   },
   "author": "Caleb Woodbine <caleb@ii.coop>",
   "license": "Apache-2.0",
   "dependencies": {
     "express": "^4.17.1",
     "knex": "^0.20.1",
     "morgan": "^1.9.1",
     "pg": "^7.12.1"
   }
 }
 #+end_src

*** Dockerfile
 #+NAME: dockerfile for auditlogger bot
 #+begin_src dockerfile :tangle ../apps/auditlogger/Dockerfile
 FROM node:12.14.1-alpine3.11
 RUN apk update && \
     apk add vim postgresql-client netcat-openbsd
 RUN adduser -D -H -h /opt/apisnoop apisnoop
 WORKDIR /opt/apisnoop
 COPY auditlogger.js .
 COPY package.json .
 RUN npm i
 EXPOSE 9900
 USER apisnoop
 CMD ["node", "auditlogger.js"]
 #+end_src

#+NAME: build auditlog bot container image
#+begin_src shell :dir ../apps/auditlogger
  export TAG=$(TZ='Pacific/Auckland'; export TZ ; date +%F-%H-%M)
  docker build -t raiinbow/auditlogger:$TAG .
  docker push raiinbow/auditlogger:$TAG
#+end_src

#+RESULTS: build auditlog bot container image
#+begin_example
Sending build context to Docker daemon  7.168kB
Step 1/10 : FROM node:12.14.1-alpine3.11
 ---> b0dc3a5e5e9e
Step 2/10 : RUN apk update &&     apk add vim postgresql-client netcat-openbsd
 ---> Using cache
 ---> 23a992794023
Step 3/10 : RUN adduser -D -H -h /opt/apisnoop apisnoop
 ---> Using cache
 ---> 68d2d4de2e96
Step 4/10 : WORKDIR /opt/apisnoop
 ---> Using cache
 ---> f8c50a302386
Step 5/10 : COPY auditlogger.js .
 ---> Using cache
 ---> 4ac0366a3624
Step 6/10 : COPY package.json .
 ---> Using cache
 ---> 9e66af2e56a5
Step 7/10 : RUN npm i
 ---> Using cache
 ---> 1abf2b3c5328
Step 8/10 : EXPOSE 9900
 ---> Using cache
 ---> 6ed780110077
Step 9/10 : USER apisnoop
 ---> Using cache
 ---> a1be9e8d88ea
Step 10/10 : CMD ["node", "auditlogger.js"]
 ---> Using cache
 ---> 683cb69457df
Successfully built 683cb69457df
Successfully tagged raiinbow/auditlogger:2020-01-20-14-30
The push refers to repository [docker.io/raiinbow/auditlogger]
a0ce3a7130b1: Preparing
5ea1d05f9d0f: Preparing
339089d0e2ea: Preparing
deef43923a5f: Preparing
ad7a6f60849a: Preparing
439f09af1a5f: Preparing
5280d2327565: Preparing
77d806cfa004: Preparing
930c8bc01816: Preparing
5216338b40a7: Preparing
439f09af1a5f: Waiting
77d806cfa004: Waiting
930c8bc01816: Waiting
5280d2327565: Waiting
ad7a6f60849a: Layer already exists
339089d0e2ea: Layer already exists
deef43923a5f: Layer already exists
a0ce3a7130b1: Layer already exists
5ea1d05f9d0f: Layer already exists
5280d2327565: Layer already exists
439f09af1a5f: Layer already exists
77d806cfa004: Layer already exists
5216338b40a7: Layer already exists
930c8bc01816: Layer already exists
2020-01-20-14-30: digest: sha256:4d6937de2a7584c939012aaa31869cd43f1593edde300dd1743495f8473f57b8 size: 2411
#+end_example

** pgadmin
*** servers.json
# python setup.py --dump-servers /tmp/server.json --user apisnoop@cncf.io
#+NAME: pgadmin default servers config
#+BEGIN_SRC json :tangle ../apps/pgadmin/servers.json
  {
      "Servers": {
          "1": {
              "Name": "apisnoop",
              "Group": "Servers",
              "Host": "postgres",
              "Port": 5432,
              "MaintenanceDB": "postgres",
              "Username": "apisnoop",
              "SSLMode": "prefer",
              "SSLCert": "<STORAGE_DIR>/.postgresql/postgresql.crt",
              "SSLKey": "<STORAGE_DIR>/.postgresql/postgresql.key",
              "SSLCompression": 0,
              "Timeout": 0,
              "UseSSHTunnel": 0,
              "TunnelPort": "22",
              "TunnelAuthentication": 0
          }
      }
  }
#+END_SRC

** docker-compose.yml

#+NAME: hasura docker-compose
#+BEGIN_SRC yaml :tangle ../apps/docker-compose.yml
  # hasura/docker-compose.yaml
  # Kompose Note: We don’t support anything 3.4 and above at the moment
  version: "3"
  services:
    postgres:
      # image: postgres:11.5 # debian-slim / alternative: postgres:11.5-alpine
      # we need our image to include plpython3u
      image: "raiinbow/postgres:${TAG}"
      build: "./postgres"
        # dockerfile: postgres/Dockerfile
        # context: .
      container_name: "${USER}-postgres"
      restart: always
      shm_size: '64GB' # we currently use big boxes
      #cap_add: # capabilities
      #  - ALL
      environment:
        # These are postgres server setup options
        - POSTGRES_DB=${PGDATABASE}
        - POSTGRES_USER=${PGUSER}
        - POSTGRES_PASSWORD=${PGPASS}
        # - PGDATA=/dev/shm/pgdata
        # - POSTGRES_PASSWORD_FILE="/run/secrets/postgres-passwd" # k8s secret style
        # - POSTGRES_INITDB_ARGS=""
        # - POSTGRES_INITDB_WALDIR=""
        # - PGDATA="/var/lib/postgresql/data/"
        # These are psql client defaults
        - PGDATABASE=${PGDATABASE}
        - PGUSER=${PGUSER}
      #volumes:
        # runs *sql and executable *sh and sources non-executable *sh
        # - ./postgres/initdb:/docker-entrypoint-initdb.d:Z
        # mainly mounted for debugging
        #- ./hasura/migrations:/migrations:Z
      healthcheck:
        test: ["CMD-SHELL", "pg_isready -U $PGUSER"]
        interval: 10s
        timeout: 5s
        retries: 5
      ports: # exposed to docker host
        - "${PGPORT}:5432"
      # Expose ports without publishing them to the host machine
      # - they’ll only be accessible to linked services.
      expose: # exposed to other containers
        - "${PGPORT}"
      networks: # separate from web due to traefik routing on sharing.io
        - db
      labels:
        ingress.class: "traefik"
        kompose.controller.type: "deployment" # or daemonset or replicationcontroller
        # kompose.service.type: "loadbalancer" # nodeport / clusterip / loadbalancer / headless
        # kompose.service.expose: true
        # Maybe see if kompose can work with let's encrypt
        #- "kompose.service.expose.tls-secret="
        # kompose.volume.size: 1Gi
        # traefik.enable: "true"
        # traefik.basic.port: 8080
        # traefik.basic.protocol: "http"
        # traefik.basic.frontend.rule: "Host:${USER}-hasura.sharing.io"
    hasura:
      #image: hasura/graphql-engine:v1.0.0-beta.5
      #image: hasura/graphql-engine:v1.0.0-beta.5.cli-migrations
      image: "raiinbow/hasura:${TAG}"
      build: "./hasura"
      # build:
      #   dockerfile: hasura/Dockerfile
      #   context: .
      #   image: "raiinbow/hasura:${USER}"
      container_name: "${USER}-hasura"
      restart: always
      networks:
        - web
        - db
      environment:
        - HASURA_GRAPHQL_DATABASE_URL=postgres://${PGUSER}:${PGPASS}@postgres:5432/${PGDATABASE}
        - HASURA_GRAPHQL_ENABLE_CONSOLE=true
      # volumes:
      #   - ./hasura/migrations:/hasura-migrations:Z
      depends_on:
        - postgres
      ports: # exposed to docker host
        - "${HASURA_PORT}:8080"
      expose:
        - "8080"
      labels:
        kompose.controller.type: "deployment" # or daemonset or replicationcontroller
        kompose.service.type: "headless" # necessary for traefik
        # kompose.service.type: "clusterip" # nodeport / clusterip / loadbalancer / headless
        # kompose.service.type: "loadbalancer" # nodeport / clusterip / loadbalancer / headless
        # kompose.service.type: "nodeport" # nodeport / clusterip / loadbalancer / headless
        kompose.service.expose: "${USER}-hasura.apisnoop.io" # true / hostname
        ingress.kubernetes.io/protocol: "http"
        kubernetes.io/ingress.class: "traefik"
        # Maybe see if kompose can work with let's encrypt
        #- "kompose.service.expose.tls-secret="
        # kompose.volume.size: 1Gi
        traefik.docker.network: "web"
        traefik.enable: "true"
        traefik.basic.port: 8080
        traefik.basic.protocol: "http"
        traefik.basic.frontend.rule: "Host:${USER}-hasura.sharing.io"
    # pgadmin:
    #   container_name: "${USER}-pgadmin"
    #   image: dpage/pgadmin4:4.12
    #   restart: always
    #   networks:
    #     - db
    #     - web
    #   environment:
    #     - PGADMIN_DEFAULT_EMAIL=apisnoop@cncf.io
    #     - PGADMIN_DEFAULT_PASSWORD=${PGPASS}
    #     # python setup.py --dump-servers /tmp/servers.json --user apisnoop@cncf.io
    #     - PGADMIN_SERVER_JSON_FILE=/apisnoop/servers.json
    #     - PGADMIN_CONFIG_APP_NAME=APISnoopQL
    #     - PGADMIN_CONFIG_APP_COPYRIGHT="Copyright (C) 2019, The Cloud Native Compute Foundation"
    #     - PGADMIN_CONFIG_LOGIN_BANNER="Welcome to APISnoopQL!"
    #     - PGADMIN_CONFIG_ALLOW_SAVE_PASSWORD=True
    #     - PGADMIN_CONFIG_MAX_QUERY_HIST_STORED=1234
    #     - PGADMIN_CONFIG_SESSION_COOKIE_NAME=apisnoop_session
    #     - PGADMIN_CONFIG_UPGRADE_CHECK_ENABLED=False
    #     - PGADMIN_CONFIG_SESSION_EXPIRATION_TIME=7
    #   volumes:
    #    - ./pgadmin:/apisnoop:Z
    #   ports: # exposed to docker host
    #     - "${PGADMIN_PORT}:80"
    #   expose:
    #     - "80"
    #   labels:
    #     - "traefik.docker.network=web"
    #     - "traefik.enable=true"
    #     - "traefik.basic.port=80"
    #     - "traefik.basic.protocol=http"
    #     - "traefik.basic.frontend.rule=Host:${USER}-pgadmin.sharing.io"
  #volumes:
  #  migrations:
  networks:
    web:
      external: true
    db:
#+END_SRC

** raiinbow.yaml
   
   #+begin_src yaml :tangle ../deployment/k8s/raiinbow.yaml
     apiVersion: v1
     kind: List
     metadata: {}
     items:
     - apiVersion: v1
       kind: Service
       metadata:
         name: hasura
       spec:
         type: ClusterIP
         clusterIP: None
         selector:
           io.apisnoop.graphql: hasura
         ports:
         - name: "8080"
           port: 8080
           targetPort: 8080
     - apiVersion: v1
       kind: Service
       metadata:
         name: postgres
       spec:
         selector:
           io.apisnoop.db: postgres
         ports:
         - name: "5432"
           port: 5432
           targetPort: 5432
     - apiVersion: apps/v1
       kind: Deployment
       metadata:
         name: hasura
       spec:
         replicas: 1
         selector:
           matchLabels:
             io.apisnoop.graphql: hasura
         template:
           metadata:
             labels:
               io.apisnoop.graphql: hasura
           spec:
             restartPolicy: Always
             containers:
             - name: hasura
               image: "raiinbow/hasura:2020-01-09-09-01"
               ports:
               - containerPort: 8080
               env:
               - name: HASURA_GRAPHQL_DATABASE_URL
                 value: "postgres://apisnoop:s3cretsauc3@postgres:5432/apisnoop"
               - name: HASURA_GRAPHQL_ENABLE_CONSOLE
                 value: "true"
     - apiVersion: apps/v1
       kind: Deployment
       metadata:
         name: postgres
       spec:
         replicas: 1
         selector:
           matchLabels:
             io.apisnoop.db: postgres
         template:
           metadata:
             labels:
               io.apisnoop.db: postgres
           spec:
             restartPolicy: Always
             containers:
             - name: postgres
               image: "raiinbow/postgres:2019-12-03-14-19"
               ports:
               - containerPort: 5432
               livenessProbe:
                 exec:
                   command:
                   - "pg_isready"
                   - "-U"
                   - "apisnoop"
                 failureThreshold: 5
                 periodSeconds: 10
                 timeoutSeconds: 5
               env:
               - name: POSTGRES_DB
                 value: apisnoop
               - name: POSTGRES_USER
                 value: apisnoop
               - name: POSTGRES_PASSWORD
                 value: s3cretsauc3
               - name: PGDATABASE
                 value: apisnoop
               - name: PGUSER
                 value: apisnoop
               - name: APISNOOP_BASELINE_BUCKET
                 value: ci-kubernetes-e2e-gci-gce
               - name: APISNOOP_BASELINE_JOB
                 value: "1201280603970867200"
     - apiVersion: apps/v1
       kind: Deployment
       metadata:
         name: apisnoop-auditlogger
       spec:
         replicas: 1
         selector:
           matchLabels:
             io.apisnoop.auditlogger: apisnoop-auditlogger
         template:
           metadata:
             labels:
               io.apisnoop.auditlogger: apisnoop-auditlogger
           spec:
             containers:
               - name: apisnoop-auditlogger
                 image: "raiinbow/auditlogger:2020-01-20-14-30"
                 #command:
                 #  - "sleep"
                 #args: 
                 #  - "+Inf"
                 ports:
                   - containerPort: 9900
     - apiVersion: v1
       kind: Service
       metadata:
         name: apisnoop-auditlogger
       spec:
         ports:
           - port: 9900
             targetPort: 9900
         selector:
           io.apisnoop.auditlogger: apisnoop-auditlogger
         clusterIP: 10.96.96.96
         type: ClusterIP
     - apiVersion: auditregistration.k8s.io/v1alpha1
       kind: AuditSink
       metadata:
         name: auditlogger
       spec:
         policy:
           level: Metadata
           stages:
           - ResponseComplete
         webhook:
           throttle:
             qps: 10
             burst: 15
           clientConfig:
             #url: "http://apisnoop-auditlogger.kube-system.svc.cluster.local:9900/events"
             # svc cluster ip of apisnoop-auditlogger
             url: "http://10.96.96.96:9900/events"
   #+end_src
   
   To build up our rainbow.yaml again, we can run:
   #+NAME: update and push rainbow images based on docker-compose.yml
   #+begin_src shell :dir ../apps :tangle no
     . .loadenv
     kompose convert --build local --volumes emptyDir -o /dev/null 2>&1 | grep Pushing
     :
   #+end_src

   #+RESULTS: update and push rainbow images based on docker-compose.yml
   #+begin_EXAMPLE
   #+end_EXAMPLE

   #+RESULTS: update and push rainbow images based on docker-ujjcompose.yml
   #+begin_EXAMPLE
   [36mINFO[0m Pushing image 'raiinbow/hasura:2019-11-10-14-36' to registry 'docker.io' 
   [36mINFO[0m Pushing image 'raiinbow/postgres:2019-11-10-14-36' to registry 'docker.io' 
   #+end_EXAMPLE

** footnotes

* Footnotes
** Local Variables

Force this instance of emacs to use the apisnoop server-name.
This allows us to tangle from the emacsclient cli.

# Local Variables:
# eval: (setq server-name "apisnoop")
# eval: (server-force-delete)
# eval: (server-start)
# ii: enabled
# End:
 
 
 

** Questions
   apisnoop socket not found, had to manually tangle the file.  What changed?
