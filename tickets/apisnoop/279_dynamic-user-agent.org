#+TITLE: Dynamic user Agent

* Ticket
* Process
** Grab update sources code
   #+begin_src python
     #!/usr/bin/env python
     import yaml
     try:
         from urllib.request import urlopen, urlretrieve
     except Exception as e:
         from urllib import urlopen, urlretrieve
     import re
     from bs4 import BeautifulSoup
     import click
     import json

     gubernator = "https://gubernator.k8s.io/builds/kubernetes-jenkins/logs/"

     gcs_logs="https://storage.googleapis.com/kubernetes-jenkins/logs/"

     def get_json(url):
         body = urlopen(url).read()
         data = json.loads(body)
         return data

     @click.command()
     @click.argument('sources')
     def main(sources):
         # https://github.com/yaml/pyyaml/wiki/PyYAML-yaml.load(input)-Deprecation
         syaml = yaml.load(open(sources).read())
         for bucket, info in syaml['buckets'].items():
             try:
                 testgrid_history = get_json(gcs_logs + bucket + "/jobResultsCache.json")
             except:
                 import ipdb; ipdb.set_trace(context=60)
             latest_success = [x for x in testgrid_history if x['result'] == 'SUCCESS'][-1]['buildnumber']
             syaml['buckets'][bucket]['jobs']=[str(latest_success)]
             if bucket == syaml['default-view']['bucket']:
                 syaml['default-view']['job']=str(latest_success)
         with open(sources, "w") as f:
             yaml_content = yaml.dump(syaml,
                                      indent=4,
                                      default_flow_style=False)
             f.write(yaml_content)
             print(yaml_content)

     if __name__ == "__main__":
         main()
   #+end_src
   
** Parse down to the only code we need
   The majority of this assumes a yaml file that includes multiple buckets and jobs.
   We dont' need to read from or write from yaml, and we don't need to figure out our bucket as it will either be the default one or an argument passed in.
    
   We only need to figure out what is the latest job for a bucket, if the job was not passed in as an argument.  So the main code to lift is 
   ~latest_success = [x for x in testgrid_history if x['result'] == 'SUCCESS'][-1]['buildnumber']~
** Integrate update_sources with current load_swaggers
   
   I'll make a new function to test this out, that posts to the same bucket_job_swagger

 #+NAME: load_swagger.py
 #+BEGIN_SRC python :eval never :exports code
   try:
       from urllib.request import urlopen, urlretrieve
       from string import Template
       import os
       import json

       def get_json(url):
           body = urlopen(url).read()
           data = json.loads(body)
           return data

       gcs_logs="https://storage.googleapis.com/kubernetes-jenkins/logs/"
       #establish bucket we'll draw test results from.
       baseline_bucket = os.environ['APISNOOP_BASELINE_BUCKET'] if 'APISNOOP_BASELINE_BUCKET' in os.environ.keys() else 'ci-kubernetes-e2e-gci-gce'
       bucket =  baseline_bucket if custom_bucket is None else custom_bucket

       #grab the latest successful test run for our chosen bucket.
       testgrid_history = get_json(gcs_logs + bucket + "/jobResultsCache.json")
       latest_success = [x for x in testgrid_history if x['result'] == 'SUCCESS'][-1]['buildnumber']

       #establish job 
       baseline_job = os.environ['APISNOOP_BASELINE_JOB'] if 'APISNOOP_BASELINE_JOB' in os.environ.keys() else latest_success
       job = baseline_job if custom_job is None else custom_job

       metadata_url = ''.join(['https://storage.googleapis.com/kubernetes-jenkins/logs/', bucket, '/', job, '/finished.json'])
       metadata = json.loads(urlopen(metadata_url).read().decode('utf-8'))
       commit_hash = metadata["version"].split("+")[1]
       swagger_url =  ''.join(['https://raw.githubusercontent.com/kubernetes/kubernetes/', commit_hash, '/api/openapi-spec/swagger.json']) 
       swagger = json.loads(urlopen(swagger_url).read().decode('utf-8')) # may change this to ascii
       sql = """
    INSERT INTO bucket_job_swagger(
              bucket,
              job,
              commit_hash, 
              passed,
              job_result,
              pod,
              infra_commit,
              job_version,
              job_timestamp,
              node_os_image,
              master_os_image,
              swagger
       )
      SELECT
              $1 as bucket,
              $2 as job,
              $3 as commit_hash,
              $4 as passed,
              $5 as job_result,
              $6 as pod,
              $7 as infra_commit,
              $8 as job_version,
              (to_timestamp($9)) AT TIME ZONE 'UTC' as job_timestamp,
              $10 as node_os_image,
              $11 as master_os_image,
              $12 as swagger
       """
       plan = plpy.prepare(sql, [
           'text','text','text','text',
           'text','text','text','text',
           'integer','text','text','jsonb'])
       rv = plpy.execute(plan, [
           bucket if not live else 'apisnoop',
           job if not live else 'live',
           commit_hash,
           metadata['passed'],
           metadata['result'],
           metadata['metadata']['pod'],
           metadata['metadata']['infra-commit'],
           metadata['version'],
           int(metadata['timestamp']),
           metadata['metadata']['node_os_image'],
           metadata['metadata']['master_os_image'],
           json.dumps(swagger)
       ])
       return ''.join(["Success!  Added the swagger for job ", job, " from bucket ", bucket])
   except Exception as err:
       return Template("something went wrong, likely this: ${error}").substitute(error = err)
 #+END_SRC
 
   #+NAME: load_bucket_job_swagger_via_curl.sql
   #+BEGIN_SRC sql-mode :noweb yes :results silent
     set role dba;
     DROP FUNCTION IF EXISTS load_swagger;
     CREATE OR REPLACE FUNCTION load_swagger(
       custom_bucket text default null,
       custom_job text default null,
       live boolean default false)
     RETURNS text AS $$
     <<load_swagger.py>>
     $$ LANGUAGE plpython3u ;
     reset role;
   #+END_SRC
   
   #+begin_src sql-mode :results silent
   delete from bucket_job_swagger;
   #+end_src
   
   #+begin_src sql-mode
     select * from load_swagger();
   #+end_src

   #+RESULTS:
   #+begin_src sql-mode
                                            load_swagger                                          
   -----------------------------------------------------------------------------------------------
    Success!  Added the swagger for job 1201639070850093059 from bucket ci-kubernetes-e2e-gci-gce
   (1 row)

   #+end_src
   
   #+begin_src sql-mode
   select bucket, job from bucket_job_swagger;
   #+end_src

   #+RESULTS:
   #+begin_src sql-mode
             bucket           |         job         
   ---------------------------+---------------------
    ci-kubernetes-e2e-gci-gce | 1201639070850093059
   (1 row)

   #+end_src
   
   #+begin_src sql-mode
     select * from load_swagger(null, null, true);
   #+end_src

   #+RESULTS:
   #+begin_src sql-mode
                                            load_swagger                                          
   -----------------------------------------------------------------------------------------------
    Success!  Added the swagger for job 1201639070850093059 from bucket ci-kubernetes-e2e-gci-gce
   (1 row)

   #+end_src
   
   #+begin_src sql-mode
   select bucket, job from bucket_job_swagger;
   #+end_src

   #+RESULTS:
   #+begin_src sql-mode
             bucket           |         job         
   ---------------------------+---------------------
    ci-kubernetes-e2e-gci-gce | 1201639070850093059
    apisnoop                  | live
   (2 rows)

   #+end_src
   
** Use pg.options vars as bucket, job argument in load swaggers
   
   Along with integrating the job fetching, we altered the arguments being passed to the function with ~current_setting('custom.bucket', true)~.  
   Current Setting lets you view all the settings of the db ([[https://www.postgresql.org/docs/current/functions-admin.html][see documentation]]).  We can also pass along some settings when the server first starts using PGOPTIONS(see [[https://dba.stackexchange.com/questions/52235/how-can-i-use-an-environment-variable-in-a-postgres-function][this stackexchange]]).  So the plan is to set a ~custom.bucket~ and ~custom.job~ using pgoptions as part of the docker-compose. 

In the function, then, we declare a bucket and job variable.  Bucket is set to default master unless custom_bucket is set and job is set to latest success of the declared bucket unless custom_job is set.

So now we create a new docker build with a special environment variable for bucket.  If it works, we'll have loaded a swagger from the latest success of an alternate bucket.


I think we want something like
#+begin_example
  - name: PGOPTIONS
  value: "-c custom.bucket=ci-kubernetes-e2e-gce-cos-k8sbeta-default"
#+end_example

however, if we add this to our raiinbow.yaml, postgres will start but will not do any migrations.  i think there is an error being caused that is a bit hard to see in the logs.
** Scratch that and use python os.env variables instead.
   pgoptions ws getting too hard to use with the docker compose file, or the raiinbow yaml file, and since we are doing it all in python we ecan use python's os.environ function to get any variables we set on the container itself.  So now we can have vars for `APISNOOP_BASELINE_BUCKET` and ~APISNOOP_BASELINE_JOB~ and it will load thse instead.
** Configure load_audits to use the same logic.
*** Python Code
**** deep_merge
#+NAME: deep_merge
#+BEGIN_SRC python :tangle no
  from copy import deepcopy
  from functools import reduce

  def deep_merge(*dicts, update=False):
      """
      Merges dicts deeply.
      Parameters
      ----------
      dicts : list[dict]
          List of dicts.
      update : bool
          Whether to update the first dict or create a new dict.
      Returns
      -------
      merged : dict
          Merged dict.
      """
      def merge_into(d1, d2):
          for key in d2:
              if key not in d1 or not isinstance(d1[key], dict):
                  d1[key] = deepcopy(d2[key])
              else:
                  d1[key] = merge_into(d1[key], d2[key])
          return d1

      if update:
          return reduce(merge_into, dicts[1:], dicts[0])
      else:
          return reduce(merge_into, dicts, {})
#+END_SRC

**** load_openapi_spec
#+NAME: load_openapi_spec
#+BEGIN_SRC python :tangle no
  def load_openapi_spec(url):
      cache=defaultdict(dict)
      openapi_spec = {}
      openapi_spec['hit_cache'] = {}

      swagger = requests.get(url).json()
      for path in swagger['paths']:
          path_data = {}
          path_parts = path.strip("/").split("/")
          path_len = len(path_parts)
          path_dict = {}
          last_part = None
          last_level = None
          current_level = path_dict
          for part in path_parts:
              if part not in current_level:
                  current_level[part] = {}
              last_part=part
              last_level = current_level
              current_level = current_level[part]
          for method, swagger_method in swagger['paths'][path].items():
              if method == 'parameters':
                  next
              else:
                  current_level[method]=swagger_method.get('operationId', '')
          cache = deep_merge(cache, {path_len:path_dict})
      openapi_spec['cache'] = cache
      return openapi_spec
#+END_SRC

#+RESULTS: load_openapi_spec
: None
**** find_operation_id
#+NAME: find_operation_id
#+BEGIN_SRC python :tangle no
  def find_operation_id(openapi_spec, event):
    verb_to_method={
      'get': 'get',
      'list': 'get',
      'proxy': 'proxy',
      'create': 'post',
      'post':'post',
      'put':'post',
      'update':'put',
      'patch':'patch',
      'connect':'connect',
      'delete':'delete',
      'deletecollection':'delete',
      'watch':'get'
    }
    method=verb_to_method[event['verb']]
    url = urlparse(event['requestURI'])
    # 1) Cached seen before results
    if url.path in openapi_spec['hit_cache']:
      if method in openapi_spec['hit_cache'][url.path].keys():
        return openapi_spec['hit_cache'][url.path][method]
    uri_parts = url.path.strip('/').split('/')
    if 'proxy' in uri_parts:
        uri_parts = uri_parts[0:uri_parts.index('proxy')]
    part_count = len(uri_parts)
    try: # may have more parts... so no match
        cache = openapi_spec['cache'][part_count]
    except Exception as e:
      plpy.warning("part_count was:" + part_count)
      plpy.warning("spec['cache'] keys was:" + openapi_spec['cache'])
      raise e
    last_part = None
    last_level = None
    current_level = cache
    for idx in range(part_count):
      part = uri_parts[idx]
      last_level = current_level
      if part in current_level:
        current_level = current_level[part] # part in current_level
      elif idx == part_count-1:
        if part == 'metrics':
          return None
        #   elif part == '': # The last V
        #     current_level = last_level
        #       else:
        variable_levels=[x for x in current_level.keys() if '{' in x] # vars at current(final) level?
        if len(variable_levels) > 1:
          raise "If we have more than one variable levels... this should never happen."
        next_level=variable_levels[0] # the var is the next level
        current_level = current_level[next_level] # variable part is final part
      else:
        next_part = uri_parts[idx+1]
        variable_levels={next_level:next_part in current_level[next_level].keys() for next_level in [x for x in current_level.keys() if '{' in x]}  
        if not variable_levels: # there is no match
          if 'example.com' in part:
            return None
          elif 'kope.io' in part:
            return None
          elif 'snapshot.storage.k8s.io' in part:
            return None
          elif 'metrics.k8s.io' in part:
            return None
          elif 'wardle.k8s.io' in part:
            return None
          elif ['openapi','v2'] == uri_parts: # not part our our spec
            return None
          else:
            print(url.path)
            return None
        next_level={v: k for k, v in variable_levels.items()}[True]
        current_level = current_level[next_level] #coo
    try:
      op_id=current_level[method]
    except Exception as err:
      plpy.warning("method was:" + method)
      plpy.warning("current_level keys:" + current_level.keys())
      raise err
    if url.path not in openapi_spec['hit_cache']:
      openapi_spec['hit_cache'][url.path]={method:op_id}
    else:
      openapi_spec['hit_cache'][url.path][method]=op_id
    return op_id
#+END_SRC
**** load_audit_events
#+NAME: load_audit_events.py
#+BEGIN_SRC python :noweb yes :exports none
  #!/usr/bin/env python3
  from urllib.request import urlopen, urlretrieve
  import os
  import re
  from bs4 import BeautifulSoup
  import subprocess
  import time
  import glob
  from tempfile import mkdtemp
  from string import Template
  from urllib.parse import urlparse
  import requests
  import hashlib
  from collections import defaultdict
  import json
  import csv
  import sys

  <<deep_merge>>
  <<load_openapi_spec>>
  <<find_operation_id>>
  def get_json(url):
      body = urlopen(url).read()
      data = json.loads(body)
      return data

  def get_html(url):
      html = urlopen(url).read()
      soup = BeautifulSoup(html, 'html.parser')
      return soup


  def download_url_to_path(url, local_path):
      local_dir = os.path.dirname(local_path)
      if not os.path.isdir(local_dir):
          os.makedirs(local_dir)
      if not os.path.isfile(local_path):
          process = subprocess.Popen(['wget', '-q', url, '-O', local_path])
          downloads[local_path] = process

  # this global dict is used to track our wget subprocesses
  # wget was used because the files can get to several halfa gig
  downloads = {}
  gcs_logs="https://storage.googleapis.com/kubernetes-jenkins/logs/"
  #establish bucket we'll draw test results from.
  baseline_bucket = os.environ['APISNOOP_BASELINE_BUCKET'] if 'APISNOOP_BASELINE_BUCKET' in os.environ.keys() else 'ci-kubernetes-e2e-gci-gce'
  bucket =  baseline_bucket if custom_bucket is None else custom_bucket

  #grab the latest successful test run for our chosen bucket.
  testgrid_history = get_json(gcs_logs + bucket + "/jobResultsCache.json")
  latest_success = [x for x in testgrid_history if x['result'] == 'SUCCESS'][-1]['buildnumber']

  #establish job 
  baseline_job = os.environ['APISNOOP_BASELINE_JOB'] if 'APISNOOP_BASELINE_JOB' in os.environ.keys() else latest_success
  job = baseline_job if custom_job is None else custom_job

  def load_audit_events(bucket,job):
      bucket_url = 'https://storage.googleapis.com/kubernetes-jenkins/logs/' + bucket + '/' + job + '/'
      artifacts_url = 'https://gcsweb.k8s.io/gcs/kubernetes-jenkins/logs/' + bucket + '/' +  job + '/' + 'artifacts'
      job_metadata_files = [
          'finished.json',
          'artifacts/metadata.json',
          'artifacts/junit_01.xml',
          'build-log.txt'
      ]
      download_path = mkdtemp( dir='/tmp', prefix='apisnoop-' + bucket + '-' + job ) + '/'
      combined_log_file = download_path + 'audit.log'

      # meta data to download
      for jobfile in job_metadata_files:
          download_url_to_path( bucket_url + jobfile,
                                download_path + jobfile )

      # Use soup to grab url of each of audit.log.* (some end in .gz)
      soup = get_html(artifacts_url)
      master_link = soup.find(href=re.compile("master"))
      master_soup = get_html(
          "https://gcsweb.k8s.io" + master_link['href'])
      log_links = master_soup.find_all(
          href=re.compile("audit.log"))

      finished_metadata = json.load(open(download_path + 'finished.json'))
      commit_hash=finished_metadata['job-version'].split('+')[1]
      # download all logs
      for link in log_links:
          log_url = link['href']
          log_file = download_path + os.path.basename(log_url)
          download_url_to_path( log_url, log_file)

      # Our Downloader uses subprocess of curl for speed
      for download in downloads.keys():
          # Sleep for 5 seconds and check for next download
          while downloads[download].poll() is None:
              time.sleep(5)
              # print("Still downloading: " + download)
          # print("Downloaded: " + download)

      # Loop through the files, (z)cat them into a combined audit.log
      with open(combined_log_file, 'ab') as log:
          for logfile in sorted(
                  glob.glob(download_path + '*kube-apiserver-audit*'), reverse=True):
              if logfile.endswith('z'):
                  subprocess.run(['zcat', logfile], stdout=log, check=True)
              else:
                  subprocess.run(['cat', logfile], stdout=log, check=True)
      # Process the resulting combined raw audit.log by adding operationId
      spec = load_openapi_spec('https://raw.githubusercontent.com/kubernetes/kubernetes/' + commit_hash +  '/api/openapi-spec/swagger.json')
      infilepath=combined_log_file
      outfilepath=combined_log_file+'+opid'
      with open(infilepath) as infile:
          with open(outfilepath,'w') as output:
              for line in infile.readlines():
                  event = json.loads(line)
                  event['operationId']=find_operation_id(spec,event)
                  output.write(json.dumps(event)+'\n')
      #####
      # Load the resulting updated audit.log directly into raw_audit_event
      try:
          # for some reason tangling isn't working to reference this SQL block
          sql = Template("""
  CREATE TEMPORARY TABLE raw_audit_event_import (data jsonb not null) ;
  COPY raw_audit_event_import (data)
  FROM '${audit_logfile}' (DELIMITER e'\x02', FORMAT 'csv', QUOTE e'\x01');

  INSERT INTO raw_audit_event(bucket, job,
                               audit_id, stage,
                               event_verb, request_uri,
                               operation_id,
                               data)
  SELECT '${bucket}', '${job}',
         (raw.data ->> 'auditID'), (raw.data ->> 'stage'),
         (raw.data ->> 'verb'), (raw.data ->> 'requestURI'),
         (raw.data ->> 'operationId'),
         raw.data 
    FROM raw_audit_event_import raw;
          """).substitute(
              audit_logfile = outfilepath,
              # audit_logfile = combined_log_file,
              bucket = bucket,
              job = job
          )
          with open(download_path + 'load.sql', 'w') as sqlfile:
            sqlfile.write(sql)
          rv = plpy.execute(sql)
          #plpy.commit()
          # this calls external binary, not part of transaction 8(
          #rv = plpy.execute("select * from audit_event_op_update();")
          #plpy.commit()
          #rv = plpy.execute("REFRESH MATERIALIZED VIEW CONCURRENTLY podspec_field_coverage_material;")
          #plpy.commit()
          return "it worked"
      except plpy.SPIError:
          return "something went wrong with plpy"
      except:
          return "something unknown went wrong"
  #if __name__ == "__main__":
  #    load_audit_events('ci-kubernetes-e2e-gci-gce','1134962072287711234')
  #else:
  load_audit_events(bucket,job)
#+END_SRC

*** Create
#+NAME: load_audit_events.sql
#+BEGIN_SRC sql-mode :noweb yes :results silent
  set role dba;
  CREATE OR REPLACE FUNCTION load_audit_events(
  custom_bucket text default null, 
  custom_job text default null)
  RETURNS text AS $$
  <<load_audit_events.py>>
  $$ LANGUAGE plpython3u ;
  reset role;
#+END_SRC

#+RESULTS: load_audit_events.sql
#+begin_src sql-mode
SET
apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# 
#+end_src


#+begin_src sql-mode
select * from zzload_audit_events();
#+end_src

#+RESULTS:
#+begin_src sql-mode
 zzload_audit_events 
---------------------
 
(1 row)

#+end_src

#+begin_src sql-mode
\d+
#+end_src

#+RESULTS:
#+begin_src sql-mode
                                          List of relations
  Schema   |               Name               |       Type        |  Owner   |  Size   | Description 
-----------+----------------------------------+-------------------+----------+---------+-------------
 pg_temp_6 | raw_audit_event_import           | table             | apisnoop | 371 MB  | 
 public    | api_operation_material           | materialized view | apisnoop | 3568 kB | 
 public    | api_operation_parameter_material | materialized view | apisnoop | 5800 kB | 
 public    | audit_event                      | view              | apisnoop | 0 bytes | 
 public    | bucket_job_swagger               | table             | apisnoop | 5432 kB | 
 public    | change_in_coverage               | view              | apisnoop | 0 bytes | 
 public    | change_in_tests                  | view              | apisnoop | 0 bytes | 
 public    | endpoint_coverage                | view              | apisnoop | 0 bytes | 
 public    | endpoints_hit_by_new_test        | view              | apisnoop | 0 bytes | 
 public    | projected_change_in_coverage     | view              | apisnoop | 0 bytes | 
 public    | raw_audit_event                  | table             | apisnoop | 829 MB  | 
 public    | stable_endpoint_stats            | view              | apisnoop | 0 bytes | 
(12 rows)

#+end_src

heckyah!  we can see that after running zload_audit_events (temp name to not clash with namespace) our raw_audit_events went up 100% from 400mb to 800mb.  In addition, there are now three bucket/job combos in our raw_audit_event table, showing that we took the hardcoded result and the new dynamically generated one.  All in all: success!

* Conclusions | Next Steps
  We can dynamically set our bucket and job with a small adjustment to the existing functions and including new env vars in the postgres portion of our raiinbow.yaml.  We are doing this using the os.environ functions of the python library, which ties us into using python in our postgres functions for populating data.  So our current method is not language agnostic, but that's not really an issue until someone wants to write a data population function in a different language that doesn't have os.env capabilities.  We should not build for that usecase when it seems like such a niche.

The next steps will be to update the functions in our official tables_and_views_bot.org, push the changes, then push the updated raiinbow.yaml file with it's commented out env vars.
