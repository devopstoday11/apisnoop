#+TITLE: 88: Audit Sink Extended work
#+TODO: TODO IN-PROGRESS BLOCKED | TADA

* Purpose
This is a continuation of ticket 88, which has already been mostly done through caleb's work.  The ticket is focused on listening to when new events are added to our live_audit_events db and then running queries on how coverage ahs changed.  These queries require an operation_id to be added to the event table, which is not included in the event data.  So this file is focused on the work to get that id added and setup some good queries.

* Process
** TADA change audit_sink to insert into live_audit_event db instead
   CLOSED: [2019-11-09 Sat 02:02]
   This is so it's easier to delete everything from a db on runs, then if everything was going into our raw audit events.
   this also sets it up nicely for when we wanna do a full e2e coverage diff report, where the result of that will go into our raw audit events.
   So we will need to first make the table and then the view.
** TADA Create a live_audit_event table
   CLOSED: [2019-11-09 Sat 02:03]
   this is identical to raw_audit_event so we can essentially copy and paste.
** TADA create trigger for when new event added into live_audit_event
   CLOSED: [2019-11-09 Sat 02:52]
   For this we want to have an iteration loop where we:
   - insert data into live_audit_event
   - run a query to return the op_id of this data
   - delete the entry
   This will let us see if the trigger is workign and repeatedly do it.  So I've created an iteration loop heading as part of this ticket.
   
   The basic trigger will take the event and create an operation id that says success!  this will show _a_ trigger is working.  the we can re-implement to make the op_id something actually useful.
   
   #+begin_src sql-mode :results silent
     CREATE FUNCTION add_op_id() RETURNS TRIGGER as $add_op_id$
     BEGIN
       NEW.operation_id := 'success';
       RETURN NEW;
     END;
     $add_op_id$ LANGUAGE plpgsql;
   #+end_src
   
   #+begin_src sql-mode :results silent
     CREATE TRIGGER add_op_id BEFORE INSERT ON live_audit_event
       FOR EACH ROW EXECUTE PROCEDURE add_op_id();
   #+end_src
   
   Running through iteration loop we can verify it works!

** TADA setup trigger to grab our open api spec and find op_id based on it.
   CLOSED: [2019-11-09 Sat 22:43]
   So now we want a slightly more involved function.  Here we want to write it in python, and check for a global variable.
   
   We will bring in the code from our load_audit_event function as this is basically the same, just for a single event instead of looping.

#+NAME: deep_merge
#+BEGIN_SRC python :tangle no
  from copy import deepcopy
  from functools import reduce

  def deep_merge(*dicts, update=False):
      """
      Merges dicts deeply.
      Parameters
      ----------
      dicts : list[dict]
          List of dicts.
      update : bool
          Whether to update the first dict or create a new dict.
      Returns
      -------
      merged : dict
          Merged dict.
      """
      def merge_into(d1, d2):
          for key in d2:
              if key not in d1 or not isinstance(d1[key], dict):
                  d1[key] = deepcopy(d2[key])
              else:
                  d1[key] = merge_into(d1[key], d2[key])
          return d1

      if update:
          return reduce(merge_into, dicts[1:], dicts[0])
      else:
          return reduce(merge_into, dicts, {})
#+END_SRC

#+NAME: load_openapi_spec
#+BEGIN_SRC python :tangle no
  def load_openapi_spec(url):
      cache=defaultdict(dict)
      openapi_spec = {}
      openapi_spec['hit_cache'] = {}

      swagger = requests.get(url).json()
      for path in swagger['paths']:
          path_data = {}
          path_parts = path.strip("/").split("/")
          path_len = len(path_parts)
          path_dict = {}
          last_part = None
          last_level = None
          current_level = path_dict
          for part in path_parts:
              if part not in current_level:
                  current_level[part] = {}
              last_part=part
              last_level = current_level
              current_level = current_level[part]
          for method, swagger_method in swagger['paths'][path].items():
              if method == 'parameters':
                  next
              else:
                  current_level[method]=swagger_method.get('operationId', '')
          cache = deep_merge(cache, {path_len:path_dict})
      openapi_spec['cache'] = cache
      #import ipdb; ipdb.set_trace(context=60)
      return openapi_spec
#+END_SRC

#+NAME: find_operation_id
#+BEGIN_SRC python :tangle no
  def find_operation_id(openapi_spec, event):
    verb_to_method={
      'get': 'get',
      'list': 'get',
      'proxy': 'proxy',
      'create': 'post',
      'post':'post',
      'put':'post',
      'update':'put',
      'patch':'patch',
      'connect':'connect',
      'delete':'delete',
      'deletecollection':'delete',
      'watch':'get'
    }
    method=verb_to_method[event['verb']]
    url = urlparse(event['requestURI'])
    # 1) Cached seen before results
    if url.path in openapi_spec['hit_cache']:
      if method in openapi_spec['hit_cache'][url.path].keys():
        return openapi_spec['hit_cache'][url.path][method]
    uri_parts = url.path.strip('/').split('/')
    if 'proxy' in uri_parts:
        uri_parts = uri_parts[0:uri_parts.index('proxy')]
    part_count = len(uri_parts)
    try: # may have more parts... so no match
        cache = openapi_spec['cache'][part_count]
    except Exception as e:
      plpy.warning("part_count was:" + part_count)
      plpy.warning("spec['cache'] keys was:" + openapi_spec['cache'])
      raise e
    #  import ipdb; ipdb.set_trace(context=60)
    last_part = None
    last_level = None
    current_level = cache
    for idx in range(part_count):
      part = uri_parts[idx]
      last_level = current_level
      if part in current_level:
        current_level = current_level[part] # part in current_level
      elif idx == part_count-1:
        if part == 'metrics': # we aren't collecting metrics for now
          return None
        #   elif part == '': # The last V
        #     current_level = last_level
        #       else:
        variable_levels=[x for x in current_level.keys() if '{' in x] # vars at current(final) level?
        if len(variable_levels) > 1:
          raise "If we have more than one variable levels... this should never happen."
          # import ipdb; ipdb.set_trace(context=60)
        next_level=variable_levels[0] # the var is the next level
        current_level = current_level[next_level] # variable part is final part
      else:
        next_part = uri_parts[idx+1]
        variable_levels={next_level:next_part in current_level[next_level].keys() for next_level in [x for x in current_level.keys() if '{' in x]}  
        if not variable_levels: # there is no match
          if 'example.com' in part:
            return None
          elif 'kope.io' in part:
            return None
          elif 'snapshot.storage.k8s.io' in part:
            return None
          elif 'metrics.k8s.io' in part:
            return None
          elif 'wardle.k8s.io' in part:
            return None
          elif ['openapi','v2'] == uri_parts: # not part our our spec
            return None
          else:
            print(url.path)
            return None
        next_level={v: k for k, v in variable_levels.items()}[True]
        current_level = current_level[next_level] #coo
    try:
      op_id=current_level[method]
    except Exception as err:
      plpy.warning("method was:" + method)
      plpy.warning("current_level keys:" + current_level.keys())
      raise err
    #   import ipdb; ipdb.set_trace(context=60)
    if url.path not in openapi_spec['hit_cache']:
      openapi_spec['hit_cache'][url.path]={method:op_id}
    else:
      openapi_spec['hit_cache'][url.path][method]=op_id
    return op_id
#+END_SRC

   #+NAME: Drop Function
   #+begin_src sql-mode :results silent
   DROP FUNCTION add_op_id CASCADE;
   #+end_src

   #+NAME: Create Function
   #+begin_src sql-mode :results silent :noweb yes
     CREATE FUNCTION add_op_id() RETURNS TRIGGER as 
     $$
       import json
       from urllib.request import urlopen, urlretrieve
       import os
       import re
       from bs4 import BeautifulSoup
       import subprocess
       import time
       import glob
       from tempfile import mkdtemp
       from string import Template
       from urllib.parse import urlparse
       import requests
       import hashlib
       from collections import defaultdict
       import json
       import csv
       import sys
       <<deep_merge>>
       <<load_openapi_spec>>
       <<find_operation_id>>
       if "spec" not in GD:
           GD["spec"] = "good times"
# load_openapi_spec('https://raw.githubusercontent.com/kubernetes/kubernetes/7d13dfe3c34f44/api/openapi-spec/swagger.json')
       # spec = GD["spec"]
       # event = json.loads(TD["new"]["data"])
       if TD["new"]["operation_id"] is None:
           TD["new"]["operation_id"] = GD["spec"];
         # find_operation_id(spec, event);
       return "MODIFY";
     $$ LANGUAGE plpython3u;
   #+end_src
   
   #+NAME: Create Trigger
   #+begin_src sql-mode :results silent
     CREATE TRIGGER add_op_id BEFORE INSERT ON raw_audit_event
       FOR EACH ROW EXECUTE PROCEDURE add_op_id();
   #+end_src
   
   If we run through the iteration loop, we can see a correct op_id applied only if there is no existing op_id. Great!
** TADA setup a coverage report specifically for the live audit_events.
   CLOSED: [2019-11-09 Sat 22:44]
   We actually don't need to do this,w e can add things to our raw_audit_event and then filter using a where clause (we can also delee using a where clause)
** TADA Update tables_and_views_bot to include this trigger.
   CLOSED: [2019-11-09 Sat 23:47]
** BLOCKED Verify everything works.
   I updated our raiinbow.yaml to include the audit sink and updated trigger functions.  I can verify that the trigger works on a local setup, but cannot verify the sink.  I can also verify that the cluster can get up and going with the latest raiinbow and with all our functions.
   
   However, this process is flaky and slow due to name resolution errors.  When you first get started, the tables will not load at all and hasura will repeatedly crash, with its logs showing the issue being that it's trying to connect to a postgres db but cannot resolve the name (even after the postgres is up and running smoothly).  

   Once I got it working, I tried running our sample go code.  It successfully returns a value, but nothing new got added to our raw_audit_event.  Checking the log, we get this error:
   #+begin_example
2019-11-10 01:57:06.040 UTC [176] ERROR:  KeyError: 'verb'
2019-11-10 01:57:06.040 UTC [176] CONTEXT:  Traceback (most recent call last):
          PL/Python function "add_op_id", line 167, in <module>
            TD["new"]["operation_id"] = find_operation_id(spec, event);
          PL/Python function "add_op_id", line 94, in find_operation_id
            method=verb_to_method[event['verb']]
        PL/Python function "add_op_id"
2019-11-10 01:57:06.040 UTC [176] STATEMENT:  insert into "raw_audit_event" ("audit_id", "bucket", "data", "event_verb", "job", "request_uri"
, "stage") values ($1, $2, $3, $4, $5, $6, $7)
   #+end_example

   What I think might be happneing is that it's trying to do an insert, expecting the json from the go code to match a basic audit event..and so it should have a verb to check against, but there isn't one there.  So we can investigate the events given from this function and compare it to lines from our audit logs.

   
   So then I tried to manually insert a new entry into raw_audit_event, using the same code block that I tested on locally.  This has its data taken from a valid raw_audit_event and so should work.  The manual insert returns this error:
#+begin_example
executing Sql-Mode code block (Insert Entry to raw_audit_event)...
"ERROR:  requests.exceptions.ConnectionError: HTTPSConnectionPool(host=’raw.githubusercontent.com’, port=443): Max retries exceeded with url\
: /kubernetes/kubernetes/7d13dfe3c34f44/api/openapi-spec/swagger.json (Caused by NewConnectionError(’<urllib3.connection.VerifiedHTTPSConnec\
tion object at 0x7f301a91cb38>: Failed to establish a new connection: [Errno -2] Name or service not known’))
CONTEXT:  Traceback (most recent call last):
  PL/Python function \"add_op_id\", line 163, in <module>
    GD[\"spec\"] = load_openapi_spec(’https://raw.githubusercontent.com/kubernetes/kubernetes/7d13dfe3c34f44/api/openapi-spec/swagger.json’)
  PL/Python function \"add_op_id\", line 55, in load_openapi_spec
    swagger = requests.get(url).json()
  PL/Python function \"add_op_id\", line 74, in get
  PL/Python function \"add_op_id\", line 59, in request
  PL/Python function \"add_op_id\", line 532, in request
  PL/Python function \"add_op_id\", line 645, in send
  PL/Python function \"add_op_id\", line 515, in send
PL/Python function \"add_op_id\"
#+end_example

So here it is getting to our add_opp_id function, so we know the trigger is working, but gets stuck at the loading of the spec, because it cannot resolve a website's name.  This seems to be another DNS name resolution issue.

What is strange is this same function worked for the loading of the audit events, though it might have happened after a few retries.  I think to properly debug, we will need to ensure the dns is working consistently, as there's too many chance variables at the moment to figure out where the issue might be.
* Iteration Loop for Trigger


  #+NAME: Insert Entry to raw_audit_event
  #+begin_src sql-mode :noweb yes :results silent
    INSERT INTO raw_audit_event( 
    bucket,
    job,
    audit_id,
    stage,
    event_verb,
    request_uri,
    data
    )
    VALUES(
      'apisnoop'
      ,'live'
      ,'audit11231'
      ,'bigstage'
      ,'create'
      ,'https://itworks.com'
      , (select data from raw_audit_event where operation_id is not null limit 1)
    )
      ;
  #+end_src
  
  
  #+NAME: Check op_id and other data from raw_audit_event
  #+begin_src sql-mode 
  select audit_id, operation_id from raw_audit_event where bucket = 'apisnoop';
  #+end_src

  #+RESULTS: Check op_id and other data from raw_audit_event
  #+begin_src sql-mode
    audit_id  |  operation_id  
  ------------+----------------
   audit11231 | readCoreV1Node
  (1 row)

  #+end_src

  #+NAME: Delete Entries from raw_audit_event
  #+begin_src sql-mode :results silent
  DELETE FROM raw_audit_event where bucket = 'apisnoop';
  #+end_src

* Footnotes
** Connect to db
   
  connect with this elisp block
  #+NAME: Connect org to postgres
  #+BEGIN_SRC emacs-lisp :results silent
    (sql-connect "apisnoop" (concat "*SQL: postgres:data*"))
  #+END_SRC
  
  check your connection with \conninfo.  If successful you should see this message in your minibuffer
  : You are connected to database "apisnoop" as user "apisnoop" on host "localhost" at port "10041".

  #+NAME: Test Connection
  #+BEGIN_SRC sql-mode :results silent
  \conninfo
  #+END_SRC
